{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import (\n",
    "    AddGaussianNoise,\n",
    "    TimeStretch,\n",
    "    Shift,\n",
    "    Gain,\n",
    "    TanhDistortion,\n",
    "    SevenBandParametricEQ,\n",
    "    SevenBandParametricEQ,\n",
    "    TimeMask,\n",
    "    ApplyImpulseResponse,\n",
    ")\n",
    "\n",
    "from audiomentations.core.transforms_interface import BaseWaveformTransform\n",
    "import parselmouth\n",
    "import random\n",
    "\n",
    "# import librosa\n",
    "import numpy as np\n",
    "\n",
    "# from random import random\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "PRAAT_CHANGEGENDER_PITCHMEDIAN_DEFAULT = 0.0\n",
    "PRAAT_CHANGEGENDER_FORMANTSHIFTRATIO_DEFAULT = 1.0\n",
    "PRAAT_CHANGEGENDER_PITCHSHIFTRATIO_DEFAULT = 1.0\n",
    "PRAAT_CHANGEGENDER_PITCHRANGERATIO_DEFAULT = 1.0\n",
    "PRAAT_CHANGEGENDER_DURATIONFACTOR_DEFAULT = 1.0\n",
    "\n",
    "\n",
    "def aug(signal, augmentations_dict, override=False, sample_rate=44100):\n",
    "    \"\"\"Main augmentation function\"\"\"\n",
    "    # Augment the signal\n",
    "\n",
    "    sig_aug = signal\n",
    "    # If its not false it is a dict containing manual transforms to apply\n",
    "    if override is not False:\n",
    "        for transform in override.values():\n",
    "            sig_aug = transform(sig_aug)\n",
    "        return sig_aug\n",
    "    if augmentations_dict is False:\n",
    "        return signal\n",
    "    transforms = aug_factory(augmentations_dict)\n",
    "\n",
    "    for transform in transforms:\n",
    "        sig_aug = transform(sig_aug, sample_rate=sample_rate)\n",
    "    return sig_aug\n",
    "\n",
    "\n",
    "def aug_factory(augmentation):\n",
    "    augmentations = []\n",
    "\n",
    "    if augmentation.get(\"gaussian_noise\", 0):\n",
    "        augmentations.append(\n",
    "            AddGaussianNoise(\n",
    "                min_amplitude=0.001,\n",
    "                max_amplitude=0.05,\n",
    "                p=augmentation[\"gaussian_noise\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if augmentation.get(\"time_stretch\", 0):\n",
    "        augmentations.append(\n",
    "            TimeStretch(min_rate=0.8, max_rate=1.2, p=augmentation[\"time_stretch\"])\n",
    "        )\n",
    "\n",
    "    if augmentation.get(\"pitch_shift_naive\", 0):\n",
    "        # augmentations.append(PitchShift(min_semitones=-3, max_semitones=3,\n",
    "        # p=augmentation[\"time_stretch\"]))\n",
    "        # n_steps = random.choice((-4, 4, 3, -3))\n",
    "        # ps = lambda x, sample_rate=44100: np.cast[\"float32\"](\n",
    "        #     librosa.effects.pitch_shift(x, sr=sample_rate, n_steps=n_steps)\n",
    "        # )\n",
    "        # augmentations.append(ps)\n",
    "        pass\n",
    "\n",
    "    if augmentation.get(\"formant_shift_parselmouth_prob\", 0):\n",
    "        # if augmentation.get(\"formant_shift_parselmouth\", 0):\n",
    "        augmentations.append(\n",
    "            FormantShiftParselmouth(\n",
    "                augmentation[\"formant_shift_parselmouth\"],\n",
    "                p=augmentation[\"formant_shift_parselmouth_prob\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if augmentation.get(\"pitch_shift_parselmouth_prob\", 0):\n",
    "        if augmentation.get(\"pitch_shift_parselmouth\", 0):\n",
    "            pitch_shift_ratio = augmentation[\"pitch_shift_parselmouth\"]\n",
    "        else:\n",
    "            pitch_shift_ratio = 1\n",
    "\n",
    "        if augmentation.get(\"pitch_range_parselmouth\", 0):\n",
    "            pitch_range_ratio = augmentation[\"pitch_range_parselmouth\"]\n",
    "        else:\n",
    "            pitch_range_ratio = 1\n",
    "\n",
    "        augmentations.append(\n",
    "            PitchShiftParselmouth(\n",
    "                pitch_shift_ratio,\n",
    "                pitch_range_ratio,\n",
    "                p=augmentation[\"pitch_shift_parselmouth_prob\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if augmentation.get(\"shift\", 0):\n",
    "        augmentations.append(\n",
    "            Shift(\n",
    "                min_fraction=-0.05,\n",
    "                max_fraction=0.2,\n",
    "                p=augmentation[\"shift\"],\n",
    "                rollover=True,\n",
    "                fade=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if augmentation.get(\"gain\", 0):\n",
    "        augmentations.append(\n",
    "            Gain(min_gain_in_db=-6, max_gain_in_db=0, p=augmentation[\"gain\"])\n",
    "        )\n",
    "\n",
    "    if augmentation.get(\"parametric_eq\", 0):\n",
    "        augmentations.append(\n",
    "            SevenBandParametricEQ(\n",
    "                min_gain_db=-2, max_gain_db=1, p=augmentation[\"parametric_eq\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if augmentation.get(\"tanh_distortion\", 0):\n",
    "        augmentations.append(\n",
    "            TanhDistortion(\n",
    "                min_distortion=0.1,\n",
    "                max_distortion=0.2,\n",
    "                p=augmentation[\"tanh_distortion\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if augmentation.get(\"time_mask\", 0):\n",
    "        augmentations.append(TimeMask(max_band_part=1 / 8, p=augmentation[\"time_mask\"]))\n",
    "\n",
    "    if augmentation.get(\"reverb\", 0):\n",
    "        ir_path = augmentation[\"reverb_path\"]\n",
    "        warnings.filterwarnings(\n",
    "            \"ignore\", message=\".* had to be resampled from 16000 hz to 44100 hz.*\"\n",
    "        )\n",
    "        augmentations.append(ApplyImpulseResponse(ir_path, p=augmentation[\"reverb\"]))\n",
    "\n",
    "    return augmentations\n",
    "\n",
    "\n",
    "class PitchShiftParselmouth(BaseWaveformTransform):\n",
    "    \"\"\"Pitch shift the sound up or down without changing the tempo\"\"\"\n",
    "\n",
    "    def __init__(self, pitch_ratio=1.4, range_ratio=1.3, p=0.5):\n",
    "        super().__init__(p)\n",
    "\n",
    "        self.range_ratio = range_ratio\n",
    "        self.init_range = 1\n",
    "        self.enable_reciprocal = True\n",
    "        if type(pitch_ratio) is list:\n",
    "            self.init_range = float(pitch_ratio[0])\n",
    "            pitch_ratio = float(pitch_ratio[1])\n",
    "            # self.enable_reciprocal = True\n",
    "\n",
    "        self.pitch_ratio = pitch_ratio\n",
    "\n",
    "    def randomize_parameters(self, samples, sample_rate):\n",
    "        super().randomize_parameters(samples, sample_rate)\n",
    "        if self.parameters[\"should_apply\"]:\n",
    "            self.parameters[\"pitch_shift_ratio\"] = random.uniform(\n",
    "                self.init_range, self.pitch_ratio\n",
    "            )\n",
    "\n",
    "            if self.enable_reciprocal:\n",
    "                use_reciprocal = random.uniform(-1, 1) > 0\n",
    "                if use_reciprocal:\n",
    "                    self.parameters[\"pitch_shift_ratio\"] = (\n",
    "                        1 / self.parameters[\"pitch_shift_ratio\"]\n",
    "                    )\n",
    "\n",
    "            self.parameters[\"pitch_range_ratio\"] = random.uniform(1, self.range_ratio)\n",
    "\n",
    "            use_reciprocal = random.uniform(-1, 1) > 0\n",
    "            if use_reciprocal:\n",
    "                self.parameters[\"pitch_range_ratio\"] = (\n",
    "                    1 / self.parameters[\"pitch_range_ratio\"]\n",
    "                )\n",
    "\n",
    "    def apply(self, samples, sample_rate):\n",
    "        # Add a check to see if samples is numpy array\n",
    "        if not isinstance(samples, np.ndarray):\n",
    "            samples = np.array(samples)\n",
    "            print(\"samples is not numpy array, converting to numpy array\")\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                action=\"ignore\",\n",
    "                category=parselmouth.PraatWarning,\n",
    "                message=\"This application uses RandomPool, which is BROKEN in older releases\",\n",
    "            )\n",
    "\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            pitch_shifted_samples = apply_formant_and_pitch_shift(\n",
    "                wav_to_Sound(samples, sampling_frequency=sample_rate),\n",
    "                pitch_shift_ratio=self.parameters[\"pitch_shift_ratio\"],\n",
    "                pitch_range_ratio=self.parameters[\"pitch_range_ratio\"],\n",
    "                duration_factor=1.0,\n",
    "            )\n",
    "        return np.squeeze(np.cast[\"float32\"](pitch_shifted_samples.values))\n",
    "\n",
    "\n",
    "class FormantShiftParselmouth(BaseWaveformTransform):\n",
    "    \"\"\"Formant shift using parselmouth\"\"\"\n",
    "\n",
    "    def __init__(self, formant_shift=1.4, p=0.5):\n",
    "        super().__init__(p)\n",
    "        self.init_range = 1\n",
    "        self.enable_reciprocal = True\n",
    "        if type(formant_shift) is list:\n",
    "            self.init_range = float(formant_shift[0])\n",
    "            formant_shift = float(formant_shift[1])\n",
    "            self.enable_reciprocal = True\n",
    "\n",
    "        self.formant_shift = formant_shift\n",
    "\n",
    "    def randomize_parameters(self, samples, sample_rate):\n",
    "        super().randomize_parameters(samples, sample_rate)\n",
    "\n",
    "        if self.parameters[\"should_apply\"]:\n",
    "            self.parameters[\"formant_shift_parselmouth\"] = random.uniform(\n",
    "                self.init_range, self.formant_shift\n",
    "            )\n",
    "\n",
    "            if self.enable_reciprocal:\n",
    "                use_reciprocal = random.uniform(-1, 1) > 0\n",
    "                if use_reciprocal:\n",
    "                    self.parameters[\"formant_shift_parselmouth\"] = (\n",
    "                        1 / self.parameters[\"formant_shift_parselmouth\"]\n",
    "                    )\n",
    "\n",
    "    def apply(self, samples, sample_rate):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                action=\"ignore\",\n",
    "                category=parselmouth.PraatWarning,\n",
    "                message=\"This application uses RandomPool, which is BROKEN in older releases\",\n",
    "            )\n",
    "\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            formant_shifted_samples = apply_formant_and_pitch_shift(\n",
    "                wav_to_Sound(samples, sampling_frequency=sample_rate),\n",
    "                formant_shift_ratio=self.parameters[\"formant_shift_parselmouth\"],\n",
    "                duration_factor=1.0,\n",
    "            )\n",
    "        return np.squeeze(np.cast[\"float32\"](formant_shifted_samples.values))\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\" Parselmouth utils for pitch and formant shifting. \n",
    "    Part of the code is adapted from https://github.com/dhchoi99/NANSY\\\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def wav_to_Sound(wav, sampling_frequency: int = 44100) -> parselmouth.Sound:\n",
    "    r\"\"\" load wav file to parselmouth Sound file\n",
    "    # __init__(self: parselmouth.Sound, other: parselmouth.Sound) -> None \\\n",
    "    # __init__(self: parselmouth.Sound, values: numpy.ndarray[numpy.float64], \n",
    "            sampling_frequency: Positive[float] = 44100.0, start_time: float = 0.0) -> None \\\n",
    "    # __init__(self: parselmouth.Sound, file_path: str) -> None\n",
    "    returns:\n",
    "        sound: parselmouth.Sound\n",
    "    \"\"\"\n",
    "    if isinstance(wav, parselmouth.Sound):\n",
    "        sound = wav\n",
    "    elif isinstance(wav, np.ndarray):\n",
    "        sound = parselmouth.Sound(wav, sampling_frequency=sampling_frequency)\n",
    "    elif isinstance(wav, list):\n",
    "        wav_np = np.asarray(wav)\n",
    "        sound = parselmouth.Sound(\n",
    "            np.asarray(wav_np), sampling_frequency=sampling_frequency\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return sound\n",
    "\n",
    "\n",
    "def get_pitch_median(wav, sr: int = None):\n",
    "    sound = wav_to_Sound(wav, sr)\n",
    "    pitch = None\n",
    "    pitch_median = PRAAT_CHANGEGENDER_PITCHMEDIAN_DEFAULT\n",
    "\n",
    "    try:\n",
    "        pitch = parselmouth.praat.call(sound, \"To Pitch\", 0.8 / 75, 75, 600)\n",
    "        pitch_median = parselmouth.praat.call(\n",
    "            pitch, \"Get quantile\", 0.0, 0.0, 0.5, \"Hertz\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        pass\n",
    "\n",
    "    return pitch, pitch_median\n",
    "\n",
    "\n",
    "def change_gender(\n",
    "    sound,\n",
    "    pitch=None,\n",
    "    formant_shift_ratio: float = PRAAT_CHANGEGENDER_FORMANTSHIFTRATIO_DEFAULT,\n",
    "    new_pitch_median: float = PRAAT_CHANGEGENDER_PITCHMEDIAN_DEFAULT,\n",
    "    pitch_range_ratio: float = PRAAT_CHANGEGENDER_PITCHRANGERATIO_DEFAULT,\n",
    "    duration_factor: float = PRAAT_CHANGEGENDER_DURATIONFACTOR_DEFAULT,\n",
    ") -> parselmouth.Sound:\n",
    "    try:\n",
    "        if pitch is None:\n",
    "            new_sound = parselmouth.praat.call(\n",
    "                sound,\n",
    "                \"Change gender\",\n",
    "                75,\n",
    "                600,\n",
    "                formant_shift_ratio,\n",
    "                new_pitch_median,\n",
    "                pitch_range_ratio,\n",
    "                duration_factor,\n",
    "            )\n",
    "        else:\n",
    "            new_sound = parselmouth.praat.call(\n",
    "                (sound, pitch),\n",
    "                \"Change gender\",\n",
    "                formant_shift_ratio,\n",
    "                new_pitch_median,\n",
    "                pitch_range_ratio,\n",
    "                duration_factor,\n",
    "            )\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "    return new_sound\n",
    "\n",
    "\n",
    "def apply_formant_and_pitch_shift(\n",
    "    sound: parselmouth.Sound,\n",
    "    formant_shift_ratio: float = PRAAT_CHANGEGENDER_FORMANTSHIFTRATIO_DEFAULT,\n",
    "    pitch_shift_ratio: float = PRAAT_CHANGEGENDER_PITCHSHIFTRATIO_DEFAULT,\n",
    "    pitch_range_ratio: float = PRAAT_CHANGEGENDER_PITCHRANGERATIO_DEFAULT,\n",
    "    duration_factor: float = PRAAT_CHANGEGENDER_DURATIONFACTOR_DEFAULT,\n",
    ") -> parselmouth.Sound:\n",
    "    \"\"\"uses praat 'Change Gender' backend to manipulate pitch and formant\n",
    "    'Change Gender' function: praat -> Sound Object -> Convert -> Change Gender\n",
    "    see Help of Praat for more details\n",
    "    # https://github.com/YannickJadoul/Parselmouth/issues/25#issuecomment-608632887 might help\n",
    "    \"\"\"\n",
    "\n",
    "    # pitch = sound.to_pitch()\n",
    "    pitch = None\n",
    "    new_pitch_median = PRAAT_CHANGEGENDER_PITCHMEDIAN_DEFAULT\n",
    "    if pitch_shift_ratio != 1.0:\n",
    "        try:\n",
    "            pitch, pitch_median = get_pitch_median(sound, None)\n",
    "            new_pitch_median = pitch_median * pitch_shift_ratio\n",
    "\n",
    "            # https://github.com/praat/praat/issues/1926#issuecomment-974909408\n",
    "            pitch_minimum = parselmouth.praat.call(\n",
    "                pitch, \"Get minimum\", 0.0, 0.0, \"Hertz\", \"Parabolic\"\n",
    "            )\n",
    "            newMedian = pitch_median * pitch_shift_ratio\n",
    "            scaledMinimum = pitch_minimum * pitch_shift_ratio\n",
    "            resultingMinimum = (\n",
    "                newMedian + (scaledMinimum - newMedian) * pitch_range_ratio\n",
    "            )\n",
    "            if resultingMinimum < 0:\n",
    "                new_pitch_median = PRAAT_CHANGEGENDER_PITCHMEDIAN_DEFAULT\n",
    "                pitch_range_ratio = PRAAT_CHANGEGENDER_PITCHRANGERATIO_DEFAULT\n",
    "\n",
    "            if math.isnan(new_pitch_median):\n",
    "                new_pitch_median = PRAAT_CHANGEGENDER_PITCHMEDIAN_DEFAULT\n",
    "                pitch_range_ratio = PRAAT_CHANGEGENDER_PITCHRANGERATIO_DEFAULT\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    new_sound = change_gender(\n",
    "        sound,\n",
    "        pitch,\n",
    "        formant_shift_ratio,\n",
    "        new_pitch_median,\n",
    "        pitch_range_ratio,\n",
    "        duration_factor,\n",
    "    )\n",
    "\n",
    "    return new_sound\n",
    "\n",
    "\n",
    "def semitones_to_ratio(x):\n",
    "    return 2 ** (x / 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "\n",
    "audio_files = os.listdir('../../../DATA/datasets/PB/train/M/M')\n",
    "\n",
    "def audioname(audio_files,audio_num):\n",
    "    for aud in audio_files:\n",
    "        if aud.split('_')[0] == audio_num:\n",
    "            return aud\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVBaseDictDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_file,\n",
    "        nr_samples,\n",
    "        normalize=True,\n",
    "        augmentations={},\n",
    "        transform_override=False,\n",
    "        batch_sampling_mode=\"sample_clips\",\n",
    "        sr=44100,\n",
    "        multi_epoch=1,\n",
    "        phase = 'TRAINING'\n",
    "    ):\n",
    "        self.csv_file = csv_file\n",
    "        self.nr_samples = nr_samples\n",
    "        self.normalize = normalize\n",
    "        self.augmentations = augmentations\n",
    "        self.transform_override = transform_override\n",
    "        self.batch_sampling_mode = batch_sampling_mode\n",
    "        self.sr = sr\n",
    "        self.multi_epoch = multi_epoch\n",
    "        self.phase = phase\n",
    "        self.groups, self.file_list, self.labels = self._prepare_groups_from_csv(csv_file)\n",
    "        self.group_names = list(self.groups.keys())\n",
    "\n",
    "    def _prepare_groups_from_csv(self, csv_file):\n",
    "        df = pd.read_csv(csv_file, index_col=0)\n",
    "        groups = {}\n",
    "        file_list = []\n",
    "        labels = []\n",
    "        df = df[df.iloc[:,1] != 'Unlabelled']\n",
    "        df = df[df.iloc[:,1] != 'Other']\n",
    "        df = df[df.iloc[:,-1] != 'Speech']\n",
    "        df = df.reset_index(drop = True)\n",
    "        # print(df.shape)\n",
    "        # print(df)\n",
    "        singers = np.unique(df['Singer'])\n",
    "        singer_to_idx = {singer: idx for idx, singer in enumerate(singers)}\n",
    "        print(singer_to_idx)\n",
    "        for _, row in df.iterrows():\n",
    "            if self.phase.upper() == 'TRAINING':\n",
    "                if ((row['Labeled'] == 1) & (row['Singer'] != 'Other') & (row['Singer'] != 'Unlabelled') &  (row['panns'] != 'Speech')):                \n",
    "                    group_name = singer_to_idx[row['Singer']]\n",
    "                    if group_name not in groups:\n",
    "                        groups[group_name] = []\n",
    "                    file_path = row['Audio_Path']\n",
    "                    groups[group_name].append(file_path)\n",
    "                    file_list.append(file_path)\n",
    "                    labels.append(group_name)\n",
    "\n",
    "            elif self.phase.upper() == 'TESTING':\n",
    "                if ((row['Labeled'] == 0) & (row['Singer'] != 'Other') & (row['Singer'] != 'Unlabelled') &  (row['panns'] != 'Speech')):                \n",
    "                    group_name = singer_to_idx[row['Singer']]\n",
    "                    if group_name not in groups:\n",
    "                        groups[group_name] = []\n",
    "                    file_path = row['Audio_Path']\n",
    "                    groups[group_name].append(file_path)\n",
    "                    file_list.append(file_path)\n",
    "                    labels.append(group_name)\n",
    "        return groups, file_list, labels\n",
    "\n",
    "    def get_fragment(self, file):\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(file)\n",
    "            # print(waveform.shape)\n",
    "\n",
    "            if sr != self.sr:\n",
    "                resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sr)\n",
    "                waveform = resampler(waveform)\n",
    "\n",
    "            if waveform.size(1) > self.nr_samples:\n",
    "                waveform = waveform[:, :self.nr_samples]\n",
    "            else:\n",
    "                padding = self.nr_samples - waveform.size(1)\n",
    "                waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "\n",
    "            if self.normalize:\n",
    "                waveform = (waveform - waveform.mean()) / waveform.std()\n",
    "\n",
    "            return waveform\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading fragment from {file}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list) * self.multi_epoch\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx % len(self.file_list)\n",
    "        file = self.file_list[idx]\n",
    "        label = self.labels[idx]\n",
    "        fragment = self.get_fragment(file)\n",
    "        if fragment is None:\n",
    "            raise ValueError(f\"Failed to load audio fragment from {file}\")\n",
    "        fragment1 = aug(\n",
    "            np.cast[\"float32\"](fragment),\n",
    "            self.augmentations,\n",
    "            sample_rate=self.sr,\n",
    "        )\n",
    "        # print('fragment',fragment.shape)\n",
    "        # print('fragment_aug', fragment1.shape)\n",
    "        return {\"clip1\": fragment, \"clip2\": fragment1, \"group_name\": label, 'audio_files': file}\n",
    "\n",
    "    def _count_files_in_dict(self, d):\n",
    "        return sum(len(files) for files in d.values())\n",
    "\n",
    "    def _count_elements_in_dict_split(self, d):\n",
    "        return len(d)\n",
    "\n",
    "    def _merge_groups(self, groups):\n",
    "        merged_groups = {}\n",
    "        for k, v in groups.items():\n",
    "            if k not in merged_groups:\n",
    "                merged_groups[k] = []\n",
    "            merged_groups[k].extend(v)\n",
    "        return merged_groups\n",
    "\n",
    "    def _print_dataset_files_info(self):\n",
    "        total_files = self._count_files_in_dict(self.groups)\n",
    "        print(f\"Total files: {total_files}\")\n",
    "        for group, files in self.groups.items():\n",
    "            print(f\"Group: {group}, Files: {len(files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVBaseDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_file,\n",
    "        batch_size=32,\n",
    "        num_workers=0, \n",
    "        nr_samples=64000,\n",
    "        normalize=True,\n",
    "        augmentations={},\n",
    "        transform_override=False,\n",
    "        batch_sampling_mode=\"sample_clips\",\n",
    "        sr=16000,\n",
    "        multi_epoch=1,\n",
    "        phase = 'TRAINING'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.csv_file = csv_file\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.nr_samples = nr_samples\n",
    "        self.normalize = normalize\n",
    "        self.augmentations = augmentations\n",
    "        self.transform_override = transform_override\n",
    "        self.batch_sampling_mode = batch_sampling_mode\n",
    "        self.sr = sr\n",
    "        self.multi_epoch = multi_epoch\n",
    "        self.phase = phase\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Any data downloading or preparation logic should be placed here\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.dataset = CSVBaseDictDataset(\n",
    "            csv_file=self.csv_file,\n",
    "            nr_samples=self.nr_samples,\n",
    "            normalize=self.normalize,\n",
    "            augmentations=self.augmentations,\n",
    "            transform_override=self.transform_override,\n",
    "            batch_sampling_mode=self.batch_sampling_mode,\n",
    "            sr=self.sr,\n",
    "            multi_epoch=self.multi_epoch,\n",
    "            phase = self.phase\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return None\n",
    "\n",
    "    def process_dataset_dirs(self):\n",
    "        # Custom logic for processing dataset directories if required\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A.S. MURALI': 0, 'AFROZ BANO': 1, 'AJAY CHAKRAVARTY': 2, 'ASLAM SABRI & PARTY': 3, 'BAKUL CHATTOPADHYAY': 4, 'BALDEV SHARAN NARANG': 5, 'BHAI NARMAL SINGH': 6, 'Bal Gandharva': 7, 'Chorus (Students)': 8, 'DINA NATH MISHRA': 9, 'DR. BASAVARAJ RAJGURU': 10, 'DR. NIRMALA SUNDARA RAJAN': 11, 'DR. NIVEDITA SINGH': 12, 'DR. PANTULA RAMA': 13, 'DWARAM DURGA PRASAD RAO & DWARAM SATYANARAYANA RAO': 14, 'GAYATRY GIRISH': 15, 'K. SARASWATHI VIDYARTHI': 16, 'KOYAL DAS GUPTA': 17, 'LOU MAJAW (SOLO)': 18, 'M K SHANKARA NAMBOODRI': 19, 'M. K. PRANESH': 20, 'M. VENKATESH KUMAR.': 21, 'M.K.SWAMINATH PILLAI & D.MURUGESA PILLAI': 22, 'M.M. DANDAPANI DESIKAR': 23, 'M.S. SHEELA': 24, 'MALA SHRI PRASAD': 25, 'MALLADI SURI BABU': 26, 'MANDIRA LAHIRI': 27, 'MANJUSHA KULKARNI PATIL': 28, 'MEETA  PANDIT': 29, 'MOHINDER SINGH': 30, 'MUJAHID HUSSAIN KHAN': 31, 'NAMAGIRIPETTAI .K.KRISHNAN': 32, 'PARMESHWAR HEGDE': 33, 'PITTUKULI MURUGADAS': 34, 'PT. DHARAM NATH MISHRA': 35, 'PT. MADHAV INGALE': 36, 'PT. SIDDARAMA JAMBALADINNI': 37, 'R. SANTANA GOPALAN': 38, 'RAJAN MISHRA & PT. SAJAN MISHRA': 39, 'RAJINDAR SINGH': 40, 'RAJSHREE PATHAK': 41, 'RAMNATH AHEERVAAR': 42, 'RITA GANGULI': 43, 'RITESH & RAJNISH MISHRA': 44, 'S. BALAMBAL': 45, 'S. RUKMINI': 46, 'S. SOWMYA': 47, 'SAVITA DEVI': 48, 'SINGH BANDHU': 49, 'SITARAM SINGH': 50, 'SMT. AVANTIKA CHAKRAVARTI': 51, 'SMT. GEETHA RAJA': 52, 'SMT. KALPANA JHOKARKAR': 53, 'SMT. KAMATCHI KUPPUSWAMI': 54, 'SMT. NITHYASREE MAHADEVAN': 55, 'SMT. VIDYA KATGADE': 56, 'SREEVALSAN J. MENON': 57, 'SUBHRA GUHA': 58, 'T. R. Mahalingam': 59, 'TIRUVAIYARU BV JAYASHREE': 60, 'TUSHAR DUTTA': 61, 'USTAD GHULAM SADIQ KHAN': 62, 'USTAD SALEEM IQBAL': 63, 'VALLAM KRISHNASWAMI PILLAI & K.P.GOVINDARAJA PILLAI': 64, 'VASANTHI KRISHNA RAO': 65, 'VIDUSHI NEELA RAMGOPAL': 66, 'VIDUSHI SUMITRA GUHA': 67, 'YASHPAL': 68}\n"
     ]
    }
   ],
   "source": [
    "csv_file = '/home/surge_siya/ALL CSV/Panns_CSV.csv'\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "# nr_samples = 160000\n",
    "nr_samples = 480000\n",
    "normalize = True\n",
    "augmentations = {\n",
    "    \"enable\": True,\n",
    "    \"gaussian_noise\": 0.5,\n",
    "    \"pitch_shift_naive\": 0,\n",
    "    \"time_stretch\": 0,\n",
    "    \"gain\": 0.5,\n",
    "    \"shift\": 0,\n",
    "    \"parametric_eq\": 0,\n",
    "    \"tanh_distortion\": 0,\n",
    "    \"time_mask\": 0.5,\n",
    "    # \"formant_shift_parselmouth\": 0,\n",
    "    # \"pitch_shift_parselmouth\": [1, 1.3],\n",
    "    # \"pitch_range_parselmouth\": 1.5,\n",
    "    # \"pitch_shift_parselmouth_prob\": 0.5\n",
    "}\n",
    "transform_override = False\n",
    "batch_sampling_mode = \"sample_clips\"\n",
    "sr = 16000\n",
    "multi_epoch = 1\n",
    "\n",
    "data_module = CSVBaseDataModule(\n",
    "    csv_file=csv_file,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    nr_samples=nr_samples,\n",
    "    normalize=normalize,\n",
    "    augmentations=augmentations,\n",
    "    transform_override=transform_override,\n",
    "    batch_sampling_mode=batch_sampling_mode,\n",
    "    sr=sr,\n",
    "    multi_epoch=multi_epoch,\n",
    ")\n",
    "\n",
    "data_module.setup()\n",
    "train_loader = data_module.train_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models, network_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Union, Callable, List, Optional\n",
    "from torchvision.models import efficientnet_b0, efficientnet_b4\n",
    "import torchvision.transforms as vt\n",
    "\n",
    "\n",
    "def get_vision_backbone(\n",
    "    vismod=\"efficientnet_b0\", num_classes=1000, pretrained=False, **kwargs\n",
    "):\n",
    "    if vismod == \"efficientnet_b0\":\n",
    "        return efficientnet_b0(pretrained=pretrained, num_classes=num_classes, **kwargs)\n",
    "    elif vismod == \"efficientnet_b4\":\n",
    "        return efficientnet_b4(pretrained=pretrained, num_classes=num_classes, **kwargs)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Grey2Rgb(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.normalize = vt.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        batch_size, freq_bins, times = data.shape\n",
    "        data /= data.max()\n",
    "        data = data.unsqueeze(1).expand(batch_size, 3, freq_bins, times)\n",
    "        data = self.normalize(data)\n",
    "        return data\n",
    "\n",
    "\n",
    "class LogScale(nn.Module):\n",
    "    def forward(self, data):\n",
    "        # eps = 1e-8\n",
    "        eps = torch.tensor(1e-8, device=data.device)\n",
    "        return torch.log(data + eps)\n",
    "\n",
    "\n",
    "class Aggregator(nn.Module):\n",
    "    \"\"\"Aggregates (in time) a list of features\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.aggregation = nn.Sequential(nn.AdaptiveAvgPool1d(1), nn.Flatten(1))\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            outputs_feature: torch.Tensor of shape(B x C x t)\n",
    "        \"\"\"\n",
    "        if isinstance(features, list):\n",
    "            output_feature = [self.aggregation(feature) for feature in features]\n",
    "        else:\n",
    "            output_feature = self.aggregation(features)\n",
    "        return output_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.error\n",
    "import pathlib\n",
    "import logging\n",
    "from enum import Enum\n",
    "import huggingface_hub\n",
    "from typing import Union\n",
    "from collections import namedtuple\n",
    "from requests.exceptions import HTTPError\n",
    "import hashlib\n",
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _missing_ok_unlink(path):\n",
    "    # missing_ok=True was added to Path.unlink() in Python 3.8\n",
    "    # This does the same.\n",
    "    try:\n",
    "        path.unlink()\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "\n",
    "class FetchFrom(Enum):\n",
    "    \"\"\"Designator where to fetch models/audios from.\n",
    "\n",
    "    Note: HuggingFace repository sources and local folder sources may be confused if their source type is undefined.\n",
    "    \"\"\"\n",
    "\n",
    "    LOCAL = 1\n",
    "    HUGGING_FACE = 2\n",
    "    URI = 3\n",
    "\n",
    "\n",
    "# For easier use\n",
    "FetchSource = namedtuple(\"FetchSource\", [\"FetchFrom\", \"path\"])\n",
    "FetchSource.__doc__ = \"\"\"NamedTuple describing a source path and how to fetch it\"\"\"\n",
    "FetchSource.__hash__ = lambda self: hash(self.path)\n",
    "FetchSource.encode = lambda self, *args, **kwargs: \"_\".join(\n",
    "    (str(self.path), str(self.FetchFrom))\n",
    ").encode(*args, **kwargs)\n",
    "# FetchSource.__str__ = lambda self: str(self.path)\n",
    "\n",
    "\n",
    "def fetch(\n",
    "    filename,\n",
    "    source,\n",
    "    savedir=\"./pretrained_model_checkpoints\",\n",
    "    overwrite=False,\n",
    "    save_filename=None,\n",
    "    use_auth_token=False,\n",
    "    revision=None,\n",
    "    cache_dir: Union[str, pathlib.Path, None] = None,\n",
    "    silent_local_fetch: bool = False,\n",
    "):\n",
    "    \"\"\"Ensures you have a local copy of the file, returns its path\n",
    "\n",
    "    In case the source is an external location, downloads the file.  In case\n",
    "    the source is already accessible on the filesystem, creates a symlink in\n",
    "    the savedir. Thus, the side effects of this function always look similar:\n",
    "    savedir/save_filename can be used to access the file. And save_filename\n",
    "    defaults to the filename arg.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    filename : str\n",
    "        Name of the file including extensions.\n",
    "    source : str or FetchSource\n",
    "        Where to look for the file. This is interpreted in special ways:\n",
    "        First, if the source begins with \"http://\" or \"https://\", it is\n",
    "        interpreted as a web address and the file is downloaded.\n",
    "        Second, if the source is a valid directory path, a symlink is\n",
    "        created to the file.\n",
    "        Otherwise, the source is interpreted as a Huggingface model hub ID, and\n",
    "        the file is downloaded from there.\n",
    "    savedir : str\n",
    "        Path where to save downloads/symlinks.\n",
    "    overwrite : bool\n",
    "        If True, always overwrite existing savedir/filename file and download\n",
    "        or recreate the link. If False (as by default), if savedir/filename\n",
    "        exists, assume it is correct and don't download/relink. Note that\n",
    "        Huggingface local cache is always used - with overwrite=True we just\n",
    "        relink from the local cache.\n",
    "    save_filename : str\n",
    "        The filename to use for saving this file. Defaults to filename if not\n",
    "        given.\n",
    "    use_auth_token : bool (default: False)\n",
    "        If true Hugginface's auth_token will be used to load private models from the HuggingFace Hub,\n",
    "        default is False because majority of models are public.\n",
    "    revision : str\n",
    "        The model revision corresponding to the HuggingFace Hub model revision.\n",
    "        This is particularly useful if you wish to pin your code to a particular\n",
    "        version of a model hosted at HuggingFace.\n",
    "    cache_dir: str or Path (default: None)\n",
    "        Location of HuggingFace cache for storing pre-trained models, to which symlinks are created.\n",
    "    silent_local_fetch: bool (default: False)\n",
    "        Surpress logging messages (quiet mode).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pathlib.Path\n",
    "        Path to file on local file system.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If file is not found\n",
    "    \"\"\"\n",
    "    if save_filename is None:\n",
    "        save_filename = filename\n",
    "    savedir = pathlib.Path(savedir)\n",
    "    savedir.mkdir(parents=True, exist_ok=True)\n",
    "    fetch_from = None\n",
    "    if isinstance(source, FetchSource):\n",
    "        fetch_from, source = source\n",
    "    sourcefile = f\"{source}/{filename}\"\n",
    "    if pathlib.Path(source).is_dir() and fetch_from not in [\n",
    "        FetchFrom.HUGGING_FACE,\n",
    "        FetchFrom.URI,\n",
    "    ]:\n",
    "        # Interpret source as local directory path & return it as destination\n",
    "        sourcepath = pathlib.Path(sourcefile).absolute()\n",
    "        MSG = f\"Destination {filename}: local file in {str(sourcepath)}.\"\n",
    "        if not silent_local_fetch:\n",
    "            logger.info(MSG)\n",
    "        return sourcepath\n",
    "    destination = savedir / save_filename\n",
    "    if destination.exists() and not overwrite:\n",
    "        MSG = f\"Fetch {filename}: Using existing file/symlink in {str(destination)}.\"\n",
    "        logger.info(MSG)\n",
    "        return destination\n",
    "    if (\n",
    "        str(source).startswith(\"http:\") or str(source).startswith(\"https:\")\n",
    "    ) or fetch_from is FetchFrom.URI:\n",
    "        # Interpret source as web address.\n",
    "        MSG = f\"Fetch {filename}: Downloading from normal URL {str(sourcefile)}.\"\n",
    "        logger.info(MSG)\n",
    "        # Download\n",
    "        try:\n",
    "            urllib.request.urlretrieve(sourcefile, destination)\n",
    "        except urllib.error.URLError:\n",
    "            raise ValueError(\n",
    "                f\"Interpreted {source} as web address, but could not download.\"\n",
    "            )\n",
    "    else:  # FetchFrom.HUGGING_FACE check is spared (no other option right now)\n",
    "        # Interpret source as huggingface hub ID\n",
    "        # Use huggingface hub's fancy cached download.\n",
    "        MSG = f\"Fetch {filename}: Delegating to Huggingface hub, source {str(source)}.\"\n",
    "        print(MSG)\n",
    "        logger.info(MSG)\n",
    "        try:\n",
    "            fetched_file = huggingface_hub.hf_hub_download(\n",
    "                repo_id=source,\n",
    "                filename=filename,\n",
    "                use_auth_token=use_auth_token,\n",
    "                revision=revision,\n",
    "                cache_dir=cache_dir,\n",
    "            )\n",
    "            logger.info(f\"HF fetch: {fetched_file}\")\n",
    "        except HTTPError as e:\n",
    "            if \"404 Client Error\" in str(e):\n",
    "                raise ValueError(\"File not found on HF hub\")\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        # Huggingface hub downloads to etag filename, symlink to the expected one:\n",
    "        sourcepath = pathlib.Path(fetched_file).absolute()\n",
    "        # Create destination directory if it does not exist\n",
    "        destination.parent.mkdir(parents=True, exist_ok=True)\n",
    "        _missing_ok_unlink(destination)\n",
    "        destination.symlink_to(sourcepath)\n",
    "    return destination\n",
    "\n",
    "\n",
    "def from_hparams(\n",
    "    cls,\n",
    "    source,\n",
    "    hparams_file=\"hyperparams.yaml\",\n",
    "    weights_file=\"model.pt\",\n",
    "    pymodule_file=\"custom.py\",\n",
    "    overrides={},\n",
    "    savedir=None,\n",
    "    use_auth_token=False,\n",
    "    revision=None,\n",
    "    download_only=False,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Fetch and load based from outside source based on HyperPyYAML file\n",
    "\n",
    "    The source can be a location on the filesystem or online/huggingface\n",
    "\n",
    "    You can use the pymodule_file to include any custom implementations\n",
    "    that are needed: if that file exists, then its location is added to\n",
    "    sys.path before Hyperparams YAML is loaded, so it can be referenced\n",
    "    in the YAML.\n",
    "\n",
    "    The hyperparams file should contain a \"modules\" key, which is a\n",
    "    dictionary of torch modules used for computation.\n",
    "\n",
    "    The hyperparams file should contain a \"pretrainer\" key, which is a\n",
    "    speechbrain.utils.parameter_transfer.Pretrainer\n",
    "\n",
    "    Adapted from https://github.com/speechbrain/\n",
    "\n",
    "    \"\"\"\n",
    "    if savedir is None:\n",
    "        clsname = cls.__name__\n",
    "        savedir = f\"./pretrained_models/{clsname}-{hashlib.md5(source.encode('UTF-8', errors='replace')).hexdigest()}\"\n",
    "    hparams_local_path = fetch(\n",
    "        filename=hparams_file,\n",
    "        source=source,\n",
    "        savedir=savedir,\n",
    "        overwrite=False,\n",
    "        save_filename=None,\n",
    "        use_auth_token=use_auth_token,\n",
    "        revision=revision,\n",
    "    )\n",
    "    weights_local_path = fetch(\n",
    "        filename=weights_file,\n",
    "        source=source,\n",
    "        savedir=savedir,\n",
    "        overwrite=False,\n",
    "        save_filename=None,\n",
    "        use_auth_token=use_auth_token,\n",
    "        revision=revision,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        pymodule_local_path = fetch(\n",
    "            filename=pymodule_file,\n",
    "            source=source,\n",
    "            savedir=savedir,\n",
    "            overwrite=False,\n",
    "            save_filename=None,\n",
    "            use_auth_token=use_auth_token,\n",
    "            revision=revision,\n",
    "        )\n",
    "        sys.path.append(str(pymodule_local_path.parent))\n",
    "    except ValueError:\n",
    "        if pymodule_file == \"custom.py\":\n",
    "            # The optional custom Python module file did not exist\n",
    "            # and had the default name\n",
    "            pass\n",
    "        else:\n",
    "            # Custom Python module file not found, but some other\n",
    "            # filename than the default was given.\n",
    "            raise\n",
    "\n",
    "    # Load the modules:\n",
    "    # with open(hparams_local_path) as fin:\n",
    "    #     hparams = load_hyperpyyaml(fin, overrides)\n",
    "\n",
    "    hparams = yaml.safe_load(open(hparams_local_path, \"r\"))\n",
    "\n",
    "    # Load on the CPU. Later the params can be moved elsewhere by specifying\n",
    "    if not download_only:\n",
    "        # Now return the system\n",
    "        model_class = cls(**hparams, **kwargs)\n",
    "        model_class.load_state_dict(torch.load(weights_local_path, map_location=\"cpu\"))\n",
    "        print(\"Model loaded from\", weights_local_path)\n",
    "        return model_class\n",
    "\n",
    "\n",
    "def from_scripted(filename, source, savedir=None):\n",
    "    \"\"\"Load a model from a scripted file\"\"\"\n",
    "    if savedir is None:\n",
    "        savedir = f\"./pretrained_models/{filename}-{hashlib.md5(source.encode('UTF-8', errors='replace')).hexdigest()}\"\n",
    "    filename = filename + \".ts\" if not filename.endswith(\".ts\") else filename\n",
    "\n",
    "    print(filename, source, savedir)\n",
    "    model_file = fetch(\n",
    "        filename=filename,\n",
    "        source=source,\n",
    "        savedir=savedir,\n",
    "    )\n",
    "\n",
    "    model = torch.jit.load(model_file)\n",
    "    print(\"Model loaded from\", model_file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Optional, List, Callable, Union\n",
    "import torchaudio.transforms as T\n",
    "from nnAudio import features\n",
    "import warnings\n",
    "\n",
    "HF_SOURCE = \"BernardoTorres/singer-identity\"\n",
    "\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spec_layer: str = \"melspectogram\",\n",
    "        n_fft: int = 2048,\n",
    "        hop_length: int = 512,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if spec_layer == \"melspectogram\":\n",
    "            n_mels = 128\n",
    "            if kwargs.get(\"n_mels\", 0):\n",
    "                n_mels = kwargs[\"n_mels\"]\n",
    "            self.spec_layer = features.MelSpectrogram(\n",
    "                n_fft=n_fft, hop_length=hop_length, verbose=False, n_mels=n_mels\n",
    "            )\n",
    "        elif spec_layer == \"stft\":\n",
    "            self.spec_layer = features.STFT(\n",
    "                n_fft=n_fft,\n",
    "                hop_length=hop_length,\n",
    "                verbose=False,\n",
    "                output_format=\"Magnitude\" ** kwargs,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.spec_layer(x)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder, used to extract embeddings from the input acoustic features\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, backbone=\"efficientnet_b0\", embedding_dim=1000, pretrained=False, **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # With attention pooling, not used in the paper\n",
    "        if backbone == \"efficientnet_b0_att\":\n",
    "            encoder_backbone = Efficientnet_att(\n",
    "                vismod=\"efficientnet_b0\",\n",
    "                num_classes=1000,\n",
    "                pretrained=pretrained,\n",
    "                embedding_dim=embedding_dim,\n",
    "                **kwargs,\n",
    "            )\n",
    "            self.net = nn.Sequential(LogScale(), Grey2Rgb(), encoder_backbone)\n",
    "\n",
    "        # Default to efficientnet backbone with average pooling, used in the paper\n",
    "        else:\n",
    "            encoder_backbone = get_vision_backbone(\n",
    "                vismod=backbone,\n",
    "                num_classes=embedding_dim,\n",
    "                pretrained=pretrained,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "            # Grey2Rgb() is used to replicate mel-spec channel (efficientnet expects 3)\n",
    "            self.net = nn.Sequential(LogScale(), Grey2Rgb(), encoder_backbone)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape [batch, channels, frames]\n",
    "        \"\"\"\n",
    "        embedding = self.net(x)\n",
    "        return embedding\n",
    "\n",
    "\n",
    "class Projection(nn.Module):\n",
    "    \"\"\"Projection head, used to reduce the dimensionality of the embedding\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=1000,\n",
    "        output_dim=128,\n",
    "        nonlinearity=None,\n",
    "        is_identity=False,\n",
    "        l2_normalize=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.l2_normalize = l2_normalize\n",
    "        self.is_identity = is_identity\n",
    "        self.output_dim = output_dim\n",
    "        if is_identity:\n",
    "            self.net = nn.Identity()\n",
    "        else:\n",
    "            if nonlinearity is None:\n",
    "                nonlinearity = torch.nn.SiLU()\n",
    "            self.net = nn.Sequential(\n",
    "                nonlinearity, torch.nn.Linear(input_dim, output_dim)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        projection = self.net(x)\n",
    "        if self.l2_normalize and not self.is_identity:\n",
    "            projection = torch.nn.functional.normalize(projection, dim=-1)\n",
    "        return projection\n",
    "\n",
    "\n",
    "class IdentityEncoder(nn.Module):\n",
    "    \"\"\"Wraps a feature extractor with an encoder, without projection head\n",
    "    Useful for loading pretrained models\"\"\"\n",
    "\n",
    "    def __init__(self, feature_extractor, encoder):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = FeatureExtractor(**feature_extractor)\n",
    "        self.encoder = Encoder(**encoder)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(self.feature_extractor(x))\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dims: List[int],\n",
    "        activation: Union[bool, nn.Module] = True,\n",
    "        use_batchnorm: Union[bool, int] = False,\n",
    "        batchnorm_fn: Optional[nn.Module] = None,\n",
    "        last_layer: Optional[nn.Module] = None,\n",
    "        bias: Optional[bool] = None,\n",
    "        layer_init: Optional[Union[Callable[[nn.Module], nn.Module], str]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_dim = dims[0]\n",
    "        self.out_dim = dims[-1]\n",
    "\n",
    "        if len(dims) < 2:\n",
    "            self.model = nn.Identity()\n",
    "\n",
    "            if activation or use_batchnorm:\n",
    "                warnings.warn(\n",
    "                    \"An activation/batch-norm is defined for the projector \"\n",
    "                    \"whereas it is the identity function.\"\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            # define activation layer\n",
    "            if activation is True:\n",
    "                activation = nn.ReLU()\n",
    "\n",
    "            # define batch-norm layer\n",
    "            if use_batchnorm is not False and batchnorm_fn is None:\n",
    "                batchnorm_fn = nn.BatchNorm1d\n",
    "\n",
    "            # useless to add bias just before a batch-norm layer but add the option for completeness\n",
    "            if bias is None:\n",
    "                bias = isinstance(use_batchnorm, bool) and not use_batchnorm\n",
    "\n",
    "            # NOTE: with old implementation use_batchnorm=True means use_batchnorm=False + bias=True\n",
    "            if use_batchnorm is True:\n",
    "                use_batchnorm = 0\n",
    "            elif use_batchnorm is False:\n",
    "                use_batchnorm = float(\"inf\")\n",
    "\n",
    "            ckpt_path = None\n",
    "            if isinstance(layer_init, str):\n",
    "                ckpt_path = layer_init\n",
    "                layer_init = lambda x: x\n",
    "            elif layer_init is None:\n",
    "                layer_init = lambda x: x\n",
    "\n",
    "            layers = []\n",
    "\n",
    "            output_dim = dims.pop()\n",
    "\n",
    "            for i in range(len(dims) - 1):\n",
    "                in_dim, out_dim = dims[i], dims[i + 1]\n",
    "                layers.append(layer_init(nn.Linear(in_dim, out_dim, bias=bias)))\n",
    "                if i >= use_batchnorm:\n",
    "                    layers.append(batchnorm_fn(out_dim))\n",
    "                if activation:\n",
    "                    layers.append(activation)\n",
    "            layers.append(nn.Linear(dims.pop(), output_dim, bias=True))\n",
    "            if last_layer is not None:\n",
    "                layers.append(last_layer)\n",
    "\n",
    "            self.model = nn.Sequential(*layers)\n",
    "\n",
    "            if ckpt_path is not None:\n",
    "                self.load_state_dict(torch.load(ckpt_path))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "class Efficientnet_att(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vismod=\"efficientnet_b0\",\n",
    "        num_classes=1000,\n",
    "        pretrained=False,\n",
    "        embedding_dim=1000,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(Efficientnet_att, self).__init__()\n",
    "\n",
    "        self.vision = get_vision_backbone(\n",
    "            vismod=vismod, num_classes=num_classes, pretrained=pretrained, **kwargs\n",
    "        ).features\n",
    "\n",
    "        self.att = nn.Sequential(\n",
    "            nn.Conv1d(1280, int(embedding_dim / 2), kernel_size=1, groups=2),\n",
    "            AttentiveStatisticPool(int(embedding_dim / 2), 128),\n",
    "        )\n",
    "\n",
    "        self.avg = nn.AvgPool2d((4, 1))\n",
    "        self.bn1 = nn.BatchNorm1d(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.vision(x)\n",
    "        y = self.avg(y).squeeze(2)\n",
    "        y = self.att(y)\n",
    "        return self.bn1(y)\n",
    "\n",
    "\n",
    "# Not used in the experiments in the paper\n",
    "class AttentiveStatisticPool(nn.Module):\n",
    "    def __init__(self, c_in, c_mid):\n",
    "        super(AttentiveStatisticPool, self).__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv1d(c_in, c_mid, kernel_size=1),\n",
    "            nn.Tanh(),  # seems like most implementations uses tanh?\n",
    "            nn.Conv1d(c_mid, c_in, kernel_size=1),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape: B x C x t\n",
    "        alpha = self.network(x)\n",
    "        mu_hat = torch.sum(alpha * x, dim=-1)\n",
    "        var = torch.sum(alpha * x**2, dim=-1) - mu_hat**2\n",
    "        std_hat = torch.sqrt(var.clamp(min=1e-9))\n",
    "        y = torch.cat([mu_hat, std_hat], dim=-1)\n",
    "        # y.shape: B x (c_in*2)\n",
    "        return y\n",
    "\n",
    "\n",
    "def load_model(\n",
    "    model, source=HF_SOURCE, torchscript=False, savedir=None, input_sr=44100\n",
    "):\n",
    "    \"\"\"Load a model from a source, can be a local path or a huggingface model hub ID\"\"\"\n",
    "\n",
    "    if torchscript:\n",
    "        if input_sr != 44100:\n",
    "            raise Exception(\"Torchscript models only support 44100 Hz input\")\n",
    "        model = from_scripted(f\"{model}/model.ts\", source, savedir=savedir)\n",
    "    elif \".\" in model:\n",
    "        # Instantiate IdentityEncoder with input_sr argument\n",
    "        model = from_hparams(\n",
    "            IdentityEncoder,\n",
    "            source,\n",
    "            hparams_file=f\"{model}/hyperparams.yaml\",\n",
    "            weights_file=f\"{model}/model.pt\",\n",
    "            savedir=savedir,\n",
    "        )\n",
    "    else:\n",
    "        # CHeck\n",
    "        model = from_hparams(\n",
    "            IdentityEncoder,\n",
    "            source,\n",
    "            hparams_file=f\"{model}/hyperparams.yaml\",\n",
    "            weights_file=f\"{model}/model.pt\",\n",
    "            savedir=savedir,\n",
    "        )\n",
    "    if input_sr != 44100:\n",
    "        # Replace feature extractor with a resampler\n",
    "        feature_extractor = model.feature_extractor\n",
    "        model.feature_extractor = nn.Sequential(\n",
    "            T.Resample(input_sr, 44100), feature_extractor\n",
    "        )\n",
    "        print(f\"Resampling input from {input_sr} to 44100 Hz\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from singer_identity.utils.core import similarity, roll\n",
    "\n",
    "\n",
    "def std_batch(x, var=1, eps=1e-8):\n",
    "    std = torch.sqrt(x.var(dim=0) + eps)\n",
    "    return torch.mean(F.relu(var - std))\n",
    "\n",
    "\n",
    "def variance_hinge_reg(x, y, var=1):\n",
    "    # From https://github.com/facebookresearch/vicreg\n",
    "    std_x = std_batch(x, var=var)\n",
    "    std_y = std_batch(y, var=var)\n",
    "    std_loss = std_x / 2 + std_y / 2\n",
    "    return std_loss\n",
    "\n",
    "\n",
    "def covariance(x):\n",
    "    # In official implementation they do mean over batch (to verify)\n",
    "    # mean = x.mean(1, keepdims=True)\n",
    "    mean = x.mean(dim=0)\n",
    "    x = x - mean\n",
    "    cov = torch.matmul(x.transpose(0, 1), x) / (x.shape[0] - 1)\n",
    "    # cov = (x.T @ x) / (x.shape[0] - 1)\n",
    "    return cov\n",
    "\n",
    "\n",
    "def covariance_reg(x, y):\n",
    "    eye = torch.eye(x.shape[1]).to(x.device)\n",
    "    cov_x = covariance(x)\n",
    "    cov_y = covariance(y)\n",
    "    assert cov_x.shape[0] == cov_x.shape[1]\n",
    "    assert cov_y.shape[0] == cov_y.shape[1]\n",
    "    cov_reg = (cov_x * (1 - eye)).pow(2).sum() / x.shape[1] + (cov_y * (1 - eye)).pow(\n",
    "        2\n",
    "    ).sum() / x.shape[1]\n",
    "    return cov_reg\n",
    "\n",
    "\n",
    "def invariance_loss(x, y):\n",
    "    return F.mse_loss(x, y)\n",
    "\n",
    "\n",
    "def vicreg_loss(x, y, gamma=1, fact_inv_loss=1, fact_var=1, fact_cov=1):\n",
    "    # Adapted from https://github.com/facebookresearch/vicreg\n",
    "    repr_loss = invariance_loss(x, y)\n",
    "    std_loss = variance_hinge_reg(x, y, var=gamma)\n",
    "    cov_loss = covariance_reg(x, y)\n",
    "    loss = fact_inv_loss * repr_loss + fact_var * std_loss + fact_cov * cov_loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_norms(*args):\n",
    "    norms = []\n",
    "    for arg in args:\n",
    "        norms.append(torch.sqrt((arg**2).sum(1)))\n",
    "    return norms\n",
    "\n",
    "\n",
    "def align_loss(x, y, alpha=2):\n",
    "    # From https://github.com/SsnL/align_uniform\n",
    "    return (x - y).norm(p=2, dim=1).pow(alpha).mean()\n",
    "\n",
    "\n",
    "def uniform_loss(x, t=2):\n",
    "    # From https://github.com/SsnL/align_uniform\n",
    "    return torch.pdist(x, p=2).pow(2).mul(-t).exp().mean().log()\n",
    "\n",
    "\n",
    "def contrastive_loss(z1, z2, temp=0.2, nr_negative=1, decouple=False):\n",
    "    cost_pos = similarity(z1, z2, temp)  # Positive samples\n",
    "    cost_neg = []\n",
    "\n",
    "    n_rolls = min(z1.shape[0] - 1, nr_negative)  # Number of negative samples\n",
    "    curr_neg_z = z2\n",
    "\n",
    "    for i in range(n_rolls):\n",
    "        curr_neg_z = roll(curr_neg_z)  # Shifts batch\n",
    "        cost_neg.append(similarity(z1, curr_neg_z, temp))  # Negative sim.\n",
    "\n",
    "    if not decouple:\n",
    "        cost_neg.append(cost_pos)  # Adds positive similarity in denominator\n",
    "\n",
    "    cost_neg = torch.stack(cost_neg).transpose(1, 0)\n",
    "    cost = (-cost_pos + torch.logsumexp(cost_neg, 1)).mean()\n",
    "    # TODO: implement similarities with less operations, but this works\n",
    "    ratio = torch.mean(cost_neg) / (\n",
    "        torch.mean(cost_pos) + torch.tensor(1e-6).type_as(z1)\n",
    "    )\n",
    "    return cost, ratio.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n"
     ]
    }
   ],
   "source": [
    "import abc\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from copy import deepcopy\n",
    "\n",
    "try:\n",
    "    from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "except ModuleNotFoundError:\n",
    "    print(\"No Lightning Bolts\")\n",
    "\n",
    "\n",
    "class Optimizer(abc.ABC):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "        self.optimizer_class = None\n",
    "\n",
    "    def __call__(self, parameters) -> torch.optim.Optimizer:\n",
    "        return self.optimizer_class(parameters, *self.args, **self.kwargs)\n",
    "\n",
    "    def __str__(self):\n",
    "        params = \"\\n\".join(f\"\\t{arg},\" for arg in self.args) + \"\\n\".join(\n",
    "            f\"\\t{k}: {v}\" for k, v in self.kwargs.items()\n",
    "        )\n",
    "        return self.optimizer_class.__name__ + \"(\\n\" + params + \"\\n)\"\n",
    "\n",
    "\n",
    "class Scheduler(abc.ABC):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "        self.scheduler_class = None\n",
    "\n",
    "    def __call__(self, optimizer):\n",
    "        return self.scheduler_class(optimizer, *self.args, **self.kwargs)\n",
    "\n",
    "    def __str__(self):\n",
    "        params = \"\\n\".join(f\"\\t{arg},\" for arg in self.args) + \"\\n\".join(\n",
    "            f\"\\t{k}: {v}\" for k, v in self.kwargs.items()\n",
    "        )\n",
    "        return self.scheduler_class.__name__ + \"(\\n\" + params + \"\\n)\"\n",
    "\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr: float = 1e-3,\n",
    "        betas: Tuple[float, float] = (0.9, 0.999),\n",
    "        eps: float = 1e-8,\n",
    "        weight_decay: float = 0.0,\n",
    "        amsgrad: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(Adam, self).__init__(\n",
    "            lr=lr,\n",
    "            betas=betas,\n",
    "            eps=eps,\n",
    "            weight_decay=weight_decay,\n",
    "            amsgrad=amsgrad,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.optimizer_class = torch.optim.Adam\n",
    "\n",
    "\n",
    "class LinearWarmupCosineAnnealing(Scheduler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        warmup_epochs: int,\n",
    "        max_epochs: int,\n",
    "        warmup_start_lr: float = 0.0,\n",
    "        eta_min: float = 0.0,\n",
    "        last_epoch: int = -1,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super(LinearWarmupCosineAnnealing, self).__init__(\n",
    "            warmup_epochs=warmup_epochs,\n",
    "            max_epochs=max_epochs,\n",
    "            warmup_start_lr=warmup_start_lr,\n",
    "            eta_min=eta_min,\n",
    "            last_epoch=last_epoch,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.scheduler_class = LinearWarmupCosineAnnealingLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from singer_identity.models.byol import TeacherStudentModel, Optimizer, Scheduler\n",
    "from singer_identity.model import IdentityEncoder, Projection, SiameseArm, MLP\n",
    "\n",
    "import copy\n",
    "\n",
    "class BYOL(TeacherStudentModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # module: nn.Module,\n",
    "        weight_callback,\n",
    "        optimizer: Optimizer,\n",
    "        backbone: dict = {},\n",
    "        projection: dict  = {},\n",
    "        predictor: dict = {},\n",
    "        feature_extractor: dict = {},\n",
    "        loss_fn: nn.Module = torch.nn.MSELoss(),\n",
    "        scheduler: Optional[Scheduler] = None,\n",
    "        normalize_projections: bool = True,\n",
    "        normalize_representations: bool = False,\n",
    "    ):\n",
    "        encoder = IdentityEncoder(feature_extractor=feature_extractor, encoder=backbone)\n",
    "        projection_layer = Projection(**projection)\n",
    "        predictor_layer = MLP(**copy.deepcopy(predictor))\n",
    "        module = SiameseArm(\n",
    "            encoder=encoder,\n",
    "            projector=projection_layer,\n",
    "            predictor=predictor_layer,\n",
    "            normalize_projections=normalize_projections,\n",
    "            normalize_representations=normalize_representations,\n",
    "        )\n",
    "        \n",
    "        super(BYOL, self).__init__(\n",
    "            module, loss_fn, weight_callback, optimizer, scheduler=scheduler\n",
    "        )\n",
    "        self.save_hyperparameters(ignore=[\"module\", \"loss_fn\"])\n",
    "\n",
    "    def shared_step(self, batch, step_name: str):\n",
    "        # print('started')\n",
    "        x1 = batch[\"clip1\"]\n",
    "        x2 = batch[\"clip2\"]\n",
    "\n",
    "        batch_size = x1.shape[0]\n",
    "\n",
    "        ys, zs, qs = self.student_network(x1)\n",
    "        with torch.no_grad():\n",
    "            yt, zt, qt = self.teacher_network(x2)\n",
    "        loss_12 = self.loss_fn(qs, zt)\n",
    "        # print(loss_12)\n",
    "\n",
    "        ys, zs, qs = self.student_network(x2)\n",
    "        with torch.no_grad():\n",
    "            yt, zt, qt = self.teacher_network(x1)\n",
    "        loss_21 = self.loss_fn(qs, zt)\n",
    "        # print(loss_21)\n",
    "\n",
    "        loss = (loss_12 + loss_21) / 2\n",
    "\n",
    "        self.log(\n",
    "            f\"loss/{step_name}\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        self.record_variables(y1=ys, z1=zs, y2=yt, z2=zt)\n",
    "\n",
    "        print(loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(\n",
    "            lr=3e-5,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-8,\n",
    "            weight_decay=1.5e-6,\n",
    "            amsgrad=False,\n",
    "        )(self.parameters())\n",
    "\n",
    "        scheduler = LinearWarmupCosineAnnealing(\n",
    "            warmup_epochs=10,\n",
    "            max_epochs=1000,\n",
    "            warmup_start_lr=0.0,\n",
    "            eta_min=0.0,\n",
    "            last_epoch=-1\n",
    "        )(optimizer)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"train\")\n",
    "    \n",
    "    def on_train_batch_end(self, outputs, batch, batch_idx):\n",
    "    # Your custom implementation here\n",
    "        print(f\"Epoch {self.current_epoch}: Loss {self.trainer.callback_metrics['loss/train']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = BYOL(\n",
    "        optimizer = {'class_path': 'singer_identity.models.byol.Adam', 'init_args': {'lr': 3e-5, 'weight_decay': 1.5e-6}},\n",
    "        feature_extractor = {'spec_layer': 'melspectogram', 'n_fft' : 2048, 'hop_length' : 512},\n",
    "        backbone = {'backbone' : \"efficientnet_b0\", 'pretrained' : True, 'embedding_dim': 1000},\n",
    "        scheduler = {'class_path': 'singer_identity.models.byol.LinearWarmupCosineAnnealing', 'init_args': {'warmup_epochs': 10,'max_epochs': 1000}},\n",
    "        predictor = {'dims': [128, 1024, 128], 'use_batchnorm': True},\n",
    "        weight_callback = {'class_path': 'singer_identity.callbacks.ma_updates.MAWeightUpdate', 'init_args': {'initial_tau': 0.99, 'max_epochs': 1000}}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "trainer = Trainer(max_epochs=31,max_steps = 1000, accelerator='cpu', num_nodes = 1, log_every_n_steps=5) \n",
    "\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "class YourModel(pl.LightningModule):\n",
    "    def __init__(self, model_cont, lr, weight_decay,num_classes, embeddings = False):\n",
    "        super().__init__()\n",
    "        self.model_cont = model_cont\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.criterion = CrossEntropyLoss()\n",
    "        self.training_losses = []\n",
    "        self.num_classes = num_classes\n",
    "        self.fc = nn.Linear(1000, self.num_classes)\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "        for param in self.model_cont.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model_cont(x)\n",
    "\n",
    "        if self.embeddings:\n",
    "            return out\n",
    "        else:\n",
    "            lastlayer = self.fc(out)\n",
    "            return lastlayer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # x, y = batch\n",
    "        x = batch['clip1']\n",
    "        y = batch['group_name']\n",
    "        singer_tensor = torch.tensor(y)\n",
    "        y_pred = self.forward(x)\n",
    "        # print(y_pred.shape)\n",
    "        loss = self.criterion(y_pred, singer_tensor)\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.training_losses.append(loss)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_loss = torch.stack(self.training_losses).mean()\n",
    "        print(f\"Epoch {self.current_epoch} - Training loss: {avg_loss:.4f}\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.fc.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        return optimizer\n",
    "    \n",
    "lr = 0.0004\n",
    "weight_decay = 1e-5\n",
    "\n",
    "test_model = YourModel(model, lr, weight_decay, 70)\n",
    "\n",
    "trainer = Trainer(max_epochs=31,accelerator='cpu')  \n",
    "\n",
    "trainer.fit(test_model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:24<00:00,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.958963282937365\n",
      "F1 Score: 0.9588954422107769\n",
      "Recall: 0.958963282937365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_true_labels = []\n",
    "all_pred_labels = []\n",
    "\n",
    "for batch in tqdm(train_loader):\n",
    "    audio_batch = batch['clip1']\n",
    "    audio_batch = audio_batch.float()\n",
    "    labels = batch['group_name']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = test_model(audio_batch)\n",
    "        y_pred = F.softmax(y_pred, dim=1)\n",
    "        predicted = torch.argmax(y_pred, 1)\n",
    "        # print(predicted)\n",
    "        # _, predicted = torch.max(y_pred.data, 1)\n",
    "\n",
    "    all_pred_labels.extend(list(predicted))\n",
    "    all_true_labels.extend(list(labels))\n",
    "\n",
    "accuracy = accuracy_score(all_true_labels, all_pred_labels)\n",
    "f1 = f1_score(all_true_labels, all_pred_labels, average='weighted')\n",
    "recall = recall_score(all_true_labels, all_pred_labels, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.92         7\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       1.00      0.86      0.92         7\n",
      "           3       1.00      1.00      1.00         6\n",
      "           4       1.00      1.00      1.00         7\n",
      "           5       1.00      1.00      1.00         7\n",
      "           6       0.86      0.86      0.86         7\n",
      "           7       1.00      1.00      1.00         6\n",
      "           8       1.00      1.00      1.00         6\n",
      "           9       1.00      1.00      1.00         7\n",
      "          10       1.00      1.00      1.00         7\n",
      "          11       0.88      1.00      0.93         7\n",
      "          12       0.78      1.00      0.88         7\n",
      "          13       1.00      1.00      1.00         5\n",
      "          14       1.00      0.86      0.92         7\n",
      "          15       1.00      1.00      1.00         6\n",
      "          16       1.00      1.00      1.00         7\n",
      "          17       0.86      1.00      0.92         6\n",
      "          18       0.88      1.00      0.93         7\n",
      "          19       0.86      1.00      0.92         6\n",
      "          20       1.00      1.00      1.00         7\n",
      "          21       1.00      1.00      1.00         7\n",
      "          22       1.00      1.00      1.00         7\n",
      "          23       1.00      1.00      1.00         7\n",
      "          24       1.00      1.00      1.00         6\n",
      "          25       0.75      0.86      0.80         7\n",
      "          26       1.00      1.00      1.00         6\n",
      "          27       1.00      0.71      0.83         7\n",
      "          28       1.00      1.00      1.00         6\n",
      "          29       1.00      1.00      1.00         6\n",
      "          30       1.00      1.00      1.00         7\n",
      "          31       0.88      1.00      0.93         7\n",
      "          32       1.00      1.00      1.00         7\n",
      "          33       1.00      1.00      1.00         7\n",
      "          34       1.00      1.00      1.00         7\n",
      "          35       1.00      0.86      0.92         7\n",
      "          36       0.88      1.00      0.93         7\n",
      "          37       1.00      0.86      0.92         7\n",
      "          38       1.00      1.00      1.00         6\n",
      "          39       1.00      1.00      1.00         7\n",
      "          40       0.88      1.00      0.93         7\n",
      "          41       1.00      1.00      1.00         6\n",
      "          42       1.00      1.00      1.00         6\n",
      "          43       1.00      0.86      0.92         7\n",
      "          44       1.00      1.00      1.00         6\n",
      "          45       0.86      0.86      0.86         7\n",
      "          46       1.00      1.00      1.00         7\n",
      "          47       1.00      0.86      0.92         7\n",
      "          48       0.83      0.71      0.77         7\n",
      "          49       1.00      1.00      1.00         7\n",
      "          50       0.75      0.86      0.80         7\n",
      "          51       1.00      1.00      1.00         7\n",
      "          52       1.00      1.00      1.00         5\n",
      "          53       0.86      0.86      0.86         7\n",
      "          54       1.00      1.00      1.00         7\n",
      "          55       1.00      0.86      0.92         7\n",
      "          56       1.00      0.86      0.92         7\n",
      "          57       0.88      1.00      0.93         7\n",
      "          58       1.00      1.00      1.00         7\n",
      "          59       1.00      1.00      1.00         7\n",
      "          60       1.00      0.86      0.92         7\n",
      "          61       1.00      1.00      1.00         7\n",
      "          62       0.88      1.00      0.93         7\n",
      "          63       1.00      1.00      1.00         7\n",
      "          64       1.00      1.00      1.00         7\n",
      "          65       1.00      1.00      1.00         7\n",
      "          66       1.00      1.00      1.00         6\n",
      "          67       1.00      1.00      1.00         6\n",
      "          68       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           0.96       463\n",
      "   macro avg       0.96      0.96      0.96       463\n",
      "weighted avg       0.96      0.96      0.96       463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(all_true_labels, all_pred_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A.S. MURALI': 0, 'AFROZ BANO': 1, 'AJAY CHAKRAVARTY': 2, 'ASLAM SABRI & PARTY': 3, 'BAKUL CHATTOPADHYAY': 4, 'BALDEV SHARAN NARANG': 5, 'BHAI NARMAL SINGH': 6, 'Bal Gandharva': 7, 'Chorus (Students)': 8, 'DINA NATH MISHRA': 9, 'DR. BASAVARAJ RAJGURU': 10, 'DR. NIRMALA SUNDARA RAJAN': 11, 'DR. NIVEDITA SINGH': 12, 'DR. PANTULA RAMA': 13, 'DWARAM DURGA PRASAD RAO & DWARAM SATYANARAYANA RAO': 14, 'GAYATRY GIRISH': 15, 'K. SARASWATHI VIDYARTHI': 16, 'KOYAL DAS GUPTA': 17, 'LOU MAJAW (SOLO)': 18, 'M K SHANKARA NAMBOODRI': 19, 'M. K. PRANESH': 20, 'M. VENKATESH KUMAR.': 21, 'M.K.SWAMINATH PILLAI & D.MURUGESA PILLAI': 22, 'M.M. DANDAPANI DESIKAR': 23, 'M.S. SHEELA': 24, 'MALA SHRI PRASAD': 25, 'MALLADI SURI BABU': 26, 'MANDIRA LAHIRI': 27, 'MANJUSHA KULKARNI PATIL': 28, 'MEETA  PANDIT': 29, 'MOHINDER SINGH': 30, 'MUJAHID HUSSAIN KHAN': 31, 'NAMAGIRIPETTAI .K.KRISHNAN': 32, 'PARMESHWAR HEGDE': 33, 'PITTUKULI MURUGADAS': 34, 'PT. DHARAM NATH MISHRA': 35, 'PT. MADHAV INGALE': 36, 'PT. SIDDARAMA JAMBALADINNI': 37, 'R. SANTANA GOPALAN': 38, 'RAJAN MISHRA & PT. SAJAN MISHRA': 39, 'RAJINDAR SINGH': 40, 'RAJSHREE PATHAK': 41, 'RAMNATH AHEERVAAR': 42, 'RITA GANGULI': 43, 'RITESH & RAJNISH MISHRA': 44, 'S. BALAMBAL': 45, 'S. RUKMINI': 46, 'S. SOWMYA': 47, 'SAVITA DEVI': 48, 'SINGH BANDHU': 49, 'SITARAM SINGH': 50, 'SMT. AVANTIKA CHAKRAVARTI': 51, 'SMT. GEETHA RAJA': 52, 'SMT. KALPANA JHOKARKAR': 53, 'SMT. KAMATCHI KUPPUSWAMI': 54, 'SMT. NITHYASREE MAHADEVAN': 55, 'SMT. VIDYA KATGADE': 56, 'SREEVALSAN J. MENON': 57, 'SUBHRA GUHA': 58, 'T. R. Mahalingam': 59, 'TIRUVAIYARU BV JAYASHREE': 60, 'TUSHAR DUTTA': 61, 'USTAD GHULAM SADIQ KHAN': 62, 'USTAD SALEEM IQBAL': 63, 'VALLAM KRISHNASWAMI PILLAI & K.P.GOVINDARAJA PILLAI': 64, 'VASANTHI KRISHNA RAO': 65, 'VIDUSHI NEELA RAMGOPAL': 66, 'VIDUSHI SUMITRA GUHA': 67, 'YASHPAL': 68}\n"
     ]
    }
   ],
   "source": [
    "data_module_test = CSVBaseDataModule(\n",
    "    csv_file=csv_file,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    nr_samples=nr_samples,\n",
    "    normalize=normalize,\n",
    "    augmentations=augmentations,\n",
    "    transform_override=transform_override,\n",
    "    batch_sampling_mode=batch_sampling_mode,\n",
    "    sr=sr,\n",
    "    multi_epoch=multi_epoch,\n",
    "    phase = 'TESTING'\n",
    ")\n",
    "\n",
    "data_module_test.setup()\n",
    "test_loader = data_module_test.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:22<00:00,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.37109375\n",
      "F1 Score: 0.3550769608938767\n",
      "Recall: 0.37109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_true_labels = []\n",
    "all_pred_labels = []\n",
    "\n",
    "for batch in tqdm(test_loader):\n",
    "    audio_batch = batch['clip1']\n",
    "    audio_batch = audio_batch.float()\n",
    "    labels = batch['group_name']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = test_model(audio_batch)\n",
    "        y_pred = F.softmax(y_pred, dim=1)\n",
    "        predicted = torch.argmax(y_pred, 1)\n",
    "        # print(predicted)\n",
    "        # _, predicted = torch.max(y_pred.data, 1)\n",
    "\n",
    "    all_pred_labels.extend(list(predicted))\n",
    "    all_true_labels.extend(list(labels))\n",
    "\n",
    "accuracy = accuracy_score(all_true_labels, all_pred_labels)\n",
    "f1 = f1_score(all_true_labels, all_pred_labels, average='weighted')\n",
    "recall = recall_score(all_true_labels, all_pred_labels, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.25      0.33         4\n",
      "           1       0.33      0.33      0.33         3\n",
      "           2       0.00      0.00      0.00         5\n",
      "           3       0.25      1.00      0.40         1\n",
      "           4       0.25      0.33      0.29         3\n",
      "           5       0.00      0.00      0.00         2\n",
      "           6       0.75      0.43      0.55         7\n",
      "           7       0.60      0.14      0.22        22\n",
      "           8       0.86      0.38      0.52        16\n",
      "           9       0.00      0.00      0.00         4\n",
      "          10       0.25      0.11      0.15         9\n",
      "          11       0.41      0.64      0.50        11\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       0.50      0.50      0.50         2\n",
      "          14       0.50      0.67      0.57         3\n",
      "          15       0.00      0.00      0.00         1\n",
      "          16       0.33      1.00      0.50         1\n",
      "          17       0.60      1.00      0.75         3\n",
      "          18       0.00      0.00      0.00         1\n",
      "          19       0.60      1.00      0.75         3\n",
      "          20       0.50      0.29      0.36         7\n",
      "          21       0.50      0.22      0.31         9\n",
      "          22       0.00      0.00      0.00         1\n",
      "          23       0.62      0.83      0.71         6\n",
      "          24       0.50      0.67      0.57         3\n",
      "          25       0.00      0.00      0.00         1\n",
      "          26       0.33      0.50      0.40         2\n",
      "          27       0.11      0.50      0.18         2\n",
      "          28       0.00      0.00      0.00         2\n",
      "          29       0.00      0.00      0.00         6\n",
      "          30       1.00      0.25      0.40         4\n",
      "          31       0.17      0.50      0.25         2\n",
      "          32       0.60      0.75      0.67         4\n",
      "          33       0.33      0.33      0.33         3\n",
      "          34       1.00      1.00      1.00         2\n",
      "          35       0.00      0.00      0.00         5\n",
      "          36       0.20      1.00      0.33         1\n",
      "          37       0.29      0.22      0.25         9\n",
      "          38       0.00      0.00      0.00         2\n",
      "          39       0.00      0.00      0.00         2\n",
      "          40       0.00      0.00      0.00         1\n",
      "          41       0.33      1.00      0.50         1\n",
      "          42       1.00      1.00      1.00         1\n",
      "          43       0.00      0.00      0.00         2\n",
      "          44       0.20      0.50      0.29         2\n",
      "          45       0.33      1.00      0.50         1\n",
      "          46       0.00      0.00      0.00         2\n",
      "          47       0.83      0.56      0.67         9\n",
      "          48       0.18      0.67      0.29         3\n",
      "          49       0.67      0.50      0.57        12\n",
      "          50       0.62      1.00      0.77         5\n",
      "          51       0.50      0.67      0.57         3\n",
      "          52       0.00      0.00      0.00         3\n",
      "          53       0.00      0.00      0.00         1\n",
      "          54       1.00      1.00      1.00         1\n",
      "          55       0.00      0.00      0.00         5\n",
      "          56       0.75      0.75      0.75         4\n",
      "          57       0.00      0.00      0.00         1\n",
      "          58       0.25      0.33      0.29         6\n",
      "          59       0.25      1.00      0.40         1\n",
      "          60       0.25      1.00      0.40         1\n",
      "          61       0.33      0.33      0.33         3\n",
      "          62       0.00      0.00      0.00         1\n",
      "          63       0.25      1.00      0.40         2\n",
      "          64       1.00      1.00      1.00         1\n",
      "          65       1.00      0.20      0.33         5\n",
      "          66       0.00      0.00      0.00         2\n",
      "          67       0.00      0.00      0.00         1\n",
      "          68       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.37       256\n",
      "   macro avg       0.33      0.41      0.32       256\n",
      "weighted avg       0.44      0.37      0.36       256\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(all_true_labels, all_pred_labels)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
