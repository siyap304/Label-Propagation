{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import (\n",
    "    AddGaussianNoise,\n",
    "    TimeStretch,\n",
    "    Shift,\n",
    "    Gain,\n",
    "    TanhDistortion,\n",
    "    SevenBandParametricEQ,\n",
    "    SevenBandParametricEQ,\n",
    "    TimeMask,\n",
    "    ApplyImpulseResponse,\n",
    ")\n",
    "\n",
    "from audiomentations.core.transforms_interface import BaseWaveformTransform\n",
    "import parselmouth\n",
    "import random\n",
    "\n",
    "# import librosa\n",
    "import numpy as np\n",
    "\n",
    "# from random import random\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "PRAAT_CHANGEGENDER_PITCHMEDIAN_DEFAULT = 0.0\n",
    "PRAAT_CHANGEGENDER_FORMANTSHIFTRATIO_DEFAULT = 1.0\n",
    "PRAAT_CHANGEGENDER_PITCHSHIFTRATIO_DEFAULT = 1.0\n",
    "PRAAT_CHANGEGENDER_PITCHRANGERATIO_DEFAULT = 1.0\n",
    "PRAAT_CHANGEGENDER_DURATIONFACTOR_DEFAULT = 1.0\n",
    "\n",
    "\n",
    "def aug(signal, augmentations_dict, override=False, sample_rate=44100):\n",
    "    \"\"\"Main augmentation function\"\"\"\n",
    "    # Augment the signal\n",
    "\n",
    "    sig_aug = signal\n",
    "    # If its not false it is a dict containing manual transforms to apply\n",
    "    if override is not False:\n",
    "        for transform in override.values():\n",
    "            sig_aug = transform(sig_aug)\n",
    "        return sig_aug\n",
    "    if augmentations_dict is False:\n",
    "        return signal\n",
    "    transforms = aug_factory(augmentations_dict)\n",
    "\n",
    "    for transform in transforms:\n",
    "        sig_aug = transform(sig_aug, sample_rate=sample_rate)\n",
    "    return sig_aug\n",
    "\n",
    "\n",
    "def aug_factory(augmentation):\n",
    "    augmentations = []\n",
    "\n",
    "    if augmentation.get(\"gaussian_noise\", 0):\n",
    "        augmentations.append(\n",
    "            AddGaussianNoise(\n",
    "                min_amplitude=0.001,\n",
    "                max_amplitude=0.05,\n",
    "                p=augmentation[\"gaussian_noise\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if augmentation.get(\"time_stretch\", 0):\n",
    "        augmentations.append(\n",
    "            TimeStretch(min_rate=0.8, max_rate=1.2, p=augmentation[\"time_stretch\"])\n",
    "        )\n",
    "\n",
    "    if augmentation.get(\"pitch_shift_naive\", 0):\n",
    "        # augmentations.append(PitchShift(min_semitones=-3, max_semitones=3,\n",
    "        # p=augmentation[\"time_stretch\"]))\n",
    "        # n_steps = random.choice((-4, 4, 3, -3))\n",
    "        # ps = lambda x, sample_rate=44100: np.cast[\"float32\"](\n",
    "        #     librosa.effects.pitch_shift(x, sr=sample_rate, n_steps=n_steps)\n",
    "        # )\n",
    "        # augmentations.append(ps)\n",
    "        pass\n",
    "\n",
    "    if augmentation.get(\"formant_shift_parselmouth_prob\", 0):\n",
    "        # if augmentation.get(\"formant_shift_parselmouth\", 0):\n",
    "        augmentations.append(\n",
    "            FormantShiftParselmouth(\n",
    "                augmentation[\"formant_shift_parselmouth\"],\n",
    "                p=augmentation[\"formant_shift_parselmouth_prob\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if augmentation.get(\"pitch_shift_parselmouth_prob\", 0):\n",
    "        if augmentation.get(\"pitch_shift_parselmouth\", 0):\n",
    "            pitch_shift_ratio = augmentation[\"pitch_shift_parselmouth\"]\n",
    "        else:\n",
    "            pitch_shift_ratio = 1\n",
    "\n",
    "        if augmentation.get(\"pitch_range_parselmouth\", 0):\n",
    "            pitch_range_ratio = augmentation[\"pitch_range_parselmouth\"]\n",
    "        else:\n",
    "            pitch_range_ratio = 1\n",
    "\n",
    "        augmentations.append(\n",
    "            PitchShiftParselmouth(\n",
    "                pitch_shift_ratio,\n",
    "                pitch_range_ratio,\n",
    "                p=augmentation[\"pitch_shift_parselmouth_prob\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if augmentation.get(\"shift\", 0):\n",
    "        augmentations.append(\n",
    "            Shift(\n",
    "                min_fraction=-0.05,\n",
    "                max_fraction=0.2,\n",
    "                p=augmentation[\"shift\"],\n",
    "                rollover=True,\n",
    "                fade=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if augmentation.get(\"gain\", 0):\n",
    "        augmentations.append(\n",
    "            Gain(min_gain_in_db=-6, max_gain_in_db=0, p=augmentation[\"gain\"])\n",
    "        )\n",
    "\n",
    "    if augmentation.get(\"parametric_eq\", 0):\n",
    "        augmentations.append(\n",
    "            SevenBandParametricEQ(\n",
    "                min_gain_db=-2, max_gain_db=1, p=augmentation[\"parametric_eq\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if augmentation.get(\"tanh_distortion\", 0):\n",
    "        augmentations.append(\n",
    "            TanhDistortion(\n",
    "                min_distortion=0.1,\n",
    "                max_distortion=0.2,\n",
    "                p=augmentation[\"tanh_distortion\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if augmentation.get(\"time_mask\", 0):\n",
    "        augmentations.append(TimeMask(max_band_part=1 / 8, p=augmentation[\"time_mask\"]))\n",
    "\n",
    "    if augmentation.get(\"reverb\", 0):\n",
    "        ir_path = augmentation[\"reverb_path\"]\n",
    "        warnings.filterwarnings(\n",
    "            \"ignore\", message=\".* had to be resampled from 16000 hz to 44100 hz.*\"\n",
    "        )\n",
    "        augmentations.append(ApplyImpulseResponse(ir_path, p=augmentation[\"reverb\"]))\n",
    "\n",
    "    return augmentations\n",
    "\n",
    "\n",
    "class PitchShiftParselmouth(BaseWaveformTransform):\n",
    "    \"\"\"Pitch shift the sound up or down without changing the tempo\"\"\"\n",
    "\n",
    "    def __init__(self, pitch_ratio=1.4, range_ratio=1.3, p=0.5):\n",
    "        super().__init__(p)\n",
    "\n",
    "        self.range_ratio = range_ratio\n",
    "        self.init_range = 1\n",
    "        self.enable_reciprocal = True\n",
    "        if type(pitch_ratio) is list:\n",
    "            self.init_range = float(pitch_ratio[0])\n",
    "            pitch_ratio = float(pitch_ratio[1])\n",
    "            # self.enable_reciprocal = True\n",
    "\n",
    "        self.pitch_ratio = pitch_ratio\n",
    "\n",
    "    def randomize_parameters(self, samples, sample_rate):\n",
    "        super().randomize_parameters(samples, sample_rate)\n",
    "        if self.parameters[\"should_apply\"]:\n",
    "            self.parameters[\"pitch_shift_ratio\"] = random.uniform(\n",
    "                self.init_range, self.pitch_ratio\n",
    "            )\n",
    "\n",
    "            if self.enable_reciprocal:\n",
    "                use_reciprocal = random.uniform(-1, 1) > 0\n",
    "                if use_reciprocal:\n",
    "                    self.parameters[\"pitch_shift_ratio\"] = (\n",
    "                        1 / self.parameters[\"pitch_shift_ratio\"]\n",
    "                    )\n",
    "\n",
    "            self.parameters[\"pitch_range_ratio\"] = random.uniform(1, self.range_ratio)\n",
    "\n",
    "            use_reciprocal = random.uniform(-1, 1) > 0\n",
    "            if use_reciprocal:\n",
    "                self.parameters[\"pitch_range_ratio\"] = (\n",
    "                    1 / self.parameters[\"pitch_range_ratio\"]\n",
    "                )\n",
    "\n",
    "    def apply(self, samples, sample_rate):\n",
    "        # Add a check to see if samples is numpy array\n",
    "        if not isinstance(samples, np.ndarray):\n",
    "            samples = np.array(samples)\n",
    "            print(\"samples is not numpy array, converting to numpy array\")\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                action=\"ignore\",\n",
    "                category=parselmouth.PraatWarning,\n",
    "                message=\"This application uses RandomPool, which is BROKEN in older releases\",\n",
    "            )\n",
    "\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            pitch_shifted_samples = apply_formant_and_pitch_shift(\n",
    "                wav_to_Sound(samples, sampling_frequency=sample_rate),\n",
    "                pitch_shift_ratio=self.parameters[\"pitch_shift_ratio\"],\n",
    "                pitch_range_ratio=self.parameters[\"pitch_range_ratio\"],\n",
    "                duration_factor=1.0,\n",
    "            )\n",
    "        return np.squeeze(np.cast[\"float32\"](pitch_shifted_samples.values))\n",
    "\n",
    "\n",
    "class FormantShiftParselmouth(BaseWaveformTransform):\n",
    "    \"\"\"Formant shift using parselmouth\"\"\"\n",
    "\n",
    "    def __init__(self, formant_shift=1.4, p=0.5):\n",
    "        super().__init__(p)\n",
    "        self.init_range = 1\n",
    "        self.enable_reciprocal = True\n",
    "        if type(formant_shift) is list:\n",
    "            self.init_range = float(formant_shift[0])\n",
    "            formant_shift = float(formant_shift[1])\n",
    "            self.enable_reciprocal = True\n",
    "\n",
    "        self.formant_shift = formant_shift\n",
    "\n",
    "    def randomize_parameters(self, samples, sample_rate):\n",
    "        super().randomize_parameters(samples, sample_rate)\n",
    "\n",
    "        if self.parameters[\"should_apply\"]:\n",
    "            self.parameters[\"formant_shift_parselmouth\"] = random.uniform(\n",
    "                self.init_range, self.formant_shift\n",
    "            )\n",
    "\n",
    "            if self.enable_reciprocal:\n",
    "                use_reciprocal = random.uniform(-1, 1) > 0\n",
    "                if use_reciprocal:\n",
    "                    self.parameters[\"formant_shift_parselmouth\"] = (\n",
    "                        1 / self.parameters[\"formant_shift_parselmouth\"]\n",
    "                    )\n",
    "\n",
    "    def apply(self, samples, sample_rate):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                action=\"ignore\",\n",
    "                category=parselmouth.PraatWarning,\n",
    "                message=\"This application uses RandomPool, which is BROKEN in older releases\",\n",
    "            )\n",
    "\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            formant_shifted_samples = apply_formant_and_pitch_shift(\n",
    "                wav_to_Sound(samples, sampling_frequency=sample_rate),\n",
    "                formant_shift_ratio=self.parameters[\"formant_shift_parselmouth\"],\n",
    "                duration_factor=1.0,\n",
    "            )\n",
    "        return np.squeeze(np.cast[\"float32\"](formant_shifted_samples.values))\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\" Parselmouth utils for pitch and formant shifting. \n",
    "    Part of the code is adapted from https://github.com/dhchoi99/NANSY\\\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def wav_to_Sound(wav, sampling_frequency: int = 44100) -> parselmouth.Sound:\n",
    "    r\"\"\" load wav file to parselmouth Sound file\n",
    "    # __init__(self: parselmouth.Sound, other: parselmouth.Sound) -> None \\\n",
    "    # __init__(self: parselmouth.Sound, values: numpy.ndarray[numpy.float64], \n",
    "            sampling_frequency: Positive[float] = 44100.0, start_time: float = 0.0) -> None \\\n",
    "    # __init__(self: parselmouth.Sound, file_path: str) -> None\n",
    "    returns:\n",
    "        sound: parselmouth.Sound\n",
    "    \"\"\"\n",
    "    if isinstance(wav, parselmouth.Sound):\n",
    "        sound = wav\n",
    "    elif isinstance(wav, np.ndarray):\n",
    "        sound = parselmouth.Sound(wav, sampling_frequency=sampling_frequency)\n",
    "    elif isinstance(wav, list):\n",
    "        wav_np = np.asarray(wav)\n",
    "        sound = parselmouth.Sound(\n",
    "            np.asarray(wav_np), sampling_frequency=sampling_frequency\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return sound\n",
    "\n",
    "\n",
    "def get_pitch_median(wav, sr: int = None):\n",
    "    sound = wav_to_Sound(wav, sr)\n",
    "    pitch = None\n",
    "    pitch_median = PRAAT_CHANGEGENDER_PITCHMEDIAN_DEFAULT\n",
    "\n",
    "    try:\n",
    "        pitch = parselmouth.praat.call(sound, \"To Pitch\", 0.8 / 75, 75, 600)\n",
    "        pitch_median = parselmouth.praat.call(\n",
    "            pitch, \"Get quantile\", 0.0, 0.0, 0.5, \"Hertz\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        pass\n",
    "\n",
    "    return pitch, pitch_median\n",
    "\n",
    "\n",
    "def change_gender(\n",
    "    sound,\n",
    "    pitch=None,\n",
    "    formant_shift_ratio: float = PRAAT_CHANGEGENDER_FORMANTSHIFTRATIO_DEFAULT,\n",
    "    new_pitch_median: float = PRAAT_CHANGEGENDER_PITCHMEDIAN_DEFAULT,\n",
    "    pitch_range_ratio: float = PRAAT_CHANGEGENDER_PITCHRANGERATIO_DEFAULT,\n",
    "    duration_factor: float = PRAAT_CHANGEGENDER_DURATIONFACTOR_DEFAULT,\n",
    ") -> parselmouth.Sound:\n",
    "    try:\n",
    "        if pitch is None:\n",
    "            new_sound = parselmouth.praat.call(\n",
    "                sound,\n",
    "                \"Change gender\",\n",
    "                75,\n",
    "                600,\n",
    "                formant_shift_ratio,\n",
    "                new_pitch_median,\n",
    "                pitch_range_ratio,\n",
    "                duration_factor,\n",
    "            )\n",
    "        else:\n",
    "            new_sound = parselmouth.praat.call(\n",
    "                (sound, pitch),\n",
    "                \"Change gender\",\n",
    "                formant_shift_ratio,\n",
    "                new_pitch_median,\n",
    "                pitch_range_ratio,\n",
    "                duration_factor,\n",
    "            )\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "    return new_sound\n",
    "\n",
    "\n",
    "def apply_formant_and_pitch_shift(\n",
    "    sound: parselmouth.Sound,\n",
    "    formant_shift_ratio: float = PRAAT_CHANGEGENDER_FORMANTSHIFTRATIO_DEFAULT,\n",
    "    pitch_shift_ratio: float = PRAAT_CHANGEGENDER_PITCHSHIFTRATIO_DEFAULT,\n",
    "    pitch_range_ratio: float = PRAAT_CHANGEGENDER_PITCHRANGERATIO_DEFAULT,\n",
    "    duration_factor: float = PRAAT_CHANGEGENDER_DURATIONFACTOR_DEFAULT,\n",
    ") -> parselmouth.Sound:\n",
    "    \"\"\"uses praat 'Change Gender' backend to manipulate pitch and formant\n",
    "    'Change Gender' function: praat -> Sound Object -> Convert -> Change Gender\n",
    "    see Help of Praat for more details\n",
    "    # https://github.com/YannickJadoul/Parselmouth/issues/25#issuecomment-608632887 might help\n",
    "    \"\"\"\n",
    "\n",
    "    # pitch = sound.to_pitch()\n",
    "    pitch = None\n",
    "    new_pitch_median = PRAAT_CHANGEGENDER_PITCHMEDIAN_DEFAULT\n",
    "    if pitch_shift_ratio != 1.0:\n",
    "        try:\n",
    "            pitch, pitch_median = get_pitch_median(sound, None)\n",
    "            new_pitch_median = pitch_median * pitch_shift_ratio\n",
    "\n",
    "            # https://github.com/praat/praat/issues/1926#issuecomment-974909408\n",
    "            pitch_minimum = parselmouth.praat.call(\n",
    "                pitch, \"Get minimum\", 0.0, 0.0, \"Hertz\", \"Parabolic\"\n",
    "            )\n",
    "            newMedian = pitch_median * pitch_shift_ratio\n",
    "            scaledMinimum = pitch_minimum * pitch_shift_ratio\n",
    "            resultingMinimum = (\n",
    "                newMedian + (scaledMinimum - newMedian) * pitch_range_ratio\n",
    "            )\n",
    "            if resultingMinimum < 0:\n",
    "                new_pitch_median = PRAAT_CHANGEGENDER_PITCHMEDIAN_DEFAULT\n",
    "                pitch_range_ratio = PRAAT_CHANGEGENDER_PITCHRANGERATIO_DEFAULT\n",
    "\n",
    "            if math.isnan(new_pitch_median):\n",
    "                new_pitch_median = PRAAT_CHANGEGENDER_PITCHMEDIAN_DEFAULT\n",
    "                pitch_range_ratio = PRAAT_CHANGEGENDER_PITCHRANGERATIO_DEFAULT\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    new_sound = change_gender(\n",
    "        sound,\n",
    "        pitch,\n",
    "        formant_shift_ratio,\n",
    "        new_pitch_median,\n",
    "        pitch_range_ratio,\n",
    "        duration_factor,\n",
    "    )\n",
    "\n",
    "    return new_sound\n",
    "\n",
    "\n",
    "def semitones_to_ratio(x):\n",
    "    return 2 ** (x / 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVBaseDictDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_file,\n",
    "        nr_samples,\n",
    "        normalize=True,\n",
    "        augmentations={},\n",
    "        transform_override=False,\n",
    "        batch_sampling_mode=\"sample_clips\",\n",
    "        sr=44100,\n",
    "        multi_epoch=1,\n",
    "    ):\n",
    "        self.csv_file = csv_file\n",
    "        self.nr_samples = nr_samples\n",
    "        self.normalize = normalize\n",
    "        self.augmentations = augmentations\n",
    "        self.transform_override = transform_override\n",
    "        self.batch_sampling_mode = batch_sampling_mode\n",
    "        self.sr = sr\n",
    "        self.multi_epoch = multi_epoch\n",
    "        self.groups, self.file_list, self.labels = self._prepare_groups_from_csv(csv_file)\n",
    "        self.group_names = list(self.groups.keys())\n",
    "\n",
    "    def _prepare_groups_from_csv(self, df):\n",
    "        groups = {}\n",
    "        file_list = []\n",
    "        labels = []\n",
    "        df = df[df.iloc[:,1] != 'Unlabelled']\n",
    "        df = df[df.iloc[:,1] != 'Other']\n",
    "        df = df[df.iloc[:,-1] != 'Speech']\n",
    "        df = df.reset_index(drop = True)\n",
    "        # print(df.shape)\n",
    "        # print(df)\n",
    "        singers = np.unique(df['Singer'])\n",
    "        singer_to_idx = {singer: idx for idx, singer in enumerate(singers)}\n",
    "        print(singer_to_idx)\n",
    "        for _, row in df.iterrows():              \n",
    "            group_name = singer_to_idx[row['Singer']]\n",
    "            if group_name not in groups:\n",
    "                groups[group_name] = []\n",
    "            file_path = row['Audio_Path']\n",
    "            groups[group_name].append(file_path)\n",
    "            file_list.append(file_path)\n",
    "            labels.append(group_name)\n",
    "        return groups, file_list, labels\n",
    "\n",
    "    def get_fragment(self, file):\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(file)\n",
    "            # print(waveform.shape)\n",
    "\n",
    "            if sr != self.sr:\n",
    "                resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sr)\n",
    "                waveform = resampler(waveform)\n",
    "\n",
    "            if waveform.size(1) > self.nr_samples:\n",
    "                waveform = waveform[:, :self.nr_samples]\n",
    "            else:\n",
    "                padding = self.nr_samples - waveform.size(1)\n",
    "                waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "\n",
    "            if self.normalize:\n",
    "                waveform = (waveform - waveform.mean()) / waveform.std()\n",
    "\n",
    "            return waveform\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading fragment from {file}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list) * self.multi_epoch\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx % len(self.file_list)\n",
    "        file = self.file_list[idx]\n",
    "        label = self.labels[idx]\n",
    "        fragment = self.get_fragment(file)\n",
    "        if fragment is None:\n",
    "            raise ValueError(f\"Failed to load audio fragment from {file}\")\n",
    "        fragment1 = aug(\n",
    "            np.cast[\"float32\"](fragment),\n",
    "            self.augmentations,\n",
    "            sample_rate=self.sr,\n",
    "        )\n",
    "        # print('fragment',fragment.shape)\n",
    "        # print('fragment_aug', fragment1.shape)\n",
    "        return {\"clip1\": fragment, \"clip2\": fragment1, \"group_name\": label, 'audio_files': file}\n",
    "\n",
    "    def _count_files_in_dict(self, d):\n",
    "        return sum(len(files) for files in d.values())\n",
    "\n",
    "    def _count_elements_in_dict_split(self, d):\n",
    "        return len(d)\n",
    "\n",
    "    def _merge_groups(self, groups):\n",
    "        merged_groups = {}\n",
    "        for k, v in groups.items():\n",
    "            if k not in merged_groups:\n",
    "                merged_groups[k] = []\n",
    "            merged_groups[k].extend(v)\n",
    "        return merged_groups\n",
    "\n",
    "    def _print_dataset_files_info(self):\n",
    "        total_files = self._count_files_in_dict(self.groups)\n",
    "        print(f\"Total files: {total_files}\")\n",
    "        for group, files in self.groups.items():\n",
    "            print(f\"Group: {group}, Files: {len(files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVBaseDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_file,\n",
    "        batch_size=32,\n",
    "        num_workers=0, \n",
    "        nr_samples=64000,\n",
    "        normalize=True,\n",
    "        augmentations={},\n",
    "        transform_override=False,\n",
    "        batch_sampling_mode=\"sample_clips\",\n",
    "        sr=16000,\n",
    "        multi_epoch=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.csv_file = csv_file\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.nr_samples = nr_samples\n",
    "        self.normalize = normalize\n",
    "        self.augmentations = augmentations\n",
    "        self.transform_override = transform_override\n",
    "        self.batch_sampling_mode = batch_sampling_mode\n",
    "        self.sr = sr\n",
    "        self.multi_epoch = multi_epoch\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Any data downloading or preparation logic should be placed here\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.dataset = CSVBaseDictDataset(\n",
    "            csv_file=self.csv_file,\n",
    "            nr_samples=self.nr_samples,\n",
    "            normalize=self.normalize,\n",
    "            augmentations=self.augmentations,\n",
    "            transform_override=self.transform_override,\n",
    "            batch_sampling_mode=self.batch_sampling_mode,\n",
    "            sr=self.sr,\n",
    "            multi_epoch=self.multi_epoch,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return None\n",
    "\n",
    "    def process_dataset_dirs(self):\n",
    "        # Custom logic for processing dataset directories if required\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file = '/home/surge_siya/ALL CSV/Panns_CSV.csv'\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "# nr_samples = 160000\n",
    "nr_samples = 480000\n",
    "normalize = True\n",
    "augmentations = {\n",
    "    \"enable\": True,\n",
    "    \"gaussian_noise\": 0.5,\n",
    "    \"pitch_shift_naive\": 0,\n",
    "    \"time_stretch\": 0,\n",
    "    \"gain\": 0.5,\n",
    "    \"shift\": 0,\n",
    "    \"parametric_eq\": 0,\n",
    "    \"tanh_distortion\": 0,\n",
    "    \"time_mask\": 0.5,\n",
    "    # \"formant_shift_parselmouth\": 0,\n",
    "    # \"pitch_shift_parselmouth\": [1, 1.3],\n",
    "    # \"pitch_range_parselmouth\": 1.5,\n",
    "    # \"pitch_shift_parselmouth_prob\": 0.5\n",
    "}\n",
    "transform_override = False\n",
    "batch_sampling_mode = \"sample_clips\"\n",
    "sr = 16000\n",
    "multi_epoch = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models, network_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Union, Callable, List, Optional\n",
    "from torchvision.models import efficientnet_b0, efficientnet_b4\n",
    "import torchvision.transforms as vt\n",
    "\n",
    "\n",
    "def get_vision_backbone(\n",
    "    vismod=\"efficientnet_b0\", num_classes=1000, pretrained=False, **kwargs\n",
    "):\n",
    "    if vismod == \"efficientnet_b0\":\n",
    "        return efficientnet_b0(pretrained=pretrained, num_classes=num_classes, **kwargs)\n",
    "    elif vismod == \"efficientnet_b4\":\n",
    "        return efficientnet_b4(pretrained=pretrained, num_classes=num_classes, **kwargs)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Grey2Rgb(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.normalize = vt.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        batch_size, freq_bins, times = data.shape\n",
    "        data /= data.max()\n",
    "        data = data.unsqueeze(1).expand(batch_size, 3, freq_bins, times)\n",
    "        data = self.normalize(data)\n",
    "        return data\n",
    "\n",
    "\n",
    "class LogScale(nn.Module):\n",
    "    def forward(self, data):\n",
    "        # eps = 1e-8\n",
    "        eps = torch.tensor(1e-8, device=data.device)\n",
    "        return torch.log(data + eps)\n",
    "\n",
    "\n",
    "class Aggregator(nn.Module):\n",
    "    \"\"\"Aggregates (in time) a list of features\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.aggregation = nn.Sequential(nn.AdaptiveAvgPool1d(1), nn.Flatten(1))\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            outputs_feature: torch.Tensor of shape(B x C x t)\n",
    "        \"\"\"\n",
    "        if isinstance(features, list):\n",
    "            output_feature = [self.aggregation(feature) for feature in features]\n",
    "        else:\n",
    "            output_feature = self.aggregation(features)\n",
    "        return output_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.error\n",
    "import pathlib\n",
    "import logging\n",
    "from enum import Enum\n",
    "import huggingface_hub\n",
    "from typing import Union\n",
    "from collections import namedtuple\n",
    "from requests.exceptions import HTTPError\n",
    "import hashlib\n",
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _missing_ok_unlink(path):\n",
    "    # missing_ok=True was added to Path.unlink() in Python 3.8\n",
    "    # This does the same.\n",
    "    try:\n",
    "        path.unlink()\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "\n",
    "class FetchFrom(Enum):\n",
    "    \"\"\"Designator where to fetch models/audios from.\n",
    "\n",
    "    Note: HuggingFace repository sources and local folder sources may be confused if their source type is undefined.\n",
    "    \"\"\"\n",
    "\n",
    "    LOCAL = 1\n",
    "    HUGGING_FACE = 2\n",
    "    URI = 3\n",
    "\n",
    "\n",
    "# For easier use\n",
    "FetchSource = namedtuple(\"FetchSource\", [\"FetchFrom\", \"path\"])\n",
    "FetchSource.__doc__ = \"\"\"NamedTuple describing a source path and how to fetch it\"\"\"\n",
    "FetchSource.__hash__ = lambda self: hash(self.path)\n",
    "FetchSource.encode = lambda self, *args, **kwargs: \"_\".join(\n",
    "    (str(self.path), str(self.FetchFrom))\n",
    ").encode(*args, **kwargs)\n",
    "# FetchSource.__str__ = lambda self: str(self.path)\n",
    "\n",
    "\n",
    "def fetch(\n",
    "    filename,\n",
    "    source,\n",
    "    savedir=\"./pretrained_model_checkpoints\",\n",
    "    overwrite=False,\n",
    "    save_filename=None,\n",
    "    use_auth_token=False,\n",
    "    revision=None,\n",
    "    cache_dir: Union[str, pathlib.Path, None] = None,\n",
    "    silent_local_fetch: bool = False,\n",
    "):\n",
    "    \"\"\"Ensures you have a local copy of the file, returns its path\n",
    "\n",
    "    In case the source is an external location, downloads the file.  In case\n",
    "    the source is already accessible on the filesystem, creates a symlink in\n",
    "    the savedir. Thus, the side effects of this function always look similar:\n",
    "    savedir/save_filename can be used to access the file. And save_filename\n",
    "    defaults to the filename arg.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    filename : str\n",
    "        Name of the file including extensions.\n",
    "    source : str or FetchSource\n",
    "        Where to look for the file. This is interpreted in special ways:\n",
    "        First, if the source begins with \"http://\" or \"https://\", it is\n",
    "        interpreted as a web address and the file is downloaded.\n",
    "        Second, if the source is a valid directory path, a symlink is\n",
    "        created to the file.\n",
    "        Otherwise, the source is interpreted as a Huggingface model hub ID, and\n",
    "        the file is downloaded from there.\n",
    "    savedir : str\n",
    "        Path where to save downloads/symlinks.\n",
    "    overwrite : bool\n",
    "        If True, always overwrite existing savedir/filename file and download\n",
    "        or recreate the link. If False (as by default), if savedir/filename\n",
    "        exists, assume it is correct and don't download/relink. Note that\n",
    "        Huggingface local cache is always used - with overwrite=True we just\n",
    "        relink from the local cache.\n",
    "    save_filename : str\n",
    "        The filename to use for saving this file. Defaults to filename if not\n",
    "        given.\n",
    "    use_auth_token : bool (default: False)\n",
    "        If true Hugginface's auth_token will be used to load private models from the HuggingFace Hub,\n",
    "        default is False because majority of models are public.\n",
    "    revision : str\n",
    "        The model revision corresponding to the HuggingFace Hub model revision.\n",
    "        This is particularly useful if you wish to pin your code to a particular\n",
    "        version of a model hosted at HuggingFace.\n",
    "    cache_dir: str or Path (default: None)\n",
    "        Location of HuggingFace cache for storing pre-trained models, to which symlinks are created.\n",
    "    silent_local_fetch: bool (default: False)\n",
    "        Surpress logging messages (quiet mode).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pathlib.Path\n",
    "        Path to file on local file system.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If file is not found\n",
    "    \"\"\"\n",
    "    if save_filename is None:\n",
    "        save_filename = filename\n",
    "    savedir = pathlib.Path(savedir)\n",
    "    savedir.mkdir(parents=True, exist_ok=True)\n",
    "    fetch_from = None\n",
    "    if isinstance(source, FetchSource):\n",
    "        fetch_from, source = source\n",
    "    sourcefile = f\"{source}/{filename}\"\n",
    "    if pathlib.Path(source).is_dir() and fetch_from not in [\n",
    "        FetchFrom.HUGGING_FACE,\n",
    "        FetchFrom.URI,\n",
    "    ]:\n",
    "        # Interpret source as local directory path & return it as destination\n",
    "        sourcepath = pathlib.Path(sourcefile).absolute()\n",
    "        MSG = f\"Destination {filename}: local file in {str(sourcepath)}.\"\n",
    "        if not silent_local_fetch:\n",
    "            logger.info(MSG)\n",
    "        return sourcepath\n",
    "    destination = savedir / save_filename\n",
    "    if destination.exists() and not overwrite:\n",
    "        MSG = f\"Fetch {filename}: Using existing file/symlink in {str(destination)}.\"\n",
    "        logger.info(MSG)\n",
    "        return destination\n",
    "    if (\n",
    "        str(source).startswith(\"http:\") or str(source).startswith(\"https:\")\n",
    "    ) or fetch_from is FetchFrom.URI:\n",
    "        # Interpret source as web address.\n",
    "        MSG = f\"Fetch {filename}: Downloading from normal URL {str(sourcefile)}.\"\n",
    "        logger.info(MSG)\n",
    "        # Download\n",
    "        try:\n",
    "            urllib.request.urlretrieve(sourcefile, destination)\n",
    "        except urllib.error.URLError:\n",
    "            raise ValueError(\n",
    "                f\"Interpreted {source} as web address, but could not download.\"\n",
    "            )\n",
    "    else:  # FetchFrom.HUGGING_FACE check is spared (no other option right now)\n",
    "        # Interpret source as huggingface hub ID\n",
    "        # Use huggingface hub's fancy cached download.\n",
    "        MSG = f\"Fetch {filename}: Delegating to Huggingface hub, source {str(source)}.\"\n",
    "        print(MSG)\n",
    "        logger.info(MSG)\n",
    "        try:\n",
    "            fetched_file = huggingface_hub.hf_hub_download(\n",
    "                repo_id=source,\n",
    "                filename=filename,\n",
    "                use_auth_token=use_auth_token,\n",
    "                revision=revision,\n",
    "                cache_dir=cache_dir,\n",
    "            )\n",
    "            logger.info(f\"HF fetch: {fetched_file}\")\n",
    "        except HTTPError as e:\n",
    "            if \"404 Client Error\" in str(e):\n",
    "                raise ValueError(\"File not found on HF hub\")\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        # Huggingface hub downloads to etag filename, symlink to the expected one:\n",
    "        sourcepath = pathlib.Path(fetched_file).absolute()\n",
    "        # Create destination directory if it does not exist\n",
    "        destination.parent.mkdir(parents=True, exist_ok=True)\n",
    "        _missing_ok_unlink(destination)\n",
    "        destination.symlink_to(sourcepath)\n",
    "    return destination\n",
    "\n",
    "\n",
    "def from_hparams(\n",
    "    cls,\n",
    "    source,\n",
    "    hparams_file=\"hyperparams.yaml\",\n",
    "    weights_file=\"model.pt\",\n",
    "    pymodule_file=\"custom.py\",\n",
    "    overrides={},\n",
    "    savedir=None,\n",
    "    use_auth_token=False,\n",
    "    revision=None,\n",
    "    download_only=False,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Fetch and load based from outside source based on HyperPyYAML file\n",
    "\n",
    "    The source can be a location on the filesystem or online/huggingface\n",
    "\n",
    "    You can use the pymodule_file to include any custom implementations\n",
    "    that are needed: if that file exists, then its location is added to\n",
    "    sys.path before Hyperparams YAML is loaded, so it can be referenced\n",
    "    in the YAML.\n",
    "\n",
    "    The hyperparams file should contain a \"modules\" key, which is a\n",
    "    dictionary of torch modules used for computation.\n",
    "\n",
    "    The hyperparams file should contain a \"pretrainer\" key, which is a\n",
    "    speechbrain.utils.parameter_transfer.Pretrainer\n",
    "\n",
    "    Adapted from https://github.com/speechbrain/\n",
    "\n",
    "    \"\"\"\n",
    "    if savedir is None:\n",
    "        clsname = cls.__name__\n",
    "        savedir = f\"./pretrained_models/{clsname}-{hashlib.md5(source.encode('UTF-8', errors='replace')).hexdigest()}\"\n",
    "    hparams_local_path = fetch(\n",
    "        filename=hparams_file,\n",
    "        source=source,\n",
    "        savedir=savedir,\n",
    "        overwrite=False,\n",
    "        save_filename=None,\n",
    "        use_auth_token=use_auth_token,\n",
    "        revision=revision,\n",
    "    )\n",
    "    weights_local_path = fetch(\n",
    "        filename=weights_file,\n",
    "        source=source,\n",
    "        savedir=savedir,\n",
    "        overwrite=False,\n",
    "        save_filename=None,\n",
    "        use_auth_token=use_auth_token,\n",
    "        revision=revision,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        pymodule_local_path = fetch(\n",
    "            filename=pymodule_file,\n",
    "            source=source,\n",
    "            savedir=savedir,\n",
    "            overwrite=False,\n",
    "            save_filename=None,\n",
    "            use_auth_token=use_auth_token,\n",
    "            revision=revision,\n",
    "        )\n",
    "        sys.path.append(str(pymodule_local_path.parent))\n",
    "    except ValueError:\n",
    "        if pymodule_file == \"custom.py\":\n",
    "            # The optional custom Python module file did not exist\n",
    "            # and had the default name\n",
    "            pass\n",
    "        else:\n",
    "            # Custom Python module file not found, but some other\n",
    "            # filename than the default was given.\n",
    "            raise\n",
    "\n",
    "    # Load the modules:\n",
    "    # with open(hparams_local_path) as fin:\n",
    "    #     hparams = load_hyperpyyaml(fin, overrides)\n",
    "\n",
    "    hparams = yaml.safe_load(open(hparams_local_path, \"r\"))\n",
    "\n",
    "    # Load on the CPU. Later the params can be moved elsewhere by specifying\n",
    "    if not download_only:\n",
    "        # Now return the system\n",
    "        model_class = cls(**hparams, **kwargs)\n",
    "        model_class.load_state_dict(torch.load(weights_local_path, map_location=\"cpu\"))\n",
    "        print(\"Model loaded from\", weights_local_path)\n",
    "        return model_class\n",
    "\n",
    "\n",
    "def from_scripted(filename, source, savedir=None):\n",
    "    \"\"\"Load a model from a scripted file\"\"\"\n",
    "    if savedir is None:\n",
    "        savedir = f\"./pretrained_models/{filename}-{hashlib.md5(source.encode('UTF-8', errors='replace')).hexdigest()}\"\n",
    "    filename = filename + \".ts\" if not filename.endswith(\".ts\") else filename\n",
    "\n",
    "    print(filename, source, savedir)\n",
    "    model_file = fetch(\n",
    "        filename=filename,\n",
    "        source=source,\n",
    "        savedir=savedir,\n",
    "    )\n",
    "\n",
    "    model = torch.jit.load(model_file)\n",
    "    print(\"Model loaded from\", model_file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Optional, List, Callable, Union\n",
    "import torchaudio.transforms as T\n",
    "from nnAudio import features\n",
    "import warnings\n",
    "\n",
    "HF_SOURCE = \"BernardoTorres/singer-identity\"\n",
    "\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spec_layer: str = \"melspectogram\",\n",
    "        n_fft: int = 2048,\n",
    "        hop_length: int = 512,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if spec_layer == \"melspectogram\":\n",
    "            n_mels = 128\n",
    "            if kwargs.get(\"n_mels\", 0):\n",
    "                n_mels = kwargs[\"n_mels\"]\n",
    "            self.spec_layer = features.MelSpectrogram(\n",
    "                n_fft=n_fft, hop_length=hop_length, verbose=False, n_mels=n_mels\n",
    "            )\n",
    "        elif spec_layer == \"stft\":\n",
    "            self.spec_layer = features.STFT(\n",
    "                n_fft=n_fft,\n",
    "                hop_length=hop_length,\n",
    "                verbose=False,\n",
    "                output_format=\"Magnitude\" ** kwargs,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.spec_layer(x)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder, used to extract embeddings from the input acoustic features\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, backbone=\"efficientnet_b0\", embedding_dim=1000, pretrained=False, **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # With attention pooling, not used in the paper\n",
    "        if backbone == \"efficientnet_b0_att\":\n",
    "            encoder_backbone = Efficientnet_att(\n",
    "                vismod=\"efficientnet_b0\",\n",
    "                num_classes=1000,\n",
    "                pretrained=pretrained,\n",
    "                embedding_dim=embedding_dim,\n",
    "                **kwargs,\n",
    "            )\n",
    "            self.net = nn.Sequential(LogScale(), Grey2Rgb(), encoder_backbone)\n",
    "\n",
    "        # Default to efficientnet backbone with average pooling, used in the paper\n",
    "        else:\n",
    "            encoder_backbone = get_vision_backbone(\n",
    "                vismod=backbone,\n",
    "                num_classes=embedding_dim,\n",
    "                pretrained=pretrained,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "            # Grey2Rgb() is used to replicate mel-spec channel (efficientnet expects 3)\n",
    "            self.net = nn.Sequential(LogScale(), Grey2Rgb(), encoder_backbone)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape [batch, channels, frames]\n",
    "        \"\"\"\n",
    "        embedding = self.net(x)\n",
    "        return embedding\n",
    "\n",
    "\n",
    "class Projection(nn.Module):\n",
    "    \"\"\"Projection head, used to reduce the dimensionality of the embedding\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=1000,\n",
    "        output_dim=128,\n",
    "        nonlinearity=None,\n",
    "        is_identity=False,\n",
    "        l2_normalize=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.l2_normalize = l2_normalize\n",
    "        self.is_identity = is_identity\n",
    "        self.output_dim = output_dim\n",
    "        if is_identity:\n",
    "            self.net = nn.Identity()\n",
    "        else:\n",
    "            if nonlinearity is None:\n",
    "                nonlinearity = torch.nn.SiLU()\n",
    "            self.net = nn.Sequential(\n",
    "                nonlinearity, torch.nn.Linear(input_dim, output_dim)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        projection = self.net(x)\n",
    "        if self.l2_normalize and not self.is_identity:\n",
    "            projection = torch.nn.functional.normalize(projection, dim=-1)\n",
    "        return projection\n",
    "\n",
    "\n",
    "class IdentityEncoder(nn.Module):\n",
    "    \"\"\"Wraps a feature extractor with an encoder, without projection head\n",
    "    Useful for loading pretrained models\"\"\"\n",
    "\n",
    "    def __init__(self, feature_extractor, encoder):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = FeatureExtractor(**feature_extractor)\n",
    "        self.encoder = Encoder(**encoder)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(self.feature_extractor(x))\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dims: List[int],\n",
    "        activation: Union[bool, nn.Module] = True,\n",
    "        use_batchnorm: Union[bool, int] = False,\n",
    "        batchnorm_fn: Optional[nn.Module] = None,\n",
    "        last_layer: Optional[nn.Module] = None,\n",
    "        bias: Optional[bool] = None,\n",
    "        layer_init: Optional[Union[Callable[[nn.Module], nn.Module], str]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_dim = dims[0]\n",
    "        self.out_dim = dims[-1]\n",
    "\n",
    "        if len(dims) < 2:\n",
    "            self.model = nn.Identity()\n",
    "\n",
    "            if activation or use_batchnorm:\n",
    "                warnings.warn(\n",
    "                    \"An activation/batch-norm is defined for the projector \"\n",
    "                    \"whereas it is the identity function.\"\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            # define activation layer\n",
    "            if activation is True:\n",
    "                activation = nn.ReLU()\n",
    "\n",
    "            # define batch-norm layer\n",
    "            if use_batchnorm is not False and batchnorm_fn is None:\n",
    "                batchnorm_fn = nn.BatchNorm1d\n",
    "\n",
    "            # useless to add bias just before a batch-norm layer but add the option for completeness\n",
    "            if bias is None:\n",
    "                bias = isinstance(use_batchnorm, bool) and not use_batchnorm\n",
    "\n",
    "            # NOTE: with old implementation use_batchnorm=True means use_batchnorm=False + bias=True\n",
    "            if use_batchnorm is True:\n",
    "                use_batchnorm = 0\n",
    "            elif use_batchnorm is False:\n",
    "                use_batchnorm = float(\"inf\")\n",
    "\n",
    "            ckpt_path = None\n",
    "            if isinstance(layer_init, str):\n",
    "                ckpt_path = layer_init\n",
    "                layer_init = lambda x: x\n",
    "            elif layer_init is None:\n",
    "                layer_init = lambda x: x\n",
    "\n",
    "            layers = []\n",
    "\n",
    "            output_dim = dims.pop()\n",
    "\n",
    "            for i in range(len(dims) - 1):\n",
    "                in_dim, out_dim = dims[i], dims[i + 1]\n",
    "                layers.append(layer_init(nn.Linear(in_dim, out_dim, bias=bias)))\n",
    "                if i >= use_batchnorm:\n",
    "                    layers.append(batchnorm_fn(out_dim))\n",
    "                if activation:\n",
    "                    layers.append(activation)\n",
    "            layers.append(nn.Linear(dims.pop(), output_dim, bias=True))\n",
    "            if last_layer is not None:\n",
    "                layers.append(last_layer)\n",
    "\n",
    "            self.model = nn.Sequential(*layers)\n",
    "\n",
    "            if ckpt_path is not None:\n",
    "                self.load_state_dict(torch.load(ckpt_path))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "class Efficientnet_att(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vismod=\"efficientnet_b0\",\n",
    "        num_classes=1000,\n",
    "        pretrained=False,\n",
    "        embedding_dim=1000,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(Efficientnet_att, self).__init__()\n",
    "\n",
    "        self.vision = get_vision_backbone(\n",
    "            vismod=vismod, num_classes=num_classes, pretrained=pretrained, **kwargs\n",
    "        ).features\n",
    "\n",
    "        self.att = nn.Sequential(\n",
    "            nn.Conv1d(1280, int(embedding_dim / 2), kernel_size=1, groups=2),\n",
    "            AttentiveStatisticPool(int(embedding_dim / 2), 128),\n",
    "        )\n",
    "\n",
    "        self.avg = nn.AvgPool2d((4, 1))\n",
    "        self.bn1 = nn.BatchNorm1d(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.vision(x)\n",
    "        y = self.avg(y).squeeze(2)\n",
    "        y = self.att(y)\n",
    "        return self.bn1(y)\n",
    "\n",
    "\n",
    "# Not used in the experiments in the paper\n",
    "class AttentiveStatisticPool(nn.Module):\n",
    "    def __init__(self, c_in, c_mid):\n",
    "        super(AttentiveStatisticPool, self).__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv1d(c_in, c_mid, kernel_size=1),\n",
    "            nn.Tanh(),  # seems like most implementations uses tanh?\n",
    "            nn.Conv1d(c_mid, c_in, kernel_size=1),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape: B x C x t\n",
    "        alpha = self.network(x)\n",
    "        mu_hat = torch.sum(alpha * x, dim=-1)\n",
    "        var = torch.sum(alpha * x**2, dim=-1) - mu_hat**2\n",
    "        std_hat = torch.sqrt(var.clamp(min=1e-9))\n",
    "        y = torch.cat([mu_hat, std_hat], dim=-1)\n",
    "        # y.shape: B x (c_in*2)\n",
    "        return y\n",
    "\n",
    "\n",
    "def load_model(\n",
    "    model, source=HF_SOURCE, torchscript=False, savedir=None, input_sr=44100\n",
    "):\n",
    "    \"\"\"Load a model from a source, can be a local path or a huggingface model hub ID\"\"\"\n",
    "\n",
    "    if torchscript:\n",
    "        if input_sr != 44100:\n",
    "            raise Exception(\"Torchscript models only support 44100 Hz input\")\n",
    "        model = from_scripted(f\"{model}/model.ts\", source, savedir=savedir)\n",
    "    elif \".\" in model:\n",
    "        # Instantiate IdentityEncoder with input_sr argument\n",
    "        model = from_hparams(\n",
    "            IdentityEncoder,\n",
    "            source,\n",
    "            hparams_file=f\"{model}/hyperparams.yaml\",\n",
    "            weights_file=f\"{model}/model.pt\",\n",
    "            savedir=savedir,\n",
    "        )\n",
    "    else:\n",
    "        # CHeck\n",
    "        model = from_hparams(\n",
    "            IdentityEncoder,\n",
    "            source,\n",
    "            hparams_file=f\"{model}/hyperparams.yaml\",\n",
    "            weights_file=f\"{model}/model.pt\",\n",
    "            savedir=savedir,\n",
    "        )\n",
    "    if input_sr != 44100:\n",
    "        # Replace feature extractor with a resampler\n",
    "        feature_extractor = model.feature_extractor\n",
    "        model.feature_extractor = nn.Sequential(\n",
    "            T.Resample(input_sr, 44100), feature_extractor\n",
    "        )\n",
    "        print(f\"Resampling input from {input_sr} to 44100 Hz\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from singer_identity.utils.core import similarity, roll\n",
    "\n",
    "\n",
    "def std_batch(x, var=1, eps=1e-8):\n",
    "    std = torch.sqrt(x.var(dim=0) + eps)\n",
    "    return torch.mean(F.relu(var - std))\n",
    "\n",
    "\n",
    "def variance_hinge_reg(x, y, var=1):\n",
    "    # From https://github.com/facebookresearch/vicreg\n",
    "    std_x = std_batch(x, var=var)\n",
    "    std_y = std_batch(y, var=var)\n",
    "    std_loss = std_x / 2 + std_y / 2\n",
    "    return std_loss\n",
    "\n",
    "\n",
    "def covariance(x):\n",
    "    # In official implementation they do mean over batch (to verify)\n",
    "    # mean = x.mean(1, keepdims=True)\n",
    "    mean = x.mean(dim=0)\n",
    "    x = x - mean\n",
    "    cov = torch.matmul(x.transpose(0, 1), x) / (x.shape[0] - 1)\n",
    "    # cov = (x.T @ x) / (x.shape[0] - 1)\n",
    "    return cov\n",
    "\n",
    "\n",
    "def covariance_reg(x, y):\n",
    "    eye = torch.eye(x.shape[1]).to(x.device)\n",
    "    cov_x = covariance(x)\n",
    "    cov_y = covariance(y)\n",
    "    assert cov_x.shape[0] == cov_x.shape[1]\n",
    "    assert cov_y.shape[0] == cov_y.shape[1]\n",
    "    cov_reg = (cov_x * (1 - eye)).pow(2).sum() / x.shape[1] + (cov_y * (1 - eye)).pow(\n",
    "        2\n",
    "    ).sum() / x.shape[1]\n",
    "    return cov_reg\n",
    "\n",
    "\n",
    "def invariance_loss(x, y):\n",
    "    return F.mse_loss(x, y)\n",
    "\n",
    "\n",
    "def vicreg_loss(x, y, gamma=1, fact_inv_loss=1, fact_var=1, fact_cov=1):\n",
    "    # Adapted from https://github.com/facebookresearch/vicreg\n",
    "    repr_loss = invariance_loss(x, y)\n",
    "    std_loss = variance_hinge_reg(x, y, var=gamma)\n",
    "    cov_loss = covariance_reg(x, y)\n",
    "    loss = fact_inv_loss * repr_loss + fact_var * std_loss + fact_cov * cov_loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_norms(*args):\n",
    "    norms = []\n",
    "    for arg in args:\n",
    "        norms.append(torch.sqrt((arg**2).sum(1)))\n",
    "    return norms\n",
    "\n",
    "\n",
    "def align_loss(x, y, alpha=2):\n",
    "    # From https://github.com/SsnL/align_uniform\n",
    "    return (x - y).norm(p=2, dim=1).pow(alpha).mean()\n",
    "\n",
    "\n",
    "def uniform_loss(x, t=2):\n",
    "    # From https://github.com/SsnL/align_uniform\n",
    "    return torch.pdist(x, p=2).pow(2).mul(-t).exp().mean().log()\n",
    "\n",
    "\n",
    "def contrastive_loss(z1, z2, temp=0.3, nr_negative=3, decouple=False):\n",
    "    cost_pos = similarity(z1, z2, temp)  # Positive samples\n",
    "    cost_neg = []\n",
    "\n",
    "    n_rolls = min(z1.shape[0] - 1, nr_negative)  # Number of negative samples\n",
    "    curr_neg_z = z2\n",
    "\n",
    "    for i in range(n_rolls):\n",
    "        curr_neg_z = roll(curr_neg_z)  # Shifts batch\n",
    "        cost_neg.append(similarity(z1, curr_neg_z, temp))  # Negative sim.\n",
    "\n",
    "    if not decouple:\n",
    "        cost_neg.append(cost_pos)  # Adds positive similarity in denominator\n",
    "\n",
    "    cost_neg = torch.stack(cost_neg).transpose(1, 0)\n",
    "    cost = (-cost_pos + torch.logsumexp(cost_neg, 1)).mean()\n",
    "    # TODO: implement similarities with less operations, but this works\n",
    "    ratio = torch.mean(cost_neg) / (\n",
    "        torch.mean(cost_pos) + torch.tensor(1e-6).type_as(z1)\n",
    "    )\n",
    "    return cost, ratio.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning.cli import instantiate_class\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class SSLTrainer(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_extractor: dict = {},\n",
    "        backbone: dict = {},\n",
    "        projection: dict = {},\n",
    "        optimizer1_init: dict = {},\n",
    "        use_contrastive_loss: bool = True,\n",
    "        temp: float = 0.3,\n",
    "        nr_negative: int = 64,\n",
    "        decouple: bool = True,\n",
    "        use_invariance_loss: bool = False,\n",
    "        fact_inv_loss: float = 1,\n",
    "        use_covariance_reg: bool = False,\n",
    "        fact_cov: float = 1,\n",
    "        use_variance_reg: bool = False,\n",
    "        fact_var: float = 1,\n",
    "        gamma: float = 1,\n",
    "        use_vicreg_loss: bool = False,\n",
    "        use_align_loss: bool = False,\n",
    "        fact_align_loss: float = 0.25,\n",
    "        fact_unif_loss: float = 0.5,\n",
    "        use_uniform_loss: bool = False,\n",
    "        compute_test_loss: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.feature_extractor = FeatureExtractor(**feature_extractor)\n",
    "        self.encoder = Encoder(**backbone)\n",
    "        self.projection = Projection(**projection)\n",
    "        self.net = nn.Sequential(self.feature_extractor, self.encoder)\n",
    "\n",
    "        self.optimizer1_init = optimizer1_init\n",
    "        self.gamma = (\n",
    "            gamma\n",
    "            if not self.projection.l2_normalize\n",
    "            else (1 / np.sqrt(self.projection.output_dim))\n",
    "        )\n",
    "        self.temp = temp\n",
    "        self.nr_negative = nr_negative\n",
    "        self.fact_inv_loss = fact_inv_loss\n",
    "        self.fact_cov = fact_cov\n",
    "        self.fact_var = fact_var\n",
    "        self.fact_align_loss = fact_align_loss\n",
    "        self.fact_unif_loss = fact_unif_loss\n",
    "        self.decouple = decouple\n",
    "\n",
    "        self.use_contrastive_loss = use_contrastive_loss\n",
    "        self.use_vicreg_loss = use_vicreg_loss\n",
    "        self.use_invariance_loss = use_invariance_loss\n",
    "        self.use_covariance_reg = use_covariance_reg\n",
    "        self.use_variance_reg = use_variance_reg\n",
    "        self.use_align_loss = use_align_loss\n",
    "        self.use_uniform_loss = use_uniform_loss\n",
    "        self.reloaded = True\n",
    "        self.compute_test_loss = compute_test_loss\n",
    "        print(\"Declaring trainer\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # features = self.feature_extractor(x)\n",
    "        # feature_embeddings = self.encoder(features)\n",
    "        # projection = self.projection(feature_embeddings)\n",
    "        # return projection\n",
    "        return self.net(x)\n",
    "\n",
    "    def encode(self, x):\n",
    "        acoustic_features = self.feature_extractor(x)\n",
    "        feature_embeddings = self.encoder(acoustic_features)\n",
    "        return feature_embeddings\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = list(self.feature_extractor.parameters())\n",
    "        params += list(self.encoder.parameters())\n",
    "        params += list(self.projection.parameters())\n",
    "        optimizer1 = instantiate_class(params, self.optimizer1_init)\n",
    "        return optimizer1\n",
    "\n",
    "    def shared_step(self, batch, batch_idx, step_name, sync_dist=True):\n",
    "        step_name = f\"/{step_name}\" if step_name != \"\" else step_name\n",
    "        pos_sample1 = batch[\"clip1\"]\n",
    "        pos_sample2 = batch[\"clip2\"]\n",
    "        # print('forward done')\n",
    "        batch_size = pos_sample1.shape[0]\n",
    "\n",
    "        # Projections of positive pairs\n",
    "        z1 = self(pos_sample1)\n",
    "        z2 = self(pos_sample2)\n",
    "\n",
    "        loss = torch.tensor(0).type_as(z1)\n",
    "        cont_loss = torch.tensor(0).type_as(z1)\n",
    "        ratio = 0\n",
    "        vicreg_loss = torch.tensor(0).type_as(z1)\n",
    "        inv_loss = torch.tensor(0).type_as(z1)\n",
    "        cov_reg = torch.tensor(0).type_as(z1)\n",
    "        var_reg = torch.tensor(0).type_as(z1)\n",
    "        align_loss = torch.tensor(0).type_as(z1)\n",
    "        uniform_loss = torch.tensor(0).type_as(z1)\n",
    "\n",
    "        if self.use_contrastive_loss:\n",
    "            cont_loss, ratio = contrastive_loss(\n",
    "                z1,\n",
    "                z2,\n",
    "                temp=self.temp,\n",
    "                nr_negative=self.nr_negative,\n",
    "                decouple=self.decouple,\n",
    "            )\n",
    "            loss += cont_loss\n",
    "            self.log(f\"loss_contrastive{step_name}\", cont_loss, batch_size=batch_size)\n",
    "            self.log(\n",
    "                f\"ratio_contrastive_pos_neg{step_name}\", ratio, batch_size=batch_size\n",
    "            )\n",
    "\n",
    "        if self.use_vicreg_loss:\n",
    "            vicreg_loss = vicreg_loss(\n",
    "                z1,\n",
    "                z2,\n",
    "                gamma=self.gamma,\n",
    "                fact_inv_loss=self.fact_inv_loss,\n",
    "                fact_var=self.fact_var,\n",
    "                fact_cov=self.fact_cov,\n",
    "            )\n",
    "            loss += vicreg_loss\n",
    "            self.log(f\"loss_vicreg{step_name}\", vicreg_loss, batch_size=batch_size)\n",
    "        else:\n",
    "            if self.use_invariance_loss:\n",
    "                inv_loss = invariance_loss(z1, z2) * self.fact_inv_loss\n",
    "                loss = loss + inv_loss\n",
    "                self.log(f\"loss_invariance{step_name}\", inv_loss, batch_size=batch_size)\n",
    "            if self.use_covariance_reg:\n",
    "                cov_reg = covariance_reg(z1, z2) * self.fact_cov\n",
    "                loss += cov_reg\n",
    "                self.log(f\"reg_covariance{step_name}\", cov_reg, batch_size=batch_size)\n",
    "            if self.use_variance_reg:\n",
    "                var_reg = variance_hinge_reg(z1, z2, self.gamma) * self.fact_var\n",
    "                loss = loss + var_reg\n",
    "                self.log(f\"reg_variance{step_name}\", var_reg, batch_size=batch_size)\n",
    "        if self.use_align_loss:\n",
    "            align_loss = align_loss(z1, z2) * self.fact_align_loss\n",
    "            loss += align_loss\n",
    "            self.log(f\"loss_align{step_name}\", align_loss, batch_size=batch_size)\n",
    "        if self.use_uniform_loss:\n",
    "            uniform_loss = (\n",
    "                (uniform_loss(z1) + uniform_loss(z2))\n",
    "                * self.fact_unif_loss\n",
    "                / 2\n",
    "            )\n",
    "            loss += uniform_loss\n",
    "            self.log(f\"loss_uniform{step_name}\", uniform_loss, batch_size=batch_size)\n",
    "\n",
    "        self.log(f\"loss{step_name}\", loss, prog_bar=True, batch_size=batch_size)\n",
    "        # print(f'loss is {loss}')\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.shared_step(batch, batch_idx, \"train\")\n",
    "        # print(f'loss is {loss}')\n",
    "        return loss\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        train_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        print(f'Epoch {self.current_epoch}: Train Loss: {train_loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kfold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import torchaudio\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class YourModel(pl.LightningModule):\n",
    "    def __init__(self, model_cont, lr, weight_decay, num_classes):\n",
    "        super().__init__()\n",
    "        self.model_cont = model_cont\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.num_classes = num_classes\n",
    "        self.fc = torch.nn.Linear(1000, self.num_classes)\n",
    "        self.train_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=self.num_classes)\n",
    "        self.train_recall = torchmetrics.Recall(task='multiclass', num_classes=self.num_classes)\n",
    "        self.train_f1 = torchmetrics.F1Score(task='multiclass', num_classes=self.num_classes)\n",
    "        \n",
    "        self.test_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=self.num_classes)\n",
    "        self.test_recall = torchmetrics.Recall(task='multiclass', num_classes=self.num_classes)\n",
    "        self.test_f1 = torchmetrics.F1Score(task='multiclass', num_classes=self.num_classes)\n",
    "        self.training_losses = []\n",
    "        self.all_y_pred_labels_train = []\n",
    "        self.all_y_true_train = []\n",
    "        self.all_y_pred_labels_test = []\n",
    "        self.all_y_true_test = []\n",
    "\n",
    "    def forward(self, x, embeddings=False):\n",
    "        out = self.model_cont(x)\n",
    "        if embeddings:\n",
    "            return out\n",
    "        else:\n",
    "            return self.fc(out)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch['clip1']\n",
    "        y = batch['group_name']\n",
    "        y = torch.tensor(y)\n",
    "        \n",
    "        y_pred = self.forward(x)\n",
    "\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        self.train_accuracy(y_pred, y)\n",
    "        self.train_recall(y_pred, y)\n",
    "        self.train_f1(y_pred, y)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        self.log('train_accuracy', self.train_accuracy, on_step=True, on_epoch=True)\n",
    "        self.log('train_recall', self.train_recall, on_step=True, on_epoch=True)\n",
    "        self.log('train_f1', self.train_f1, on_step=True, on_epoch=True)\n",
    "\n",
    "        y_pred_labels = torch.argmax(y_pred, dim=1)\n",
    "        \n",
    "        self.all_y_pred_labels_train.extend(y_pred_labels.cpu().numpy())\n",
    "        self.all_y_true_train.extend(y.cpu().numpy())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        train_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        train_acc = self.train_accuracy.compute()\n",
    "        train_rec = self.train_recall.compute()\n",
    "        train_f1 = self.train_f1.compute()\n",
    "        print(f'Epoch {self.current_epoch}: Train Loss: {train_loss}, Train Accuracy: {train_acc}, Train Recall: {train_rec}, Train F1: {train_f1}')\n",
    "\n",
    "        y_true_np = np.array(self.all_y_true_train)\n",
    "        y_pred_np = np.array(self.all_y_pred_labels_train)\n",
    "        if self.current_epoch == 5:\n",
    "            print(classification_report(y_true_np, y_pred_np))\n",
    "\n",
    "        self.all_y_pred_labels_train = []\n",
    "        self.all_y_true_train = []\n",
    "\n",
    "        self.train_accuracy.reset()\n",
    "        self.train_recall.reset()\n",
    "        self.train_f1.reset()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch['clip1']\n",
    "        y = batch['group_name']\n",
    "        y = torch.tensor(y)\n",
    "        y_pred = self.forward(x)\n",
    "        self.test_accuracy(y_pred, y)\n",
    "        self.test_recall(y_pred, y)\n",
    "        self.test_f1(y_pred, y)\n",
    "        self.log('test_accuracy', self.test_accuracy, on_step=True, on_epoch=True)\n",
    "        self.log('test_recall', self.test_recall, on_step=True, on_epoch=True)\n",
    "        self.log('test_f1', self.test_f1, on_step=True, on_epoch=True)\n",
    "\n",
    "        y_pred_labels = torch.argmax(y_pred, dim=1)\n",
    "        \n",
    "        self.all_y_pred_labels_test.extend(y_pred_labels.cpu().numpy())\n",
    "        self.all_y_true_test.extend(y.cpu().numpy())\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        test_acc = self.test_accuracy.compute()\n",
    "        test_rec = self.test_recall.compute()\n",
    "        test_f1 = self.test_f1.compute()\n",
    "        print(f'Test Accuracy: {test_acc}, Test Recall: {test_rec}, Test F1: {test_f1}')\n",
    "        \n",
    "        y_true_np = np.array(self.all_y_true_test)\n",
    "        y_pred_np = np.array(self.all_y_pred_labels_test)\n",
    "        print(classification_report(y_true_np, y_pred_np))\n",
    "\n",
    "        self.all_y_pred_labels_test = []\n",
    "        self.all_y_true_test = []\n",
    "\n",
    "        self.test_accuracy.reset()\n",
    "        self.test_recall.reset()\n",
    "        self.test_f1.reset()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "{'A.S. MURALI': 0, 'AFROZ BANO': 1, 'AJAY CHAKRAVARTY': 2, 'ASLAM SABRI & PARTY': 3, 'BAKUL CHATTOPADHYAY': 4, 'BALDEV SHARAN NARANG': 5, 'BHAI NARMAL SINGH': 6, 'Bal Gandharva': 7, 'Chorus (Students)': 8, 'DINA NATH MISHRA': 9, 'DR. BASAVARAJ RAJGURU': 10, 'DR. NIRMALA SUNDARA RAJAN': 11, 'DR. NIVEDITA SINGH': 12, 'DR. PANTULA RAMA': 13, 'DWARAM DURGA PRASAD RAO & DWARAM SATYANARAYANA RAO': 14, 'GAYATRY GIRISH': 15, 'K. SARASWATHI VIDYARTHI': 16, 'KOYAL DAS GUPTA': 17, 'LOU MAJAW (SOLO)': 18, 'M K SHANKARA NAMBOODRI': 19, 'M. K. PRANESH': 20, 'M. VENKATESH KUMAR.': 21, 'M.K.SWAMINATH PILLAI & D.MURUGESA PILLAI': 22, 'M.M. DANDAPANI DESIKAR': 23, 'M.S. SHEELA': 24, 'MALA SHRI PRASAD': 25, 'MALLADI SURI BABU': 26, 'MANDIRA LAHIRI': 27, 'MANJUSHA KULKARNI PATIL': 28, 'MEETA  PANDIT': 29, 'MOHINDER SINGH': 30, 'MUJAHID HUSSAIN KHAN': 31, 'NAMAGIRIPETTAI .K.KRISHNAN': 32, 'PARMESHWAR HEGDE': 33, 'PITTUKULI MURUGADAS': 34, 'PT. DHARAM NATH MISHRA': 35, 'PT. MADHAV INGALE': 36, 'PT. SIDDARAMA JAMBALADINNI': 37, 'R. SANTANA GOPALAN': 38, 'RAJAN MISHRA & PT. SAJAN MISHRA': 39, 'RAJINDAR SINGH': 40, 'RAJSHREE PATHAK': 41, 'RAMNATH AHEERVAAR': 42, 'RITA GANGULI': 43, 'RITESH & RAJNISH MISHRA': 44, 'S. BALAMBAL': 45, 'S. RUKMINI': 46, 'S. SOWMYA': 47, 'SAVITA DEVI': 48, 'SINGH BANDHU': 49, 'SITARAM SINGH': 50, 'SMT. AVANTIKA CHAKRAVARTI': 51, 'SMT. GEETHA RAJA': 52, 'SMT. KALPANA JHOKARKAR': 53, 'SMT. KAMATCHI KUPPUSWAMI': 54, 'SMT. NITHYASREE MAHADEVAN': 55, 'SMT. VIDYA KATGADE': 56, 'SREEVALSAN J. MENON': 57, 'SUBHRA GUHA': 58, 'T. R. Mahalingam': 59, 'TIRUVAIYARU BV JAYASHREE': 60, 'TUSHAR DUTTA': 61, 'USTAD GHULAM SADIQ KHAN': 62, 'USTAD SALEEM IQBAL': 63, 'VALLAM KRISHNASWAMI PILLAI & K.P.GOVINDARAJA PILLAI': 64, 'VASANTHI KRISHNA RAO': 65, 'VIDUSHI NEELA RAMGOPAL': 66, 'VIDUSHI SUMITRA GUHA': 67, 'YASHPAL': 68}\n",
      "{'A.S. MURALI': 0, 'AFROZ BANO': 1, 'AJAY CHAKRAVARTY': 2, 'ASLAM SABRI & PARTY': 3, 'BAKUL CHATTOPADHYAY': 4, 'BALDEV SHARAN NARANG': 5, 'BHAI NARMAL SINGH': 6, 'Bal Gandharva': 7, 'Chorus (Students)': 8, 'DINA NATH MISHRA': 9, 'DR. BASAVARAJ RAJGURU': 10, 'DR. NIRMALA SUNDARA RAJAN': 11, 'DR. NIVEDITA SINGH': 12, 'DR. PANTULA RAMA': 13, 'DWARAM DURGA PRASAD RAO & DWARAM SATYANARAYANA RAO': 14, 'GAYATRY GIRISH': 15, 'K. SARASWATHI VIDYARTHI': 16, 'KOYAL DAS GUPTA': 17, 'LOU MAJAW (SOLO)': 18, 'M K SHANKARA NAMBOODRI': 19, 'M. K. PRANESH': 20, 'M. VENKATESH KUMAR.': 21, 'M.K.SWAMINATH PILLAI & D.MURUGESA PILLAI': 22, 'M.M. DANDAPANI DESIKAR': 23, 'M.S. SHEELA': 24, 'MALA SHRI PRASAD': 25, 'MALLADI SURI BABU': 26, 'MANDIRA LAHIRI': 27, 'MANJUSHA KULKARNI PATIL': 28, 'MEETA  PANDIT': 29, 'MOHINDER SINGH': 30, 'MUJAHID HUSSAIN KHAN': 31, 'NAMAGIRIPETTAI .K.KRISHNAN': 32, 'PARMESHWAR HEGDE': 33, 'PITTUKULI MURUGADAS': 34, 'PT. DHARAM NATH MISHRA': 35, 'PT. MADHAV INGALE': 36, 'PT. SIDDARAMA JAMBALADINNI': 37, 'R. SANTANA GOPALAN': 38, 'RAJAN MISHRA & PT. SAJAN MISHRA': 39, 'RAJINDAR SINGH': 40, 'RAJSHREE PATHAK': 41, 'RAMNATH AHEERVAAR': 42, 'RITA GANGULI': 43, 'RITESH & RAJNISH MISHRA': 44, 'S. BALAMBAL': 45, 'S. RUKMINI': 46, 'S. SOWMYA': 47, 'SAVITA DEVI': 48, 'SINGH BANDHU': 49, 'SITARAM SINGH': 50, 'SMT. AVANTIKA CHAKRAVARTI': 51, 'SMT. GEETHA RAJA': 52, 'SMT. KALPANA JHOKARKAR': 53, 'SMT. KAMATCHI KUPPUSWAMI': 54, 'SMT. NITHYASREE MAHADEVAN': 55, 'SMT. VIDYA KATGADE': 56, 'SREEVALSAN J. MENON': 57, 'SUBHRA GUHA': 58, 'T. R. Mahalingam': 59, 'TIRUVAIYARU BV JAYASHREE': 60, 'TUSHAR DUTTA': 61, 'USTAD GHULAM SADIQ KHAN': 62, 'USTAD SALEEM IQBAL': 63, 'VALLAM KRISHNASWAMI PILLAI & K.P.GOVINDARAJA PILLAI': 64, 'VASANTHI KRISHNA RAO': 65, 'VIDUSHI NEELA RAMGOPAL': 66, 'VIDUSHI SUMITRA GUHA': 67, 'YASHPAL': 68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Declaring trainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/pytorch_lightning/trainer/configuration_validator.py:106: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
      "You are using a CUDA device ('NVIDIA A30') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A.S. MURALI': 0, 'AFROZ BANO': 1, 'AJAY CHAKRAVARTY': 2, 'ASLAM SABRI & PARTY': 3, 'BAKUL CHATTOPADHYAY': 4, 'BALDEV SHARAN NARANG': 5, 'BHAI NARMAL SINGH': 6, 'Bal Gandharva': 7, 'Chorus (Students)': 8, 'DINA NATH MISHRA': 9, 'DR. BASAVARAJ RAJGURU': 10, 'DR. NIRMALA SUNDARA RAJAN': 11, 'DR. NIVEDITA SINGH': 12, 'DR. PANTULA RAMA': 13, 'DWARAM DURGA PRASAD RAO & DWARAM SATYANARAYANA RAO': 14, 'GAYATRY GIRISH': 15, 'K. SARASWATHI VIDYARTHI': 16, 'KOYAL DAS GUPTA': 17, 'LOU MAJAW (SOLO)': 18, 'M K SHANKARA NAMBOODRI': 19, 'M. K. PRANESH': 20, 'M. VENKATESH KUMAR.': 21, 'M.K.SWAMINATH PILLAI & D.MURUGESA PILLAI': 22, 'M.M. DANDAPANI DESIKAR': 23, 'M.S. SHEELA': 24, 'MALA SHRI PRASAD': 25, 'MALLADI SURI BABU': 26, 'MANDIRA LAHIRI': 27, 'MANJUSHA KULKARNI PATIL': 28, 'MEETA  PANDIT': 29, 'MOHINDER SINGH': 30, 'MUJAHID HUSSAIN KHAN': 31, 'NAMAGIRIPETTAI .K.KRISHNAN': 32, 'PARMESHWAR HEGDE': 33, 'PITTUKULI MURUGADAS': 34, 'PT. DHARAM NATH MISHRA': 35, 'PT. MADHAV INGALE': 36, 'PT. SIDDARAMA JAMBALADINNI': 37, 'R. SANTANA GOPALAN': 38, 'RAJAN MISHRA & PT. SAJAN MISHRA': 39, 'RAJINDAR SINGH': 40, 'RAJSHREE PATHAK': 41, 'RAMNATH AHEERVAAR': 42, 'RITA GANGULI': 43, 'RITESH & RAJNISH MISHRA': 44, 'S. BALAMBAL': 45, 'S. RUKMINI': 46, 'S. SOWMYA': 47, 'SAVITA DEVI': 48, 'SINGH BANDHU': 49, 'SITARAM SINGH': 50, 'SMT. AVANTIKA CHAKRAVARTI': 51, 'SMT. GEETHA RAJA': 52, 'SMT. KALPANA JHOKARKAR': 53, 'SMT. KAMATCHI KUPPUSWAMI': 54, 'SMT. NITHYASREE MAHADEVAN': 55, 'SMT. VIDYA KATGADE': 56, 'SREEVALSAN J. MENON': 57, 'SUBHRA GUHA': 58, 'T. R. Mahalingam': 59, 'TIRUVAIYARU BV JAYASHREE': 60, 'TUSHAR DUTTA': 61, 'USTAD GHULAM SADIQ KHAN': 62, 'USTAD SALEEM IQBAL': 63, 'VALLAM KRISHNASWAMI PILLAI & K.P.GOVINDARAJA PILLAI': 64, 'VASANTHI KRISHNA RAO': 65, 'VIDUSHI NEELA RAMGOPAL': 66, 'VIDUSHI SUMITRA GUHA': 67, 'YASHPAL': 68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name              | Type             | Params\n",
      "-------------------------------------------------------\n",
      "0 | feature_extractor | FeatureExtractor | 0     \n",
      "1 | encoder           | Encoder          | 5.3 M \n",
      "2 | projection        | Projection       | 128 K \n",
      "3 | net               | Sequential       | 5.3 M \n",
      "-------------------------------------------------------\n",
      "5.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.4 M     Total params\n",
      "21.667    Total estimated model params size (MB)\n",
      "2024-07-01 17:20:48.506350: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-01 17:20:48.559346: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-01 17:20:49.695167: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 15/15 [00:11<00:00,  1.31it/s, loss=1.63, v_num=213, loss/train=1.430]Epoch 0: Train Loss: 1.6348161697387695\n",
      "Epoch 1: 100%|| 15/15 [00:10<00:00,  1.43it/s, loss=1.35, v_num=213, loss/train=1.220]Epoch 1: Train Loss: 1.3055071830749512\n",
      "Epoch 2: 100%|| 15/15 [00:10<00:00,  1.43it/s, loss=1.16, v_num=213, loss/train=1.170]Epoch 2: Train Loss: 1.1492725610733032\n",
      "Epoch 3: 100%|| 15/15 [00:10<00:00,  1.39it/s, loss=1.06, v_num=213, loss/train=0.916]Epoch 3: Train Loss: 1.0297876596450806\n",
      "Epoch 4: 100%|| 15/15 [00:10<00:00,  1.44it/s, loss=0.942, v_num=213, loss/train=0.837]Epoch 4: Train Loss: 0.9324694871902466\n",
      "Epoch 5: 100%|| 15/15 [00:10<00:00,  1.42it/s, loss=0.898, v_num=213, loss/train=0.809]Epoch 5: Train Loss: 0.8911947011947632\n",
      "Epoch 6: 100%|| 15/15 [00:10<00:00,  1.43it/s, loss=0.831, v_num=213, loss/train=0.816]Epoch 6: Train Loss: 0.8226702809333801\n",
      "Epoch 7: 100%|| 15/15 [00:10<00:00,  1.44it/s, loss=0.778, v_num=213, loss/train=0.714]Epoch 7: Train Loss: 0.773503303527832\n",
      "Epoch 8: 100%|| 15/15 [00:10<00:00,  1.42it/s, loss=0.745, v_num=213, loss/train=0.688]Epoch 8: Train Loss: 0.7435666918754578\n",
      "Epoch 9: 100%|| 15/15 [00:10<00:00,  1.44it/s, loss=0.706, v_num=213, loss/train=0.684]Epoch 9: Train Loss: 0.7050929069519043\n",
      "Epoch 10: 100%|| 15/15 [00:10<00:00,  1.43it/s, loss=0.683, v_num=213, loss/train=0.685]Epoch 10: Train Loss: 0.6841349005699158\n",
      "Epoch 11: 100%|| 15/15 [00:10<00:00,  1.43it/s, loss=0.673, v_num=213, loss/train=0.616]Epoch 11: Train Loss: 0.6705116033554077\n",
      "Epoch 12: 100%|| 15/15 [00:10<00:00,  1.42it/s, loss=0.644, v_num=213, loss/train=0.567]Epoch 12: Train Loss: 0.6419554948806763\n",
      "Epoch 12: 100%|| 15/15 [00:10<00:00,  1.42it/s, loss=0.644, v_num=213, loss/train=0.567]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=13` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|| 15/15 [00:10<00:00,  1.37it/s, loss=0.644, v_num=213, loss/train=0.567]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A30') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name           | Type               | Params\n",
      "------------------------------------------------------\n",
      "0 | model_cont     | SSLTrainer         | 5.4 M \n",
      "1 | criterion      | CrossEntropyLoss   | 0     \n",
      "2 | fc             | Linear             | 69.1 K\n",
      "3 | train_accuracy | MulticlassAccuracy | 0     \n",
      "4 | train_recall   | MulticlassRecall   | 0     \n",
      "5 | train_f1       | MulticlassF1Score  | 0     \n",
      "6 | test_accuracy  | MulticlassAccuracy | 0     \n",
      "7 | test_recall    | MulticlassRecall   | 0     \n",
      "8 | test_f1        | MulticlassF1Score  | 0     \n",
      "------------------------------------------------------\n",
      "5.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.5 M     Total params\n",
      "21.943    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                                                                | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_192113/4169865365.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   7%|                                                                                           | 1/15 [00:01<00:18,  1.32s/it, loss=4.66, v_num=214]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/pytorch_lightning/utilities/data.py:83: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 32. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 15/15 [00:06<00:00,  2.42it/s, loss=3.72, v_num=214]Epoch 0: Train Loss: 3.7152867317199707, Train Accuracy: 0.15448851883411407, Train Recall: 0.15448851883411407, Train F1: 0.15448851883411407\n",
      "Epoch 0: 100%|| 15/15 [00:06<00:00,  2.42it/s, loss=3.72, v_num=214]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/pytorch_lightning/utilities/data.py:83: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 31. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MulticlassF1Score was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|| 15/15 [00:06<00:00,  2.44it/s, loss=1.94, v_num=214]Epoch 1: Train Loss: 1.4912018775939941, Train Accuracy: 0.6492692828178406, Train Recall: 0.6492692828178406, Train F1: 0.6492692828178406\n",
      "Epoch 2: 100%|| 15/15 [00:06<00:00,  2.42it/s, loss=0.718, v_num=214]Epoch 2: Train Loss: 0.5484740138053894, Train Accuracy: 0.893528163433075, Train Recall: 0.893528163433075, Train F1: 0.893528163433075\n",
      "Epoch 3: 100%|| 15/15 [00:06<00:00,  2.49it/s, loss=0.31, v_num=214]Epoch 3: Train Loss: 0.21653258800506592, Train Accuracy: 0.9665970802307129, Train Recall: 0.9665970802307129, Train F1: 0.9665970802307129\n",
      "Epoch 4: 100%|| 15/15 [00:06<00:00,  2.45it/s, loss=0.12, v_num=214]Epoch 4: Train Loss: 0.0975625216960907, Train Accuracy: 0.9895615577697754, Train Recall: 0.9895615577697754, Train F1: 0.9895615577697754\n",
      "Epoch 5: 100%|| 15/15 [00:06<00:00,  2.48it/s, loss=0.0705, v_num=214]Epoch 5: Train Loss: 0.056755341589450836, Train Accuracy: 0.9895615577697754, Train Recall: 0.9895615577697754, Train F1: 0.9895615577697754\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         8\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       0.89      1.00      0.94         8\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       1.00      1.00      1.00         7\n",
      "           5       1.00      1.00      1.00         6\n",
      "           6       1.00      1.00      1.00        10\n",
      "           7       1.00      1.00      1.00        18\n",
      "           8       1.00      1.00      1.00        15\n",
      "           9       1.00      1.00      1.00         7\n",
      "          10       1.00      1.00      1.00        11\n",
      "          11       0.92      0.92      0.92        12\n",
      "          12       1.00      1.00      1.00         6\n",
      "          13       1.00      0.75      0.86         4\n",
      "          14       1.00      1.00      1.00         6\n",
      "          15       1.00      1.00      1.00         4\n",
      "          16       1.00      1.00      1.00         6\n",
      "          17       1.00      1.00      1.00         6\n",
      "          18       1.00      1.00      1.00         5\n",
      "          19       1.00      1.00      1.00         6\n",
      "          20       1.00      1.00      1.00         9\n",
      "          21       1.00      1.00      1.00        11\n",
      "          22       1.00      1.00      1.00         6\n",
      "          23       1.00      1.00      1.00         8\n",
      "          24       1.00      1.00      1.00         6\n",
      "          25       1.00      1.00      1.00         6\n",
      "          26       0.83      1.00      0.91         5\n",
      "          27       1.00      1.00      1.00         6\n",
      "          28       1.00      1.00      1.00         6\n",
      "          29       1.00      1.00      1.00         8\n",
      "          30       1.00      0.86      0.92         7\n",
      "          31       1.00      1.00      1.00         6\n",
      "          32       1.00      1.00      1.00         7\n",
      "          33       1.00      1.00      1.00         7\n",
      "          34       1.00      1.00      1.00         6\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      1.00      1.00        11\n",
      "          38       1.00      1.00      1.00         5\n",
      "          39       1.00      1.00      1.00         6\n",
      "          40       1.00      1.00      1.00         5\n",
      "          41       1.00      1.00      1.00         5\n",
      "          42       1.00      1.00      1.00         5\n",
      "          43       1.00      1.00      1.00         6\n",
      "          44       0.83      1.00      0.91         5\n",
      "          45       1.00      1.00      1.00         5\n",
      "          46       1.00      1.00      1.00         6\n",
      "          47       0.91      1.00      0.95        10\n",
      "          48       1.00      1.00      1.00         7\n",
      "          49       1.00      1.00      1.00        13\n",
      "          50       1.00      1.00      1.00         8\n",
      "          51       1.00      1.00      1.00         6\n",
      "          52       1.00      1.00      1.00         6\n",
      "          53       1.00      1.00      1.00         5\n",
      "          54       1.00      1.00      1.00         5\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       1.00      1.00      1.00         8\n",
      "          57       1.00      1.00      1.00         5\n",
      "          58       1.00      1.00      1.00         9\n",
      "          59       1.00      1.00      1.00         5\n",
      "          60       1.00      1.00      1.00         5\n",
      "          61       1.00      1.00      1.00         6\n",
      "          62       1.00      0.60      0.75         5\n",
      "          63       1.00      1.00      1.00         6\n",
      "          64       1.00      1.00      1.00         5\n",
      "          65       1.00      1.00      1.00         8\n",
      "          66       1.00      1.00      1.00         6\n",
      "          67       1.00      1.00      1.00         5\n",
      "          68       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           0.99       479\n",
      "   macro avg       0.99      0.99      0.99       479\n",
      "weighted avg       0.99      0.99      0.99       479\n",
      "\n",
      "Epoch 5: 100%|| 15/15 [00:06<00:00,  2.47it/s, loss=0.0705, v_num=214]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MulticlassF1Score was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "`Trainer.fit` stopped: `max_epochs=6` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 15/15 [00:06<00:00,  2.34it/s, loss=0.0705, v_num=214]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A30') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:488: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:  12%|                                                                                              | 1/8 [00:00<00:01,  6.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_192113/4169865365.py:89: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|| 8/8 [00:01<00:00,  4.30it/s]Test Accuracy: 0.5958333611488342, Test Recall: 0.5958333611488342, Test F1: 0.5958333611488342\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.33      0.29         3\n",
      "           1       0.67      0.67      0.67         3\n",
      "           2       0.50      0.25      0.33         4\n",
      "           3       0.67      0.67      0.67         3\n",
      "           4       0.50      0.33      0.40         3\n",
      "           5       0.50      0.33      0.40         3\n",
      "           6       0.67      1.00      0.80         4\n",
      "           7       0.90      0.90      0.90        10\n",
      "           8       0.80      0.57      0.67         7\n",
      "           9       0.67      0.50      0.57         4\n",
      "          10       0.57      0.80      0.67         5\n",
      "          11       0.60      0.50      0.55         6\n",
      "          12       0.00      0.00      0.00         2\n",
      "          13       0.00      0.00      0.00         3\n",
      "          14       1.00      0.75      0.86         4\n",
      "          15       0.00      0.00      0.00         3\n",
      "          16       0.50      1.00      0.67         2\n",
      "          17       1.00      1.00      1.00         3\n",
      "          18       1.00      1.00      1.00         3\n",
      "          19       0.30      1.00      0.46         3\n",
      "          20       1.00      0.60      0.75         5\n",
      "          21       0.50      0.40      0.44         5\n",
      "          22       0.67      1.00      0.80         2\n",
      "          23       0.50      0.80      0.62         5\n",
      "          24       1.00      0.33      0.50         3\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       1.00      0.33      0.50         3\n",
      "          27       0.00      0.00      0.00         3\n",
      "          28       1.00      0.50      0.67         2\n",
      "          29       1.00      0.25      0.40         4\n",
      "          30       0.33      1.00      0.50         4\n",
      "          31       0.50      0.33      0.40         3\n",
      "          32       0.67      0.50      0.57         4\n",
      "          33       0.00      0.00      0.00         3\n",
      "          34       0.75      1.00      0.86         3\n",
      "          35       0.50      0.50      0.50         4\n",
      "          36       0.50      0.67      0.57         3\n",
      "          37       0.50      0.60      0.55         5\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       1.00      0.33      0.50         3\n",
      "          40       1.00      0.67      0.80         3\n",
      "          41       0.00      0.00      0.00         2\n",
      "          42       1.00      1.00      1.00         2\n",
      "          43       0.25      0.67      0.36         3\n",
      "          44       0.50      0.33      0.40         3\n",
      "          45       1.00      0.67      0.80         3\n",
      "          46       0.50      1.00      0.67         3\n",
      "          47       1.00      1.00      1.00         6\n",
      "          48       0.50      0.67      0.57         3\n",
      "          49       1.00      1.00      1.00         6\n",
      "          50       1.00      0.75      0.86         4\n",
      "          51       0.75      0.75      0.75         4\n",
      "          52       0.00      0.00      0.00         2\n",
      "          53       0.00      0.00      0.00         3\n",
      "          54       0.75      1.00      0.86         3\n",
      "          55       0.38      0.75      0.50         4\n",
      "          56       1.00      0.67      0.80         3\n",
      "          57       0.33      0.67      0.44         3\n",
      "          58       0.38      0.75      0.50         4\n",
      "          59       1.00      0.67      0.80         3\n",
      "          60       0.00      0.00      0.00         3\n",
      "          61       0.00      0.00      0.00         4\n",
      "          62       0.33      1.00      0.50         3\n",
      "          63       0.67      0.67      0.67         3\n",
      "          64       0.67      0.67      0.67         3\n",
      "          65       0.50      0.25      0.33         4\n",
      "          66       1.00      0.50      0.67         2\n",
      "          67       1.00      0.50      0.67         2\n",
      "          68       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.60       240\n",
      "   macro avg       0.60      0.57      0.55       240\n",
      "weighted avg       0.62      0.60      0.57       240\n",
      "\n",
      "Testing DataLoader 0: 100%|| 8/8 [00:01<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">   Runningstage.testing    </span><span style=\"font-weight: bold\">                           </span>\n",
       "<span style=\"font-weight: bold\">          metric           </span><span style=\"font-weight: bold\">       DataLoader 0        </span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">    test_accuracy_epoch    </span><span style=\"color: #800080; text-decoration-color: #800080\">    0.5958333611488342     </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">       test_f1_epoch       </span><span style=\"color: #800080; text-decoration-color: #800080\">    0.5958333611488342     </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">     test_recall_epoch     </span><span style=\"color: #800080; text-decoration-color: #800080\">    0.5958333611488342     </span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1m  Runningstage.testing   \u001b[0m\u001b[1m \u001b[0m\u001b[1m                           \u001b[0m\n",
       "\u001b[1m \u001b[0m\u001b[1m         metric          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       "\u001b[36m \u001b[0m\u001b[36m   test_accuracy_epoch   \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m   0.5958333611488342    \u001b[0m\u001b[35m \u001b[0m\n",
       "\u001b[36m \u001b[0m\u001b[36m      test_f1_epoch      \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m   0.5958333611488342    \u001b[0m\u001b[35m \u001b[0m\n",
       "\u001b[36m \u001b[0m\u001b[36m    test_recall_epoch    \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m   0.5958333611488342    \u001b[0m\u001b[35m \u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "{'A.S. MURALI': 0, 'AFROZ BANO': 1, 'AJAY CHAKRAVARTY': 2, 'ASLAM SABRI & PARTY': 3, 'BAKUL CHATTOPADHYAY': 4, 'BALDEV SHARAN NARANG': 5, 'BHAI NARMAL SINGH': 6, 'Bal Gandharva': 7, 'Chorus (Students)': 8, 'DINA NATH MISHRA': 9, 'DR. BASAVARAJ RAJGURU': 10, 'DR. NIRMALA SUNDARA RAJAN': 11, 'DR. NIVEDITA SINGH': 12, 'DR. PANTULA RAMA': 13, 'DWARAM DURGA PRASAD RAO & DWARAM SATYANARAYANA RAO': 14, 'GAYATRY GIRISH': 15, 'K. SARASWATHI VIDYARTHI': 16, 'KOYAL DAS GUPTA': 17, 'LOU MAJAW (SOLO)': 18, 'M K SHANKARA NAMBOODRI': 19, 'M. K. PRANESH': 20, 'M. VENKATESH KUMAR.': 21, 'M.K.SWAMINATH PILLAI & D.MURUGESA PILLAI': 22, 'M.M. DANDAPANI DESIKAR': 23, 'M.S. SHEELA': 24, 'MALA SHRI PRASAD': 25, 'MALLADI SURI BABU': 26, 'MANDIRA LAHIRI': 27, 'MANJUSHA KULKARNI PATIL': 28, 'MEETA  PANDIT': 29, 'MOHINDER SINGH': 30, 'MUJAHID HUSSAIN KHAN': 31, 'NAMAGIRIPETTAI .K.KRISHNAN': 32, 'PARMESHWAR HEGDE': 33, 'PITTUKULI MURUGADAS': 34, 'PT. DHARAM NATH MISHRA': 35, 'PT. MADHAV INGALE': 36, 'PT. SIDDARAMA JAMBALADINNI': 37, 'R. SANTANA GOPALAN': 38, 'RAJAN MISHRA & PT. SAJAN MISHRA': 39, 'RAJINDAR SINGH': 40, 'RAJSHREE PATHAK': 41, 'RAMNATH AHEERVAAR': 42, 'RITA GANGULI': 43, 'RITESH & RAJNISH MISHRA': 44, 'S. BALAMBAL': 45, 'S. RUKMINI': 46, 'S. SOWMYA': 47, 'SAVITA DEVI': 48, 'SINGH BANDHU': 49, 'SITARAM SINGH': 50, 'SMT. AVANTIKA CHAKRAVARTI': 51, 'SMT. GEETHA RAJA': 52, 'SMT. KALPANA JHOKARKAR': 53, 'SMT. KAMATCHI KUPPUSWAMI': 54, 'SMT. NITHYASREE MAHADEVAN': 55, 'SMT. VIDYA KATGADE': 56, 'SREEVALSAN J. MENON': 57, 'SUBHRA GUHA': 58, 'T. R. Mahalingam': 59, 'TIRUVAIYARU BV JAYASHREE': 60, 'TUSHAR DUTTA': 61, 'USTAD GHULAM SADIQ KHAN': 62, 'USTAD SALEEM IQBAL': 63, 'VALLAM KRISHNASWAMI PILLAI & K.P.GOVINDARAJA PILLAI': 64, 'VASANTHI KRISHNA RAO': 65, 'VIDUSHI NEELA RAMGOPAL': 66, 'VIDUSHI SUMITRA GUHA': 67, 'YASHPAL': 68}\n",
      "{'A.S. MURALI': 0, 'AFROZ BANO': 1, 'AJAY CHAKRAVARTY': 2, 'ASLAM SABRI & PARTY': 3, 'BAKUL CHATTOPADHYAY': 4, 'BALDEV SHARAN NARANG': 5, 'BHAI NARMAL SINGH': 6, 'Bal Gandharva': 7, 'Chorus (Students)': 8, 'DINA NATH MISHRA': 9, 'DR. BASAVARAJ RAJGURU': 10, 'DR. NIRMALA SUNDARA RAJAN': 11, 'DR. NIVEDITA SINGH': 12, 'DR. PANTULA RAMA': 13, 'DWARAM DURGA PRASAD RAO & DWARAM SATYANARAYANA RAO': 14, 'GAYATRY GIRISH': 15, 'K. SARASWATHI VIDYARTHI': 16, 'KOYAL DAS GUPTA': 17, 'LOU MAJAW (SOLO)': 18, 'M K SHANKARA NAMBOODRI': 19, 'M. K. PRANESH': 20, 'M. VENKATESH KUMAR.': 21, 'M.K.SWAMINATH PILLAI & D.MURUGESA PILLAI': 22, 'M.M. DANDAPANI DESIKAR': 23, 'M.S. SHEELA': 24, 'MALA SHRI PRASAD': 25, 'MALLADI SURI BABU': 26, 'MANDIRA LAHIRI': 27, 'MANJUSHA KULKARNI PATIL': 28, 'MEETA  PANDIT': 29, 'MOHINDER SINGH': 30, 'MUJAHID HUSSAIN KHAN': 31, 'NAMAGIRIPETTAI .K.KRISHNAN': 32, 'PARMESHWAR HEGDE': 33, 'PITTUKULI MURUGADAS': 34, 'PT. DHARAM NATH MISHRA': 35, 'PT. MADHAV INGALE': 36, 'PT. SIDDARAMA JAMBALADINNI': 37, 'R. SANTANA GOPALAN': 38, 'RAJAN MISHRA & PT. SAJAN MISHRA': 39, 'RAJINDAR SINGH': 40, 'RAJSHREE PATHAK': 41, 'RAMNATH AHEERVAAR': 42, 'RITA GANGULI': 43, 'RITESH & RAJNISH MISHRA': 44, 'S. BALAMBAL': 45, 'S. RUKMINI': 46, 'S. SOWMYA': 47, 'SAVITA DEVI': 48, 'SINGH BANDHU': 49, 'SITARAM SINGH': 50, 'SMT. AVANTIKA CHAKRAVARTI': 51, 'SMT. GEETHA RAJA': 52, 'SMT. KALPANA JHOKARKAR': 53, 'SMT. KAMATCHI KUPPUSWAMI': 54, 'SMT. NITHYASREE MAHADEVAN': 55, 'SMT. VIDYA KATGADE': 56, 'SREEVALSAN J. MENON': 57, 'SUBHRA GUHA': 58, 'T. R. Mahalingam': 59, 'TIRUVAIYARU BV JAYASHREE': 60, 'TUSHAR DUTTA': 61, 'USTAD GHULAM SADIQ KHAN': 62, 'USTAD SALEEM IQBAL': 63, 'VALLAM KRISHNASWAMI PILLAI & K.P.GOVINDARAJA PILLAI': 64, 'VASANTHI KRISHNA RAO': 65, 'VIDUSHI NEELA RAMGOPAL': 66, 'VIDUSHI SUMITRA GUHA': 67, 'YASHPAL': 68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/pytorch_lightning/trainer/configuration_validator.py:106: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
      "You are using a CUDA device ('NVIDIA A30') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name              | Type             | Params\n",
      "-------------------------------------------------------\n",
      "0 | feature_extractor | FeatureExtractor | 0     \n",
      "1 | encoder           | Encoder          | 5.3 M \n",
      "2 | projection        | Projection       | 128 K \n",
      "3 | net               | Sequential       | 5.3 M \n",
      "-------------------------------------------------------\n",
      "5.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.4 M     Total params\n",
      "21.667    Total estimated model params size (MB)\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Declaring trainer\n",
      "{'A.S. MURALI': 0, 'AFROZ BANO': 1, 'AJAY CHAKRAVARTY': 2, 'ASLAM SABRI & PARTY': 3, 'BAKUL CHATTOPADHYAY': 4, 'BALDEV SHARAN NARANG': 5, 'BHAI NARMAL SINGH': 6, 'Bal Gandharva': 7, 'Chorus (Students)': 8, 'DINA NATH MISHRA': 9, 'DR. BASAVARAJ RAJGURU': 10, 'DR. NIRMALA SUNDARA RAJAN': 11, 'DR. NIVEDITA SINGH': 12, 'DR. PANTULA RAMA': 13, 'DWARAM DURGA PRASAD RAO & DWARAM SATYANARAYANA RAO': 14, 'GAYATRY GIRISH': 15, 'K. SARASWATHI VIDYARTHI': 16, 'KOYAL DAS GUPTA': 17, 'LOU MAJAW (SOLO)': 18, 'M K SHANKARA NAMBOODRI': 19, 'M. K. PRANESH': 20, 'M. VENKATESH KUMAR.': 21, 'M.K.SWAMINATH PILLAI & D.MURUGESA PILLAI': 22, 'M.M. DANDAPANI DESIKAR': 23, 'M.S. SHEELA': 24, 'MALA SHRI PRASAD': 25, 'MALLADI SURI BABU': 26, 'MANDIRA LAHIRI': 27, 'MANJUSHA KULKARNI PATIL': 28, 'MEETA  PANDIT': 29, 'MOHINDER SINGH': 30, 'MUJAHID HUSSAIN KHAN': 31, 'NAMAGIRIPETTAI .K.KRISHNAN': 32, 'PARMESHWAR HEGDE': 33, 'PITTUKULI MURUGADAS': 34, 'PT. DHARAM NATH MISHRA': 35, 'PT. MADHAV INGALE': 36, 'PT. SIDDARAMA JAMBALADINNI': 37, 'R. SANTANA GOPALAN': 38, 'RAJAN MISHRA & PT. SAJAN MISHRA': 39, 'RAJINDAR SINGH': 40, 'RAJSHREE PATHAK': 41, 'RAMNATH AHEERVAAR': 42, 'RITA GANGULI': 43, 'RITESH & RAJNISH MISHRA': 44, 'S. BALAMBAL': 45, 'S. RUKMINI': 46, 'S. SOWMYA': 47, 'SAVITA DEVI': 48, 'SINGH BANDHU': 49, 'SITARAM SINGH': 50, 'SMT. AVANTIKA CHAKRAVARTI': 51, 'SMT. GEETHA RAJA': 52, 'SMT. KALPANA JHOKARKAR': 53, 'SMT. KAMATCHI KUPPUSWAMI': 54, 'SMT. NITHYASREE MAHADEVAN': 55, 'SMT. VIDYA KATGADE': 56, 'SREEVALSAN J. MENON': 57, 'SUBHRA GUHA': 58, 'T. R. Mahalingam': 59, 'TIRUVAIYARU BV JAYASHREE': 60, 'TUSHAR DUTTA': 61, 'USTAD GHULAM SADIQ KHAN': 62, 'USTAD SALEEM IQBAL': 63, 'VALLAM KRISHNASWAMI PILLAI & K.P.GOVINDARAJA PILLAI': 64, 'VASANTHI KRISHNA RAO': 65, 'VIDUSHI NEELA RAMGOPAL': 66, 'VIDUSHI SUMITRA GUHA': 67, 'YASHPAL': 68}\n",
      "Epoch 0: 100%|| 15/15 [00:10<00:00,  1.45it/s, loss=1.57, v_num=215, loss/train=1.450]Epoch 0: Train Loss: 1.5726739168167114\n",
      "Epoch 1: 100%|| 15/15 [00:10<00:00,  1.41it/s, loss=1.32, v_num=215, loss/train=1.060]Epoch 1: Train Loss: 1.2641857862472534\n",
      "Epoch 2: 100%|| 15/15 [00:10<00:00,  1.42it/s, loss=1.14, v_num=215, loss/train=1.100]Epoch 2: Train Loss: 1.127185344696045\n",
      "Epoch 3: 100%|| 15/15 [00:10<00:00,  1.45it/s, loss=1.02, v_num=215, loss/train=0.878]Epoch 3: Train Loss: 0.995695173740387\n",
      "Epoch 4: 100%|| 15/15 [00:10<00:00,  1.45it/s, loss=0.93, v_num=215, loss/train=0.888]Epoch 4: Train Loss: 0.9166205525398254\n",
      "Epoch 5: 100%|| 15/15 [00:10<00:00,  1.45it/s, loss=0.874, v_num=215, loss/train=0.770]Epoch 5: Train Loss: 0.8672692775726318\n",
      "Epoch 6: 100%|| 15/15 [00:10<00:00,  1.44it/s, loss=0.829, v_num=215, loss/train=0.776]Epoch 6: Train Loss: 0.825183629989624\n",
      "Epoch 7: 100%|| 15/15 [00:10<00:00,  1.45it/s, loss=0.794, v_num=215, loss/train=0.750]Epoch 7: Train Loss: 0.7975285649299622\n",
      "Epoch 8: 100%|| 15/15 [00:11<00:00,  1.34it/s, loss=0.738, v_num=215, loss/train=0.707]Epoch 8: Train Loss: 0.7381746172904968\n",
      "Epoch 9: 100%|| 15/15 [00:11<00:00,  1.34it/s, loss=0.717, v_num=215, loss/train=0.746]Epoch 9: Train Loss: 0.7154259085655212\n",
      "Epoch 10: 100%|| 15/15 [00:10<00:00,  1.42it/s, loss=0.694, v_num=215, loss/train=0.689]Epoch 10: Train Loss: 0.6943016648292542\n",
      "Epoch 11: 100%|| 15/15 [00:10<00:00,  1.40it/s, loss=0.68, v_num=215, loss/train=0.605]Epoch 11: Train Loss: 0.6790413856506348\n",
      "Epoch 12: 100%|| 15/15 [00:10<00:00,  1.41it/s, loss=0.648, v_num=215, loss/train=0.580]Epoch 12: Train Loss: 0.6440981030464172\n",
      "Epoch 12: 100%|| 15/15 [00:10<00:00,  1.41it/s, loss=0.648, v_num=215, loss/train=0.580]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=13` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|| 15/15 [00:11<00:00,  1.36it/s, loss=0.648, v_num=215, loss/train=0.580]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A30') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name           | Type               | Params\n",
      "------------------------------------------------------\n",
      "0 | model_cont     | SSLTrainer         | 5.4 M \n",
      "1 | criterion      | CrossEntropyLoss   | 0     \n",
      "2 | fc             | Linear             | 69.1 K\n",
      "3 | train_accuracy | MulticlassAccuracy | 0     \n",
      "4 | train_recall   | MulticlassRecall   | 0     \n",
      "5 | train_f1       | MulticlassF1Score  | 0     \n",
      "6 | test_accuracy  | MulticlassAccuracy | 0     \n",
      "7 | test_recall    | MulticlassRecall   | 0     \n",
      "8 | test_f1        | MulticlassF1Score  | 0     \n",
      "------------------------------------------------------\n",
      "5.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.5 M     Total params\n",
      "21.943    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                                                                | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_192113/4169865365.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 15/15 [00:06<00:00,  2.33it/s, loss=3.69, v_num=216]Epoch 0: Train Loss: 3.685511350631714, Train Accuracy: 0.17327766120433807, Train Recall: 0.17327766120433807, Train F1: 0.17327766120433807\n",
      "Epoch 0: 100%|| 15/15 [00:06<00:00,  2.33it/s, loss=3.69, v_num=216]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MulticlassF1Score was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|| 15/15 [00:06<00:00,  2.43it/s, loss=1.9, v_num=216]Epoch 1: Train Loss: 1.4496798515319824, Train Accuracy: 0.6576200127601624, Train Recall: 0.6576200127601624, Train F1: 0.6576200127601624\n",
      "Epoch 2: 100%|| 15/15 [00:06<00:00,  2.47it/s, loss=0.742, v_num=216]Epoch 2: Train Loss: 0.5641359686851501, Train Accuracy: 0.8643006086349487, Train Recall: 0.8643006086349487, Train F1: 0.8643006086349487\n",
      "Epoch 3: 100%|| 15/15 [00:06<00:00,  2.42it/s, loss=0.323, v_num=216]Epoch 3: Train Loss: 0.2305545061826706, Train Accuracy: 0.9582463502883911, Train Recall: 0.9582463502883911, Train F1: 0.9582463502883911\n",
      "Epoch 4: 100%|| 15/15 [00:06<00:00,  2.41it/s, loss=0.146, v_num=216]Epoch 4: Train Loss: 0.12611427903175354, Train Accuracy: 0.9749478101730347, Train Recall: 0.9749478101730347, Train F1: 0.9749478101730347\n",
      "Epoch 5: 100%|| 15/15 [00:06<00:00,  2.33it/s, loss=0.0909, v_num=216]Epoch 5: Train Loss: 0.0787956491112709, Train Accuracy: 0.9937369227409363, Train Recall: 0.9937369227409363, Train F1: 0.9937369227409363\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         7\n",
      "           1       0.88      1.00      0.93         7\n",
      "           2       1.00      1.00      1.00         8\n",
      "           3       1.00      1.00      1.00         5\n",
      "           4       1.00      1.00      1.00         6\n",
      "           5       1.00      1.00      1.00         6\n",
      "           6       1.00      1.00      1.00         9\n",
      "           7       1.00      1.00      1.00        19\n",
      "           8       1.00      1.00      1.00        14\n",
      "           9       0.89      1.00      0.94         8\n",
      "          10       1.00      1.00      1.00        11\n",
      "          11       1.00      1.00      1.00        12\n",
      "          12       1.00      1.00      1.00         5\n",
      "          13       1.00      1.00      1.00         5\n",
      "          14       1.00      1.00      1.00         7\n",
      "          15       1.00      1.00      1.00         5\n",
      "          16       1.00      1.00      1.00         5\n",
      "          17       1.00      1.00      1.00         6\n",
      "          18       1.00      1.00      1.00         5\n",
      "          19       1.00      1.00      1.00         6\n",
      "          20       1.00      1.00      1.00         9\n",
      "          21       1.00      1.00      1.00        10\n",
      "          22       1.00      1.00      1.00         5\n",
      "          23       1.00      1.00      1.00         9\n",
      "          24       1.00      1.00      1.00         6\n",
      "          25       1.00      1.00      1.00         5\n",
      "          26       1.00      1.00      1.00         5\n",
      "          27       1.00      1.00      1.00         6\n",
      "          28       1.00      1.00      1.00         5\n",
      "          29       1.00      1.00      1.00         8\n",
      "          30       1.00      1.00      1.00         7\n",
      "          31       1.00      0.83      0.91         6\n",
      "          32       1.00      1.00      1.00         7\n",
      "          33       1.00      0.83      0.91         6\n",
      "          34       1.00      1.00      1.00         6\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      1.00      1.00        10\n",
      "          38       1.00      1.00      1.00         6\n",
      "          39       1.00      1.00      1.00         6\n",
      "          40       0.86      1.00      0.92         6\n",
      "          41       1.00      1.00      1.00         5\n",
      "          42       1.00      1.00      1.00         5\n",
      "          43       1.00      0.83      0.91         6\n",
      "          44       1.00      1.00      1.00         5\n",
      "          45       1.00      1.00      1.00         6\n",
      "          46       1.00      1.00      1.00         6\n",
      "          47       1.00      1.00      1.00        11\n",
      "          48       1.00      1.00      1.00         7\n",
      "          49       1.00      1.00      1.00        13\n",
      "          50       1.00      1.00      1.00         8\n",
      "          51       1.00      1.00      1.00         7\n",
      "          52       1.00      1.00      1.00         5\n",
      "          53       1.00      1.00      1.00         5\n",
      "          54       1.00      1.00      1.00         6\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       1.00      1.00      1.00         7\n",
      "          57       1.00      1.00      1.00         5\n",
      "          58       1.00      1.00      1.00         9\n",
      "          59       1.00      1.00      1.00         6\n",
      "          60       1.00      1.00      1.00         5\n",
      "          61       1.00      1.00      1.00         7\n",
      "          62       1.00      1.00      1.00         6\n",
      "          63       1.00      1.00      1.00         6\n",
      "          64       1.00      1.00      1.00         5\n",
      "          65       1.00      1.00      1.00         8\n",
      "          66       1.00      1.00      1.00         5\n",
      "          67       1.00      1.00      1.00         5\n",
      "          68       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.99       479\n",
      "   macro avg       0.99      0.99      0.99       479\n",
      "weighted avg       0.99      0.99      0.99       479\n",
      "\n",
      "Epoch 5: 100%|| 15/15 [00:06<00:00,  2.33it/s, loss=0.0909, v_num=216]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MulticlassF1Score was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "`Trainer.fit` stopped: `max_epochs=6` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 15/15 [00:06<00:00,  2.21it/s, loss=0.0909, v_num=216]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A30') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:488: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:  12%|                                                                                              | 1/8 [00:00<00:01,  6.29it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_192113/4169865365.py:89: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|| 8/8 [00:01<00:00,  4.98it/s]Test Accuracy: 0.6666666865348816, Test Recall: 0.6666666865348816, Test F1: 0.6666666865348816\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.25      0.33         4\n",
      "           1       1.00      1.00      1.00         3\n",
      "           2       0.75      0.75      0.75         4\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       0.80      1.00      0.89         4\n",
      "           5       0.50      0.33      0.40         3\n",
      "           6       0.80      0.80      0.80         5\n",
      "           7       0.75      1.00      0.86         9\n",
      "           8       1.00      0.88      0.93         8\n",
      "           9       0.75      1.00      0.86         3\n",
      "          10       0.57      0.80      0.67         5\n",
      "          11       1.00      0.83      0.91         6\n",
      "          12       1.00      0.67      0.80         3\n",
      "          13       0.50      1.00      0.67         2\n",
      "          14       0.60      1.00      0.75         3\n",
      "          15       0.33      0.50      0.40         2\n",
      "          16       1.00      1.00      1.00         3\n",
      "          17       0.67      0.67      0.67         3\n",
      "          18       1.00      0.67      0.80         3\n",
      "          19       1.00      1.00      1.00         3\n",
      "          20       0.57      0.80      0.67         5\n",
      "          21       0.60      0.50      0.55         6\n",
      "          22       0.00      0.00      0.00         3\n",
      "          23       0.60      0.75      0.67         4\n",
      "          24       0.67      0.67      0.67         3\n",
      "          25       1.00      0.33      0.50         3\n",
      "          26       1.00      0.67      0.80         3\n",
      "          27       1.00      0.33      0.50         3\n",
      "          28       0.67      0.67      0.67         3\n",
      "          29       0.25      0.75      0.38         4\n",
      "          30       0.67      0.50      0.57         4\n",
      "          31       0.33      0.33      0.33         3\n",
      "          32       0.67      0.50      0.57         4\n",
      "          33       0.75      0.75      0.75         4\n",
      "          34       1.00      1.00      1.00         3\n",
      "          35       0.67      0.50      0.57         4\n",
      "          36       1.00      0.33      0.50         3\n",
      "          37       0.67      0.33      0.44         6\n",
      "          38       0.50      0.50      0.50         2\n",
      "          39       0.60      1.00      0.75         3\n",
      "          40       0.50      0.50      0.50         2\n",
      "          41       1.00      0.50      0.67         2\n",
      "          42       1.00      1.00      1.00         2\n",
      "          43       1.00      1.00      1.00         3\n",
      "          44       0.50      0.67      0.57         3\n",
      "          45       0.67      1.00      0.80         2\n",
      "          46       0.50      0.67      0.57         3\n",
      "          47       1.00      1.00      1.00         5\n",
      "          48       1.00      0.33      0.50         3\n",
      "          49       0.71      0.83      0.77         6\n",
      "          50       0.60      0.75      0.67         4\n",
      "          51       1.00      0.33      0.50         3\n",
      "          52       0.00      0.00      0.00         3\n",
      "          53       0.00      0.00      0.00         3\n",
      "          54       0.50      1.00      0.67         2\n",
      "          55       0.67      0.50      0.57         4\n",
      "          56       0.33      0.50      0.40         4\n",
      "          57       0.75      1.00      0.86         3\n",
      "          58       1.00      0.50      0.67         4\n",
      "          59       1.00      1.00      1.00         2\n",
      "          60       0.33      0.33      0.33         3\n",
      "          61       1.00      0.33      0.50         3\n",
      "          62       0.50      0.50      0.50         2\n",
      "          63       0.60      1.00      0.75         3\n",
      "          64       0.60      1.00      0.75         3\n",
      "          65       0.00      0.00      0.00         4\n",
      "          66       1.00      0.33      0.50         3\n",
      "          67       1.00      1.00      1.00         2\n",
      "          68       0.50      0.67      0.57         3\n",
      "\n",
      "    accuracy                           0.67       240\n",
      "   macro avg       0.70      0.66      0.64       240\n",
      "weighted avg       0.70      0.67      0.65       240\n",
      "\n",
      "Testing DataLoader 0: 100%|| 8/8 [00:01<00:00,  4.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">   Runningstage.testing    </span><span style=\"font-weight: bold\">                           </span>\n",
       "<span style=\"font-weight: bold\">          metric           </span><span style=\"font-weight: bold\">       DataLoader 0        </span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">    test_accuracy_epoch    </span><span style=\"color: #800080; text-decoration-color: #800080\">    0.6666666865348816     </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">       test_f1_epoch       </span><span style=\"color: #800080; text-decoration-color: #800080\">    0.6666666865348816     </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">     test_recall_epoch     </span><span style=\"color: #800080; text-decoration-color: #800080\">    0.6666666865348816     </span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1m  Runningstage.testing   \u001b[0m\u001b[1m \u001b[0m\u001b[1m                           \u001b[0m\n",
       "\u001b[1m \u001b[0m\u001b[1m         metric          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       "\u001b[36m \u001b[0m\u001b[36m   test_accuracy_epoch   \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m   0.6666666865348816    \u001b[0m\u001b[35m \u001b[0m\n",
       "\u001b[36m \u001b[0m\u001b[36m      test_f1_epoch      \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m   0.6666666865348816    \u001b[0m\u001b[35m \u001b[0m\n",
       "\u001b[36m \u001b[0m\u001b[36m    test_recall_epoch    \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m   0.6666666865348816    \u001b[0m\u001b[35m \u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2\n",
      "{'A.S. MURALI': 0, 'AFROZ BANO': 1, 'AJAY CHAKRAVARTY': 2, 'ASLAM SABRI & PARTY': 3, 'BAKUL CHATTOPADHYAY': 4, 'BALDEV SHARAN NARANG': 5, 'BHAI NARMAL SINGH': 6, 'Bal Gandharva': 7, 'Chorus (Students)': 8, 'DINA NATH MISHRA': 9, 'DR. BASAVARAJ RAJGURU': 10, 'DR. NIRMALA SUNDARA RAJAN': 11, 'DR. NIVEDITA SINGH': 12, 'DR. PANTULA RAMA': 13, 'DWARAM DURGA PRASAD RAO & DWARAM SATYANARAYANA RAO': 14, 'GAYATRY GIRISH': 15, 'K. SARASWATHI VIDYARTHI': 16, 'KOYAL DAS GUPTA': 17, 'LOU MAJAW (SOLO)': 18, 'M K SHANKARA NAMBOODRI': 19, 'M. K. PRANESH': 20, 'M. VENKATESH KUMAR.': 21, 'M.K.SWAMINATH PILLAI & D.MURUGESA PILLAI': 22, 'M.M. DANDAPANI DESIKAR': 23, 'M.S. SHEELA': 24, 'MALA SHRI PRASAD': 25, 'MALLADI SURI BABU': 26, 'MANDIRA LAHIRI': 27, 'MANJUSHA KULKARNI PATIL': 28, 'MEETA  PANDIT': 29, 'MOHINDER SINGH': 30, 'MUJAHID HUSSAIN KHAN': 31, 'NAMAGIRIPETTAI .K.KRISHNAN': 32, 'PARMESHWAR HEGDE': 33, 'PITTUKULI MURUGADAS': 34, 'PT. DHARAM NATH MISHRA': 35, 'PT. MADHAV INGALE': 36, 'PT. SIDDARAMA JAMBALADINNI': 37, 'R. SANTANA GOPALAN': 38, 'RAJAN MISHRA & PT. SAJAN MISHRA': 39, 'RAJINDAR SINGH': 40, 'RAJSHREE PATHAK': 41, 'RAMNATH AHEERVAAR': 42, 'RITA GANGULI': 43, 'RITESH & RAJNISH MISHRA': 44, 'S. BALAMBAL': 45, 'S. RUKMINI': 46, 'S. SOWMYA': 47, 'SAVITA DEVI': 48, 'SINGH BANDHU': 49, 'SITARAM SINGH': 50, 'SMT. AVANTIKA CHAKRAVARTI': 51, 'SMT. GEETHA RAJA': 52, 'SMT. KALPANA JHOKARKAR': 53, 'SMT. KAMATCHI KUPPUSWAMI': 54, 'SMT. NITHYASREE MAHADEVAN': 55, 'SMT. VIDYA KATGADE': 56, 'SREEVALSAN J. MENON': 57, 'SUBHRA GUHA': 58, 'T. R. Mahalingam': 59, 'TIRUVAIYARU BV JAYASHREE': 60, 'TUSHAR DUTTA': 61, 'USTAD GHULAM SADIQ KHAN': 62, 'USTAD SALEEM IQBAL': 63, 'VALLAM KRISHNASWAMI PILLAI & K.P.GOVINDARAJA PILLAI': 64, 'VASANTHI KRISHNA RAO': 65, 'VIDUSHI NEELA RAMGOPAL': 66, 'VIDUSHI SUMITRA GUHA': 67, 'YASHPAL': 68}\n",
      "{'A.S. MURALI': 0, 'AFROZ BANO': 1, 'AJAY CHAKRAVARTY': 2, 'ASLAM SABRI & PARTY': 3, 'BAKUL CHATTOPADHYAY': 4, 'BALDEV SHARAN NARANG': 5, 'BHAI NARMAL SINGH': 6, 'Bal Gandharva': 7, 'Chorus (Students)': 8, 'DINA NATH MISHRA': 9, 'DR. BASAVARAJ RAJGURU': 10, 'DR. NIRMALA SUNDARA RAJAN': 11, 'DR. NIVEDITA SINGH': 12, 'DR. PANTULA RAMA': 13, 'DWARAM DURGA PRASAD RAO & DWARAM SATYANARAYANA RAO': 14, 'GAYATRY GIRISH': 15, 'K. SARASWATHI VIDYARTHI': 16, 'KOYAL DAS GUPTA': 17, 'LOU MAJAW (SOLO)': 18, 'M K SHANKARA NAMBOODRI': 19, 'M. K. PRANESH': 20, 'M. VENKATESH KUMAR.': 21, 'M.K.SWAMINATH PILLAI & D.MURUGESA PILLAI': 22, 'M.M. DANDAPANI DESIKAR': 23, 'M.S. SHEELA': 24, 'MALA SHRI PRASAD': 25, 'MALLADI SURI BABU': 26, 'MANDIRA LAHIRI': 27, 'MANJUSHA KULKARNI PATIL': 28, 'MEETA  PANDIT': 29, 'MOHINDER SINGH': 30, 'MUJAHID HUSSAIN KHAN': 31, 'NAMAGIRIPETTAI .K.KRISHNAN': 32, 'PARMESHWAR HEGDE': 33, 'PITTUKULI MURUGADAS': 34, 'PT. DHARAM NATH MISHRA': 35, 'PT. MADHAV INGALE': 36, 'PT. SIDDARAMA JAMBALADINNI': 37, 'R. SANTANA GOPALAN': 38, 'RAJAN MISHRA & PT. SAJAN MISHRA': 39, 'RAJINDAR SINGH': 40, 'RAJSHREE PATHAK': 41, 'RAMNATH AHEERVAAR': 42, 'RITA GANGULI': 43, 'RITESH & RAJNISH MISHRA': 44, 'S. BALAMBAL': 45, 'S. RUKMINI': 46, 'S. SOWMYA': 47, 'SAVITA DEVI': 48, 'SINGH BANDHU': 49, 'SITARAM SINGH': 50, 'SMT. AVANTIKA CHAKRAVARTI': 51, 'SMT. GEETHA RAJA': 52, 'SMT. KALPANA JHOKARKAR': 53, 'SMT. KAMATCHI KUPPUSWAMI': 54, 'SMT. NITHYASREE MAHADEVAN': 55, 'SMT. VIDYA KATGADE': 56, 'SREEVALSAN J. MENON': 57, 'SUBHRA GUHA': 58, 'T. R. Mahalingam': 59, 'TIRUVAIYARU BV JAYASHREE': 60, 'TUSHAR DUTTA': 61, 'USTAD GHULAM SADIQ KHAN': 62, 'USTAD SALEEM IQBAL': 63, 'VALLAM KRISHNASWAMI PILLAI & K.P.GOVINDARAJA PILLAI': 64, 'VASANTHI KRISHNA RAO': 65, 'VIDUSHI NEELA RAMGOPAL': 66, 'VIDUSHI SUMITRA GUHA': 67, 'YASHPAL': 68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/pytorch_lightning/trainer/configuration_validator.py:106: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
      "You are using a CUDA device ('NVIDIA A30') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name              | Type             | Params\n",
      "-------------------------------------------------------\n",
      "0 | feature_extractor | FeatureExtractor | 0     \n",
      "1 | encoder           | Encoder          | 5.3 M \n",
      "2 | projection        | Projection       | 128 K \n",
      "3 | net               | Sequential       | 5.3 M \n",
      "-------------------------------------------------------\n",
      "5.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.4 M     Total params\n",
      "21.667    Total estimated model params size (MB)\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Declaring trainer\n",
      "{'A.S. MURALI': 0, 'AFROZ BANO': 1, 'AJAY CHAKRAVARTY': 2, 'ASLAM SABRI & PARTY': 3, 'BAKUL CHATTOPADHYAY': 4, 'BALDEV SHARAN NARANG': 5, 'BHAI NARMAL SINGH': 6, 'Bal Gandharva': 7, 'Chorus (Students)': 8, 'DINA NATH MISHRA': 9, 'DR. BASAVARAJ RAJGURU': 10, 'DR. NIRMALA SUNDARA RAJAN': 11, 'DR. NIVEDITA SINGH': 12, 'DR. PANTULA RAMA': 13, 'DWARAM DURGA PRASAD RAO & DWARAM SATYANARAYANA RAO': 14, 'GAYATRY GIRISH': 15, 'K. SARASWATHI VIDYARTHI': 16, 'KOYAL DAS GUPTA': 17, 'LOU MAJAW (SOLO)': 18, 'M K SHANKARA NAMBOODRI': 19, 'M. K. PRANESH': 20, 'M. VENKATESH KUMAR.': 21, 'M.K.SWAMINATH PILLAI & D.MURUGESA PILLAI': 22, 'M.M. DANDAPANI DESIKAR': 23, 'M.S. SHEELA': 24, 'MALA SHRI PRASAD': 25, 'MALLADI SURI BABU': 26, 'MANDIRA LAHIRI': 27, 'MANJUSHA KULKARNI PATIL': 28, 'MEETA  PANDIT': 29, 'MOHINDER SINGH': 30, 'MUJAHID HUSSAIN KHAN': 31, 'NAMAGIRIPETTAI .K.KRISHNAN': 32, 'PARMESHWAR HEGDE': 33, 'PITTUKULI MURUGADAS': 34, 'PT. DHARAM NATH MISHRA': 35, 'PT. MADHAV INGALE': 36, 'PT. SIDDARAMA JAMBALADINNI': 37, 'R. SANTANA GOPALAN': 38, 'RAJAN MISHRA & PT. SAJAN MISHRA': 39, 'RAJINDAR SINGH': 40, 'RAJSHREE PATHAK': 41, 'RAMNATH AHEERVAAR': 42, 'RITA GANGULI': 43, 'RITESH & RAJNISH MISHRA': 44, 'S. BALAMBAL': 45, 'S. RUKMINI': 46, 'S. SOWMYA': 47, 'SAVITA DEVI': 48, 'SINGH BANDHU': 49, 'SITARAM SINGH': 50, 'SMT. AVANTIKA CHAKRAVARTI': 51, 'SMT. GEETHA RAJA': 52, 'SMT. KALPANA JHOKARKAR': 53, 'SMT. KAMATCHI KUPPUSWAMI': 54, 'SMT. NITHYASREE MAHADEVAN': 55, 'SMT. VIDYA KATGADE': 56, 'SREEVALSAN J. MENON': 57, 'SUBHRA GUHA': 58, 'T. R. Mahalingam': 59, 'TIRUVAIYARU BV JAYASHREE': 60, 'TUSHAR DUTTA': 61, 'USTAD GHULAM SADIQ KHAN': 62, 'USTAD SALEEM IQBAL': 63, 'VALLAM KRISHNASWAMI PILLAI & K.P.GOVINDARAJA PILLAI': 64, 'VASANTHI KRISHNA RAO': 65, 'VIDUSHI NEELA RAMGOPAL': 66, 'VIDUSHI SUMITRA GUHA': 67, 'YASHPAL': 68}\n",
      "Epoch 0: 100%|| 15/15 [00:10<00:00,  1.41it/s, loss=1.59, v_num=217, loss/train=1.520]Epoch 0: Train Loss: 1.5891512632369995\n",
      "Epoch 1: 100%|| 15/15 [00:10<00:00,  1.39it/s, loss=1.33, v_num=217, loss/train=1.300]Epoch 1: Train Loss: 1.281050682067871\n",
      "Epoch 2: 100%|| 15/15 [00:10<00:00,  1.36it/s, loss=1.15, v_num=217, loss/train=1.150]Epoch 2: Train Loss: 1.1148425340652466\n",
      "Epoch 3: 100%|| 15/15 [00:11<00:00,  1.35it/s, loss=1.01, v_num=217, loss/train=0.914]Epoch 3: Train Loss: 0.9744254946708679\n",
      "Epoch 4: 100%|| 15/15 [00:10<00:00,  1.36it/s, loss=0.92, v_num=217, loss/train=0.888]Epoch 4: Train Loss: 0.918202817440033\n",
      "Epoch 5: 100%|| 15/15 [00:10<00:00,  1.39it/s, loss=0.868, v_num=217, loss/train=0.819]Epoch 5: Train Loss: 0.8546710014343262\n",
      "Epoch 6: 100%|| 15/15 [00:10<00:00,  1.41it/s, loss=0.818, v_num=217, loss/train=0.793]Epoch 6: Train Loss: 0.813147246837616\n",
      "Epoch 7: 100%|| 15/15 [00:10<00:00,  1.43it/s, loss=0.77, v_num=217, loss/train=0.778]Epoch 7: Train Loss: 0.7694080471992493\n",
      "Epoch 8: 100%|| 15/15 [00:10<00:00,  1.40it/s, loss=0.75, v_num=217, loss/train=0.748]Epoch 8: Train Loss: 0.7435721755027771\n",
      "Epoch 9: 100%|| 15/15 [00:10<00:00,  1.40it/s, loss=0.723, v_num=217, loss/train=0.717]Epoch 9: Train Loss: 0.7140833735466003\n",
      "Epoch 10: 100%|| 15/15 [00:10<00:00,  1.41it/s, loss=0.706, v_num=217, loss/train=0.687]Epoch 10: Train Loss: 0.7022462487220764\n",
      "Epoch 11: 100%|| 15/15 [00:10<00:00,  1.41it/s, loss=0.672, v_num=217, loss/train=0.616]Epoch 11: Train Loss: 0.658657431602478\n",
      "Epoch 12: 100%|| 15/15 [00:10<00:00,  1.42it/s, loss=0.649, v_num=217, loss/train=0.630]Epoch 12: Train Loss: 0.6513200998306274\n",
      "Epoch 12: 100%|| 15/15 [00:10<00:00,  1.42it/s, loss=0.649, v_num=217, loss/train=0.630]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=13` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|| 15/15 [00:10<00:00,  1.38it/s, loss=0.649, v_num=217, loss/train=0.630]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A30') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name           | Type               | Params\n",
      "------------------------------------------------------\n",
      "0 | model_cont     | SSLTrainer         | 5.4 M \n",
      "1 | criterion      | CrossEntropyLoss   | 0     \n",
      "2 | fc             | Linear             | 69.1 K\n",
      "3 | train_accuracy | MulticlassAccuracy | 0     \n",
      "4 | train_recall   | MulticlassRecall   | 0     \n",
      "5 | train_f1       | MulticlassF1Score  | 0     \n",
      "6 | test_accuracy  | MulticlassAccuracy | 0     \n",
      "7 | test_recall    | MulticlassRecall   | 0     \n",
      "8 | test_f1        | MulticlassF1Score  | 0     \n",
      "------------------------------------------------------\n",
      "5.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.5 M     Total params\n",
      "21.943    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                                                                | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_192113/4169865365.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 15/15 [00:06<00:00,  2.43it/s, loss=3.65, v_num=221]Epoch 0: Train Loss: 3.654693841934204, Train Accuracy: 0.17499999701976776, Train Recall: 0.17499999701976776, Train F1: 0.17499999701976776\n",
      "Epoch 0: 100%|| 15/15 [00:06<00:00,  2.43it/s, loss=3.65, v_num=221]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MulticlassF1Score was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|| 15/15 [00:06<00:00,  2.45it/s, loss=1.99, v_num=221]Epoch 1: Train Loss: 1.5965068340301514, Train Accuracy: 0.6187499761581421, Train Recall: 0.6187499761581421, Train F1: 0.6187499761581421\n",
      "Epoch 2: 100%|| 15/15 [00:06<00:00,  2.43it/s, loss=0.812, v_num=221]Epoch 2: Train Loss: 0.6361797451972961, Train Accuracy: 0.8541666865348816, Train Recall: 0.8541666865348816, Train F1: 0.8541666865348816\n",
      "Epoch 3: 100%|| 15/15 [00:06<00:00,  2.42it/s, loss=0.331, v_num=221]Epoch 3: Train Loss: 0.25883474946022034, Train Accuracy: 0.9645833373069763, Train Recall: 0.9645833373069763, Train F1: 0.9645833373069763\n",
      "Epoch 4: 100%|| 15/15 [00:06<00:00,  2.41it/s, loss=0.166, v_num=221]Epoch 4: Train Loss: 0.14287588000297546, Train Accuracy: 0.9854166507720947, Train Recall: 0.9854166507720947, Train F1: 0.9854166507720947\n",
      "Epoch 5: 100%|| 15/15 [00:06<00:00,  2.49it/s, loss=0.0978, v_num=221]Epoch 5: Train Loss: 0.09142930060625076, Train Accuracy: 0.9895833134651184, Train Recall: 0.9895833134651184, Train F1: 0.9895833134651184\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         7\n",
      "           1       1.00      1.00      1.00         6\n",
      "           2       1.00      1.00      1.00         8\n",
      "           3       1.00      1.00      1.00         5\n",
      "           4       1.00      1.00      1.00         7\n",
      "           5       1.00      0.83      0.91         6\n",
      "           6       0.90      1.00      0.95         9\n",
      "           7       1.00      1.00      1.00        19\n",
      "           8       1.00      1.00      1.00        15\n",
      "           9       1.00      1.00      1.00         7\n",
      "          10       1.00      1.00      1.00        10\n",
      "          11       1.00      1.00      1.00        12\n",
      "          12       1.00      1.00      1.00         5\n",
      "          13       1.00      1.00      1.00         5\n",
      "          14       1.00      1.00      1.00         7\n",
      "          15       1.00      1.00      1.00         5\n",
      "          16       1.00      1.00      1.00         5\n",
      "          17       1.00      1.00      1.00         6\n",
      "          18       1.00      1.00      1.00         6\n",
      "          19       1.00      1.00      1.00         6\n",
      "          20       1.00      1.00      1.00        10\n",
      "          21       1.00      1.00      1.00        11\n",
      "          22       1.00      0.80      0.89         5\n",
      "          23       1.00      1.00      1.00         9\n",
      "          24       0.86      1.00      0.92         6\n",
      "          25       1.00      1.00      1.00         5\n",
      "          26       1.00      1.00      1.00         6\n",
      "          27       1.00      1.00      1.00         6\n",
      "          28       1.00      1.00      1.00         5\n",
      "          29       1.00      1.00      1.00         8\n",
      "          30       1.00      1.00      1.00         8\n",
      "          31       0.86      1.00      0.92         6\n",
      "          32       0.88      0.88      0.88         8\n",
      "          33       1.00      1.00      1.00         7\n",
      "          34       1.00      1.00      1.00         6\n",
      "          35       1.00      0.88      0.93         8\n",
      "          36       1.00      1.00      1.00         6\n",
      "          37       1.00      1.00      1.00        11\n",
      "          38       1.00      1.00      1.00         5\n",
      "          39       1.00      1.00      1.00         6\n",
      "          40       1.00      1.00      1.00         5\n",
      "          41       1.00      1.00      1.00         4\n",
      "          42       1.00      1.00      1.00         4\n",
      "          43       1.00      1.00      1.00         6\n",
      "          44       1.00      1.00      1.00         6\n",
      "          45       1.00      1.00      1.00         5\n",
      "          46       1.00      1.00      1.00         6\n",
      "          47       1.00      1.00      1.00        11\n",
      "          48       1.00      1.00      1.00         6\n",
      "          49       1.00      1.00      1.00        12\n",
      "          50       1.00      1.00      1.00         8\n",
      "          51       1.00      1.00      1.00         7\n",
      "          52       0.83      1.00      0.91         5\n",
      "          53       1.00      1.00      1.00         6\n",
      "          54       1.00      1.00      1.00         5\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       1.00      1.00      1.00         7\n",
      "          57       1.00      1.00      1.00         6\n",
      "          58       1.00      1.00      1.00         8\n",
      "          59       1.00      1.00      1.00         5\n",
      "          60       1.00      0.83      0.91         6\n",
      "          61       1.00      1.00      1.00         7\n",
      "          62       1.00      1.00      1.00         5\n",
      "          63       1.00      1.00      1.00         6\n",
      "          64       1.00      1.00      1.00         6\n",
      "          65       1.00      1.00      1.00         8\n",
      "          66       1.00      1.00      1.00         5\n",
      "          67       1.00      1.00      1.00         4\n",
      "          68       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.99       480\n",
      "   macro avg       0.99      0.99      0.99       480\n",
      "weighted avg       0.99      0.99      0.99       480\n",
      "\n",
      "Epoch 5: 100%|| 15/15 [00:06<00:00,  2.48it/s, loss=0.0978, v_num=221]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MulticlassRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MulticlassF1Score was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "`Trainer.fit` stopped: `max_epochs=6` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 15/15 [00:06<00:00,  2.35it/s, loss=0.0978, v_num=221]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A30') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:488: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:  12%|                                                                                              | 1/8 [00:00<00:01,  5.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_192113/4169865365.py:89: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|| 8/8 [00:01<00:00,  4.64it/s]Test Accuracy: 0.6736401915550232, Test Recall: 0.6736401915550232, Test F1: 0.6736401915550232\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.50      0.75      0.60         4\n",
      "           2       0.25      0.50      0.33         4\n",
      "           3       1.00      0.50      0.67         2\n",
      "           4       0.43      1.00      0.60         3\n",
      "           5       0.33      0.67      0.44         3\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       0.89      0.89      0.89         9\n",
      "           8       1.00      1.00      1.00         7\n",
      "           9       1.00      0.50      0.67         4\n",
      "          10       0.67      0.67      0.67         6\n",
      "          11       0.71      0.83      0.77         6\n",
      "          12       0.50      0.33      0.40         3\n",
      "          13       0.33      0.50      0.40         2\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       0.00      0.00      0.00         2\n",
      "          16       1.00      0.67      0.80         3\n",
      "          17       1.00      0.33      0.50         3\n",
      "          18       1.00      1.00      1.00         2\n",
      "          19       1.00      0.33      0.50         3\n",
      "          20       0.80      1.00      0.89         4\n",
      "          21       0.56      1.00      0.71         5\n",
      "          22       0.75      1.00      0.86         3\n",
      "          23       0.75      0.75      0.75         4\n",
      "          24       0.50      0.33      0.40         3\n",
      "          25       0.00      0.00      0.00         3\n",
      "          26       1.00      0.50      0.67         2\n",
      "          27       1.00      0.33      0.50         3\n",
      "          28       0.67      0.67      0.67         3\n",
      "          29       0.14      0.25      0.18         4\n",
      "          30       0.75      1.00      0.86         3\n",
      "          31       0.50      0.33      0.40         3\n",
      "          32       1.00      0.67      0.80         3\n",
      "          33       0.67      0.67      0.67         3\n",
      "          34       1.00      1.00      1.00         3\n",
      "          35       0.57      1.00      0.73         4\n",
      "          36       1.00      1.00      1.00         2\n",
      "          37       0.80      0.80      0.80         5\n",
      "          38       1.00      0.67      0.80         3\n",
      "          39       0.25      0.33      0.29         3\n",
      "          40       1.00      0.67      0.80         3\n",
      "          41       0.60      1.00      0.75         3\n",
      "          42       1.00      1.00      1.00         3\n",
      "          43       0.50      0.67      0.57         3\n",
      "          44       1.00      0.50      0.67         2\n",
      "          45       1.00      1.00      1.00         3\n",
      "          46       1.00      0.67      0.80         3\n",
      "          47       0.71      1.00      0.83         5\n",
      "          48       0.67      0.50      0.57         4\n",
      "          49       0.56      0.71      0.62         7\n",
      "          50       1.00      1.00      1.00         4\n",
      "          51       0.50      0.67      0.57         3\n",
      "          52       0.50      0.33      0.40         3\n",
      "          53       0.00      0.00      0.00         2\n",
      "          54       1.00      1.00      1.00         3\n",
      "          55       0.75      0.75      0.75         4\n",
      "          56       1.00      0.50      0.67         4\n",
      "          57       0.00      0.00      0.00         2\n",
      "          58       1.00      0.80      0.89         5\n",
      "          59       1.00      0.67      0.80         3\n",
      "          60       0.14      0.50      0.22         2\n",
      "          61       1.00      0.33      0.50         3\n",
      "          62       0.50      0.33      0.40         3\n",
      "          63       1.00      1.00      1.00         3\n",
      "          64       1.00      1.00      1.00         2\n",
      "          65       1.00      0.50      0.67         4\n",
      "          66       1.00      0.33      0.50         3\n",
      "          67       0.33      0.33      0.33         3\n",
      "          68       0.50      0.67      0.57         3\n",
      "\n",
      "    accuracy                           0.67       239\n",
      "   macro avg       0.70      0.64      0.64       239\n",
      "weighted avg       0.72      0.67      0.67       239\n",
      "\n",
      "Testing DataLoader 0: 100%|| 8/8 [00:01<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">   Runningstage.testing    </span><span style=\"font-weight: bold\">                           </span>\n",
       "<span style=\"font-weight: bold\">          metric           </span><span style=\"font-weight: bold\">       DataLoader 0        </span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">    test_accuracy_epoch    </span><span style=\"color: #800080; text-decoration-color: #800080\">    0.6736401915550232     </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">       test_f1_epoch       </span><span style=\"color: #800080; text-decoration-color: #800080\">    0.6736401915550232     </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">     test_recall_epoch     </span><span style=\"color: #800080; text-decoration-color: #800080\">    0.6736401915550232     </span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1m  Runningstage.testing   \u001b[0m\u001b[1m \u001b[0m\u001b[1m                           \u001b[0m\n",
       "\u001b[1m \u001b[0m\u001b[1m         metric          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       "\u001b[36m \u001b[0m\u001b[36m   test_accuracy_epoch   \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m   0.6736401915550232    \u001b[0m\u001b[35m \u001b[0m\n",
       "\u001b[36m \u001b[0m\u001b[36m      test_f1_epoch      \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m   0.6736401915550232    \u001b[0m\u001b[35m \u001b[0m\n",
       "\u001b[36m \u001b[0m\u001b[36m    test_recall_epoch    \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m   0.6736401915550232    \u001b[0m\u001b[35m \u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "csv_path = '/home/surge_siya/ALL CSV/Panns_CSV.csv'\n",
    "df = pd.read_csv(csv_path,index_col=0)\n",
    "df = df[df.iloc[:,1] != 'Unlabelled']\n",
    "df = df[df.iloc[:,1] != 'Other']\n",
    "df = df[df.iloc[:,-1] != 'Speech']\n",
    "num_classes = df['Singer'].nunique()\n",
    "\n",
    "# kfold = KFold(n_splits=5, shuffle=True)\n",
    "# for fold, (train_idx, test_idx) in enumerate(kfold.split(df)):\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "    \n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(df, df['Singer'])):\n",
    "        print(f'Fold {fold}')\n",
    "        train_df = df.iloc[train_idx]\n",
    "        test_df = df.iloc[test_idx]\n",
    "        # print('train',train_df.shape)\n",
    "        # print('test',test_df.shape)\n",
    "\n",
    "        train_module = CSVBaseDataModule(\n",
    "        csv_file = train_df,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        nr_samples=nr_samples,\n",
    "        normalize=normalize,\n",
    "        augmentations=augmentations,\n",
    "        transform_override=transform_override,\n",
    "        batch_sampling_mode=batch_sampling_mode,\n",
    "        sr=sr,\n",
    "        multi_epoch=multi_epoch,\n",
    "        )\n",
    "\n",
    "        train_module.setup()\n",
    "        train_loader = train_module.train_dataloader()\n",
    "\n",
    "        test_module = CSVBaseDataModule(\n",
    "        csv_file = test_df,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        nr_samples=nr_samples,\n",
    "        normalize=normalize,\n",
    "        augmentations=augmentations,\n",
    "        transform_override=transform_override,\n",
    "        batch_sampling_mode=batch_sampling_mode,\n",
    "        sr=sr,\n",
    "        multi_epoch=multi_epoch,\n",
    "        )\n",
    "\n",
    "        test_module.setup()\n",
    "        test_loader = test_module.train_dataloader()\n",
    "\n",
    "        model_SOTA = SSLTrainer(\n",
    "        optimizer1_init = {'class_path': 'torch.optim.Adam', 'init_args': {'lr': 0.0001, 'weight_decay': 1e-5}},\n",
    "        feature_extractor = {'spec_layer': 'melspectogram', 'n_fft' : 2048, 'hop_length' : 512},\n",
    "        backbone = {'backbone' : \"efficientnet_b0\", 'pretrained' : True, 'embedding_dim': 1000},\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(max_epochs=13,max_steps = 1000, num_nodes = 1, accelerator='gpu', devices=1) \n",
    "\n",
    "        trainer.fit(model_SOTA, train_module)\n",
    "        \n",
    "        model = YourModel(model_cont=model_SOTA, lr=0.0004, weight_decay=1e-5, num_classes=num_classes)\n",
    "        \n",
    "        trainer = pl.Trainer(max_epochs=6, accelerator='gpu', devices=1)\n",
    "        trainer.fit(model, train_loader)\n",
    "        trainer.test(model, test_loader)\n",
    "\n",
    "        torch.save(model.state_dict(), f'/home/surge_siya/k{fold}.pth')\n",
    "        del model, model_SOTA\n",
    "# train_model(df, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "siya",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
