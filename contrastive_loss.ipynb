{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import (\n",
    "    AddGaussianNoise,\n",
    "    TimeStretch,\n",
    "    Shift,\n",
    "    Gain,\n",
    "    TanhDistortion,\n",
    "    SevenBandParametricEQ,\n",
    "    SevenBandParametricEQ,\n",
    "    TimeMask,\n",
    "    ApplyImpulseResponse,\n",
    ")\n",
    "\n",
    "from audiomentations.core.transforms_interface import BaseWaveformTransform\n",
    "import parselmouth\n",
    "import random\n",
    "\n",
    "# import librosa\n",
    "import numpy as np\n",
    "\n",
    "# from random import random\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "PRAAT_CHANGEGENDER_PITCHMEDIAN_DEFAULT = 0.0\n",
    "PRAAT_CHANGEGENDER_FORMANTSHIFTRATIO_DEFAULT = 1.0\n",
    "PRAAT_CHANGEGENDER_PITCHSHIFTRATIO_DEFAULT = 1.0\n",
    "PRAAT_CHANGEGENDER_PITCHRANGERATIO_DEFAULT = 1.0\n",
    "PRAAT_CHANGEGENDER_DURATIONFACTOR_DEFAULT = 1.0\n",
    "\n",
    "\n",
    "def aug(signal, augmentations_dict, override=False, sample_rate=44100):\n",
    "    \"\"\"Main augmentation function\"\"\"\n",
    "    # Augment the signal\n",
    "\n",
    "    sig_aug = signal\n",
    "    # If its not false it is a dict containing manual transforms to apply\n",
    "    if override is not False:\n",
    "        for transform in override.values():\n",
    "            sig_aug = transform(sig_aug)\n",
    "        return sig_aug\n",
    "    if augmentations_dict is False:\n",
    "        return signal\n",
    "    transforms = aug_factory(augmentations_dict)\n",
    "\n",
    "    for transform in transforms:\n",
    "        sig_aug = transform(sig_aug, sample_rate=sample_rate)\n",
    "    return sig_aug\n",
    "\n",
    "\n",
    "def aug_factory(augmentation):\n",
    "    augmentations = []\n",
    "\n",
    "    if augmentation.get(\"gaussian_noise\", 0):\n",
    "        augmentations.append(\n",
    "            AddGaussianNoise(\n",
    "                min_amplitude=0.07,\n",
    "                max_amplitude=0.7,\n",
    "                p=augmentation[\"gaussian_noise\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if augmentation.get(\"time_stretch\", 0):\n",
    "        augmentations.append(\n",
    "            TimeStretch(min_rate=0.8, max_rate=1.2, p=augmentation[\"time_stretch\"])\n",
    "        )\n",
    "\n",
    "    if augmentation.get(\"pitch_shift_naive\", 0):\n",
    "        # augmentations.append(PitchShift(min_semitones=-3, max_semitones=3,\n",
    "        # p=augmentation[\"time_stretch\"]))\n",
    "        # n_steps = random.choice((-4, 4, 3, -3))\n",
    "        # ps = lambda x, sample_rate=44100: np.cast[\"float32\"](\n",
    "        #     librosa.effects.pitch_shift(x, sr=sample_rate, n_steps=n_steps)\n",
    "        # )\n",
    "        # augmentations.append(ps)\n",
    "        pass\n",
    "\n",
    "    if augmentation.get(\"formant_shift_parselmouth_prob\", 0):\n",
    "        # if augmentation.get(\"formant_shift_parselmouth\", 0):\n",
    "        augmentations.append(\n",
    "            FormantShiftParselmouth(\n",
    "                augmentation[\"formant_shift_parselmouth\"],\n",
    "                p=augmentation[\"formant_shift_parselmouth_prob\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if augmentation.get(\"pitch_shift_parselmouth_prob\", 0):\n",
    "        if augmentation.get(\"pitch_shift_parselmouth\", 0):\n",
    "            pitch_shift_ratio = augmentation[\"pitch_shift_parselmouth\"]\n",
    "        else:\n",
    "            pitch_shift_ratio = 1\n",
    "\n",
    "        if augmentation.get(\"pitch_range_parselmouth\", 0):\n",
    "            pitch_range_ratio = augmentation[\"pitch_range_parselmouth\"]\n",
    "        else:\n",
    "            pitch_range_ratio = 1\n",
    "\n",
    "        augmentations.append(\n",
    "            PitchShiftParselmouth(\n",
    "                pitch_shift_ratio,\n",
    "                pitch_range_ratio,\n",
    "                p=augmentation[\"pitch_shift_parselmouth_prob\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if augmentation.get(\"shift\", 0):\n",
    "        augmentations.append(\n",
    "            Shift(\n",
    "                min_fraction=-0.05,\n",
    "                max_fraction=0.2,\n",
    "                p=augmentation[\"shift\"],\n",
    "                rollover=True,\n",
    "                fade=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if augmentation.get(\"gain\", 0):\n",
    "        augmentations.append(\n",
    "            Gain(min_gain_in_db=-6, max_gain_in_db=0, p=augmentation[\"gain\"])\n",
    "        )\n",
    "\n",
    "    if augmentation.get(\"parametric_eq\", 0):\n",
    "        augmentations.append(\n",
    "            SevenBandParametricEQ(\n",
    "                min_gain_db=-2, max_gain_db=1, p=augmentation[\"parametric_eq\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if augmentation.get(\"tanh_distortion\", 0):\n",
    "        augmentations.append(\n",
    "            TanhDistortion(\n",
    "                min_distortion=0.1,\n",
    "                max_distortion=0.2,\n",
    "                p=augmentation[\"tanh_distortion\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if augmentation.get(\"time_mask\", 0):\n",
    "        augmentations.append(TimeMask(min_band_part=0.2, max_band_part=0.6, p=augmentation[\"time_mask\"]))\n",
    "\n",
    "    if augmentation.get(\"reverb\", 0):\n",
    "        ir_path = augmentation[\"reverb_path\"]\n",
    "        warnings.filterwarnings(\n",
    "            \"ignore\", message=\".* had to be resampled from 16000 hz to 44100 hz.*\"\n",
    "        )\n",
    "        augmentations.append(ApplyImpulseResponse(ir_path, p=augmentation[\"reverb\"]))\n",
    "\n",
    "    return augmentations\n",
    "\n",
    "\n",
    "class PitchShiftParselmouth(BaseWaveformTransform):\n",
    "    \"\"\"Pitch shift the sound up or down without changing the tempo\"\"\"\n",
    "\n",
    "    def __init__(self, pitch_ratio=1.4, range_ratio=1.3, p=0.5):\n",
    "        super().__init__(p)\n",
    "\n",
    "        self.range_ratio = range_ratio\n",
    "        self.init_range = 1\n",
    "        self.enable_reciprocal = True\n",
    "        if type(pitch_ratio) is list:\n",
    "            self.init_range = float(pitch_ratio[0])\n",
    "            pitch_ratio = float(pitch_ratio[1])\n",
    "            # self.enable_reciprocal = True\n",
    "\n",
    "        self.pitch_ratio = pitch_ratio\n",
    "\n",
    "    def randomize_parameters(self, samples, sample_rate):\n",
    "        super().randomize_parameters(samples, sample_rate)\n",
    "        if self.parameters[\"should_apply\"]:\n",
    "            self.parameters[\"pitch_shift_ratio\"] = random.uniform(\n",
    "                self.init_range, self.pitch_ratio\n",
    "            )\n",
    "\n",
    "            if self.enable_reciprocal:\n",
    "                use_reciprocal = random.uniform(-1, 1) > 0\n",
    "                if use_reciprocal:\n",
    "                    self.parameters[\"pitch_shift_ratio\"] = (\n",
    "                        1 / self.parameters[\"pitch_shift_ratio\"]\n",
    "                    )\n",
    "\n",
    "            self.parameters[\"pitch_range_ratio\"] = random.uniform(1, self.range_ratio)\n",
    "\n",
    "            use_reciprocal = random.uniform(-1, 1) > 0\n",
    "            if use_reciprocal:\n",
    "                self.parameters[\"pitch_range_ratio\"] = (\n",
    "                    1 / self.parameters[\"pitch_range_ratio\"]\n",
    "                )\n",
    "\n",
    "    def apply(self, samples, sample_rate):\n",
    "        # Add a check to see if samples is numpy array\n",
    "        if not isinstance(samples, np.ndarray):\n",
    "            samples = np.array(samples)\n",
    "            print(\"samples is not numpy array, converting to numpy array\")\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                action=\"ignore\",\n",
    "                category=parselmouth.PraatWarning,\n",
    "                message=\"This application uses RandomPool, which is BROKEN in older releases\",\n",
    "            )\n",
    "\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            pitch_shifted_samples = apply_formant_and_pitch_shift(\n",
    "                wav_to_Sound(samples, sampling_frequency=sample_rate),\n",
    "                pitch_shift_ratio=self.parameters[\"pitch_shift_ratio\"],\n",
    "                pitch_range_ratio=self.parameters[\"pitch_range_ratio\"],\n",
    "                duration_factor=1.0,\n",
    "            )\n",
    "        return np.squeeze(np.cast[\"float32\"](pitch_shifted_samples.values))\n",
    "\n",
    "\n",
    "class FormantShiftParselmouth(BaseWaveformTransform):\n",
    "    \"\"\"Formant shift using parselmouth\"\"\"\n",
    "\n",
    "    def __init__(self, formant_shift=1.4, p=0.5):\n",
    "        super().__init__(p)\n",
    "        self.init_range = 1\n",
    "        self.enable_reciprocal = True\n",
    "        if type(formant_shift) is list:\n",
    "            self.init_range = float(formant_shift[0])\n",
    "            formant_shift = float(formant_shift[1])\n",
    "            self.enable_reciprocal = True\n",
    "\n",
    "        self.formant_shift = formant_shift\n",
    "\n",
    "    def randomize_parameters(self, samples, sample_rate):\n",
    "        super().randomize_parameters(samples, sample_rate)\n",
    "\n",
    "        if self.parameters[\"should_apply\"]:\n",
    "            self.parameters[\"formant_shift_parselmouth\"] = random.uniform(\n",
    "                self.init_range, self.formant_shift\n",
    "            )\n",
    "\n",
    "            if self.enable_reciprocal:\n",
    "                use_reciprocal = random.uniform(-1, 1) > 0\n",
    "                if use_reciprocal:\n",
    "                    self.parameters[\"formant_shift_parselmouth\"] = (\n",
    "                        1 / self.parameters[\"formant_shift_parselmouth\"]\n",
    "                    )\n",
    "\n",
    "    def apply(self, samples, sample_rate):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                action=\"ignore\",\n",
    "                category=parselmouth.PraatWarning,\n",
    "                message=\"This application uses RandomPool, which is BROKEN in older releases\",\n",
    "            )\n",
    "\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            formant_shifted_samples = apply_formant_and_pitch_shift(\n",
    "                wav_to_Sound(samples, sampling_frequency=sample_rate),\n",
    "                formant_shift_ratio=self.parameters[\"formant_shift_parselmouth\"],\n",
    "                duration_factor=1.0,\n",
    "            )\n",
    "        return np.squeeze(np.cast[\"float32\"](formant_shifted_samples.values))\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\" Parselmouth utils for pitch and formant shifting. \n",
    "    Part of the code is adapted from https://github.com/dhchoi99/NANSY\\\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def wav_to_Sound(wav, sampling_frequency: int = 44100) -> parselmouth.Sound:\n",
    "    r\"\"\" load wav file to parselmouth Sound file\n",
    "    # __init__(self: parselmouth.Sound, other: parselmouth.Sound) -> None \\\n",
    "    # __init__(self: parselmouth.Sound, values: numpy.ndarray[numpy.float64], \n",
    "            sampling_frequency: Positive[float] = 44100.0, start_time: float = 0.0) -> None \\\n",
    "    # __init__(self: parselmouth.Sound, file_path: str) -> None\n",
    "    returns:\n",
    "        sound: parselmouth.Sound\n",
    "    \"\"\"\n",
    "    if isinstance(wav, parselmouth.Sound):\n",
    "        sound = wav\n",
    "    elif isinstance(wav, np.ndarray):\n",
    "        sound = parselmouth.Sound(wav, sampling_frequency=sampling_frequency)\n",
    "    elif isinstance(wav, list):\n",
    "        wav_np = np.asarray(wav)\n",
    "        sound = parselmouth.Sound(\n",
    "            np.asarray(wav_np), sampling_frequency=sampling_frequency\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return sound\n",
    "\n",
    "\n",
    "def get_pitch_median(wav, sr: int = None):\n",
    "    sound = wav_to_Sound(wav, sr)\n",
    "    pitch = None\n",
    "    pitch_median = PRAAT_CHANGEGENDER_PITCHMEDIAN_DEFAULT\n",
    "\n",
    "    try:\n",
    "        pitch = parselmouth.praat.call(sound, \"To Pitch\", 0.8 / 75, 75, 600)\n",
    "        pitch_median = parselmouth.praat.call(\n",
    "            pitch, \"Get quantile\", 0.0, 0.0, 0.5, \"Hertz\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        pass\n",
    "\n",
    "    return pitch, pitch_median\n",
    "\n",
    "\n",
    "def change_gender(\n",
    "    sound,\n",
    "    pitch=None,\n",
    "    formant_shift_ratio: float = PRAAT_CHANGEGENDER_FORMANTSHIFTRATIO_DEFAULT,\n",
    "    new_pitch_median: float = PRAAT_CHANGEGENDER_PITCHMEDIAN_DEFAULT,\n",
    "    pitch_range_ratio: float = PRAAT_CHANGEGENDER_PITCHRANGERATIO_DEFAULT,\n",
    "    duration_factor: float = PRAAT_CHANGEGENDER_DURATIONFACTOR_DEFAULT,\n",
    ") -> parselmouth.Sound:\n",
    "    try:\n",
    "        if pitch is None:\n",
    "            new_sound = parselmouth.praat.call(\n",
    "                sound,\n",
    "                \"Change gender\",\n",
    "                75,\n",
    "                600,\n",
    "                formant_shift_ratio,\n",
    "                new_pitch_median,\n",
    "                pitch_range_ratio,\n",
    "                duration_factor,\n",
    "            )\n",
    "        else:\n",
    "            new_sound = parselmouth.praat.call(\n",
    "                (sound, pitch),\n",
    "                \"Change gender\",\n",
    "                formant_shift_ratio,\n",
    "                new_pitch_median,\n",
    "                pitch_range_ratio,\n",
    "                duration_factor,\n",
    "            )\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "    return new_sound\n",
    "\n",
    "\n",
    "def apply_formant_and_pitch_shift(\n",
    "    sound: parselmouth.Sound,\n",
    "    formant_shift_ratio: float = PRAAT_CHANGEGENDER_FORMANTSHIFTRATIO_DEFAULT,\n",
    "    pitch_shift_ratio: float = PRAAT_CHANGEGENDER_PITCHSHIFTRATIO_DEFAULT,\n",
    "    pitch_range_ratio: float = PRAAT_CHANGEGENDER_PITCHRANGERATIO_DEFAULT,\n",
    "    duration_factor: float = PRAAT_CHANGEGENDER_DURATIONFACTOR_DEFAULT,\n",
    ") -> parselmouth.Sound:\n",
    "    \"\"\"uses praat 'Change Gender' backend to manipulate pitch and formant\n",
    "    'Change Gender' function: praat -> Sound Object -> Convert -> Change Gender\n",
    "    see Help of Praat for more details\n",
    "    # https://github.com/YannickJadoul/Parselmouth/issues/25#issuecomment-608632887 might help\n",
    "    \"\"\"\n",
    "\n",
    "    # pitch = sound.to_pitch()\n",
    "    pitch = None\n",
    "    new_pitch_median = PRAAT_CHANGEGENDER_PITCHMEDIAN_DEFAULT\n",
    "    if pitch_shift_ratio != 1.0:\n",
    "        try:\n",
    "            pitch, pitch_median = get_pitch_median(sound, None)\n",
    "            new_pitch_median = pitch_median * pitch_shift_ratio\n",
    "\n",
    "            # https://github.com/praat/praat/issues/1926#issuecomment-974909408\n",
    "            pitch_minimum = parselmouth.praat.call(\n",
    "                pitch, \"Get minimum\", 0.0, 0.0, \"Hertz\", \"Parabolic\"\n",
    "            )\n",
    "            newMedian = pitch_median * pitch_shift_ratio\n",
    "            scaledMinimum = pitch_minimum * pitch_shift_ratio\n",
    "            resultingMinimum = (\n",
    "                newMedian + (scaledMinimum - newMedian) * pitch_range_ratio\n",
    "            )\n",
    "            if resultingMinimum < 0:\n",
    "                new_pitch_median = PRAAT_CHANGEGENDER_PITCHMEDIAN_DEFAULT\n",
    "                pitch_range_ratio = PRAAT_CHANGEGENDER_PITCHRANGERATIO_DEFAULT\n",
    "\n",
    "            if math.isnan(new_pitch_median):\n",
    "                new_pitch_median = PRAAT_CHANGEGENDER_PITCHMEDIAN_DEFAULT\n",
    "                pitch_range_ratio = PRAAT_CHANGEGENDER_PITCHRANGERATIO_DEFAULT\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    new_sound = change_gender(\n",
    "        sound,\n",
    "        pitch,\n",
    "        formant_shift_ratio,\n",
    "        new_pitch_median,\n",
    "        pitch_range_ratio,\n",
    "        duration_factor,\n",
    "    )\n",
    "\n",
    "    return new_sound\n",
    "\n",
    "\n",
    "def semitones_to_ratio(x):\n",
    "    return 2 ** (x / 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVBaseDictDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_file,\n",
    "        nr_samples,\n",
    "        normalize=True,\n",
    "        augmentations={},\n",
    "        transform_override=False,\n",
    "        batch_sampling_mode=\"sample_clips\",\n",
    "        sr=44100,\n",
    "        multi_epoch=1,\n",
    "        phase = 'TRAINING'\n",
    "    ):\n",
    "        self.csv_file = csv_file\n",
    "        self.nr_samples = nr_samples\n",
    "        self.normalize = normalize\n",
    "        self.augmentations = augmentations\n",
    "        self.transform_override = transform_override\n",
    "        self.batch_sampling_mode = batch_sampling_mode\n",
    "        self.sr = sr\n",
    "        self.multi_epoch = multi_epoch\n",
    "        self.phase = phase\n",
    "        self.groups, self.file_list, self.labels = self._prepare_groups_from_csv(csv_file)\n",
    "        self.group_names = list(self.groups.keys())\n",
    "\n",
    "    def _prepare_groups_from_csv(self, csv_file):\n",
    "        df = pd.read_csv(csv_file, index_col=0)\n",
    "        groups = {}\n",
    "        file_list = []\n",
    "        labels = []\n",
    "        df = df[df.iloc[:,1] != 'Unlabelled']\n",
    "        df = df[df.iloc[:,1] != 'Other']\n",
    "        df = df[df.iloc[:,-1] != 'Speech']\n",
    "        df = df.reset_index(drop = True)\n",
    "        # print(df.shape)\n",
    "        # print(df)\n",
    "        singers = np.unique(df['Singer'])\n",
    "        singer_to_idx = {singer: idx for idx, singer in enumerate(singers)}\n",
    "        print(singer_to_idx)\n",
    "        for _, row in df.iterrows():\n",
    "            if self.phase.upper() == 'TRAINING':\n",
    "                if ((row['Labeled'] == 1) & (row['Singer'] != 'Other') & (row['Singer'] != 'Unlabelled') &  (row['panns'] != 'Speech')):                \n",
    "                    group_name = singer_to_idx[row['Singer']]\n",
    "                    if group_name not in groups:\n",
    "                        groups[group_name] = []\n",
    "                    file_path = row['Audio_Path']\n",
    "                    groups[group_name].append(file_path)\n",
    "                    file_list.append(file_path)\n",
    "                    labels.append(group_name)\n",
    "\n",
    "            elif self.phase.upper() == 'TESTING':\n",
    "                if ((row['Labeled'] == 0) & (row['Singer'] != 'Other') & (row['Singer'] != 'Unlabelled') &  (row['panns'] != 'Speech')):                \n",
    "                    group_name = singer_to_idx[row['Singer']]\n",
    "                    if group_name not in groups:\n",
    "                        groups[group_name] = []\n",
    "                    file_path = row['Audio_Path']\n",
    "                    groups[group_name].append(file_path)\n",
    "                    file_list.append(file_path)\n",
    "                    labels.append(group_name)\n",
    "        return groups, file_list, labels\n",
    "\n",
    "    def get_fragment(self, file):\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(file)\n",
    "            # print(waveform.shape)\n",
    "\n",
    "            if sr != self.sr:\n",
    "                resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sr)\n",
    "                waveform = resampler(waveform)\n",
    "\n",
    "            if waveform.size(1) > self.nr_samples:\n",
    "                waveform = waveform[:, :self.nr_samples]\n",
    "            else:\n",
    "                padding = self.nr_samples - waveform.size(1)\n",
    "                waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "\n",
    "            if self.normalize:\n",
    "                waveform = (waveform - waveform.mean()) / waveform.std()\n",
    "\n",
    "            return waveform\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading fragment from {file}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list) * self.multi_epoch\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx % len(self.file_list)\n",
    "        file = self.file_list[idx]\n",
    "        label = self.labels[idx]\n",
    "        fragment = self.get_fragment(file)\n",
    "        if fragment is None:\n",
    "            raise ValueError(f\"Failed to load audio fragment from {file}\")\n",
    "        fragment1 = aug(\n",
    "            np.cast[\"float32\"](fragment),\n",
    "            self.augmentations,\n",
    "            sample_rate=self.sr,\n",
    "        )\n",
    "        # print('fragment',fragment.shape)\n",
    "        # print('fragment_aug', fragment1.shape)\n",
    "        return {\"clip1\": fragment, \"clip2\": fragment1, \"group_name\": label, 'audio_files': file}\n",
    "\n",
    "    def _count_files_in_dict(self, d):\n",
    "        return sum(len(files) for files in d.values())\n",
    "\n",
    "    def _count_elements_in_dict_split(self, d):\n",
    "        return len(d)\n",
    "\n",
    "    def _merge_groups(self, groups):\n",
    "        merged_groups = {}\n",
    "        for k, v in groups.items():\n",
    "            if k not in merged_groups:\n",
    "                merged_groups[k] = []\n",
    "            merged_groups[k].extend(v)\n",
    "        return merged_groups\n",
    "\n",
    "    def _print_dataset_files_info(self):\n",
    "        total_files = self._count_files_in_dict(self.groups)\n",
    "        print(f\"Total files: {total_files}\")\n",
    "        for group, files in self.groups.items():\n",
    "            print(f\"Group: {group}, Files: {len(files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVBaseDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_file,\n",
    "        batch_size=32,\n",
    "        num_workers=0, \n",
    "        nr_samples=64000,\n",
    "        normalize=True,\n",
    "        augmentations={},\n",
    "        transform_override=False,\n",
    "        batch_sampling_mode=\"sample_clips\",\n",
    "        sr=16000,\n",
    "        multi_epoch=1,\n",
    "        phase = 'TRAINING'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.csv_file = csv_file\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.nr_samples = nr_samples\n",
    "        self.normalize = normalize\n",
    "        self.augmentations = augmentations\n",
    "        self.transform_override = transform_override\n",
    "        self.batch_sampling_mode = batch_sampling_mode\n",
    "        self.sr = sr\n",
    "        self.multi_epoch = multi_epoch\n",
    "        self.phase = phase\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Any data downloading or preparation logic should be placed here\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.dataset = CSVBaseDictDataset(\n",
    "            csv_file=self.csv_file,\n",
    "            nr_samples=self.nr_samples,\n",
    "            normalize=self.normalize,\n",
    "            augmentations=self.augmentations,\n",
    "            transform_override=self.transform_override,\n",
    "            batch_sampling_mode=self.batch_sampling_mode,\n",
    "            sr=self.sr,\n",
    "            multi_epoch=self.multi_epoch,\n",
    "            phase = self.phase\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return None\n",
    "\n",
    "    def process_dataset_dirs(self):\n",
    "        # Custom logic for processing dataset directories if required\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A.S. MURALI': 0, 'AFROZ BANO': 1, 'AJAY CHAKRAVARTY': 2, 'ASLAM SABRI & PARTY': 3, 'BAKUL CHATTOPADHYAY': 4, 'BALDEV SHARAN NARANG': 5, 'BHAI NARMAL SINGH': 6, 'Bal Gandharva': 7, 'Chorus (Students)': 8, 'DINA NATH MISHRA': 9, 'DR. BASAVARAJ RAJGURU': 10, 'DR. NIRMALA SUNDARA RAJAN': 11, 'DR. NIVEDITA SINGH': 12, 'DR. PANTULA RAMA': 13, 'DWARAM DURGA PRASAD RAO & DWARAM SATYANARAYANA RAO': 14, 'GAYATRY GIRISH': 15, 'K. SARASWATHI VIDYARTHI': 16, 'KOYAL DAS GUPTA': 17, 'LOU MAJAW (SOLO)': 18, 'M K SHANKARA NAMBOODRI': 19, 'M. K. PRANESH': 20, 'M. VENKATESH KUMAR.': 21, 'M.K.SWAMINATH PILLAI & D.MURUGESA PILLAI': 22, 'M.M. DANDAPANI DESIKAR': 23, 'M.S. SHEELA': 24, 'MALA SHRI PRASAD': 25, 'MALLADI SURI BABU': 26, 'MANDIRA LAHIRI': 27, 'MANJUSHA KULKARNI PATIL': 28, 'MEETA  PANDIT': 29, 'MOHINDER SINGH': 30, 'MUJAHID HUSSAIN KHAN': 31, 'NAMAGIRIPETTAI .K.KRISHNAN': 32, 'PARMESHWAR HEGDE': 33, 'PITTUKULI MURUGADAS': 34, 'PT. DHARAM NATH MISHRA': 35, 'PT. MADHAV INGALE': 36, 'PT. SIDDARAMA JAMBALADINNI': 37, 'R. SANTANA GOPALAN': 38, 'RAJAN MISHRA & PT. SAJAN MISHRA': 39, 'RAJINDAR SINGH': 40, 'RAJSHREE PATHAK': 41, 'RAMNATH AHEERVAAR': 42, 'RITA GANGULI': 43, 'RITESH & RAJNISH MISHRA': 44, 'S. BALAMBAL': 45, 'S. RUKMINI': 46, 'S. SOWMYA': 47, 'SAVITA DEVI': 48, 'SINGH BANDHU': 49, 'SITARAM SINGH': 50, 'SMT. AVANTIKA CHAKRAVARTI': 51, 'SMT. GEETHA RAJA': 52, 'SMT. KALPANA JHOKARKAR': 53, 'SMT. KAMATCHI KUPPUSWAMI': 54, 'SMT. NITHYASREE MAHADEVAN': 55, 'SMT. VIDYA KATGADE': 56, 'SREEVALSAN J. MENON': 57, 'SUBHRA GUHA': 58, 'T. R. Mahalingam': 59, 'TIRUVAIYARU BV JAYASHREE': 60, 'TUSHAR DUTTA': 61, 'USTAD GHULAM SADIQ KHAN': 62, 'USTAD SALEEM IQBAL': 63, 'VALLAM KRISHNASWAMI PILLAI & K.P.GOVINDARAJA PILLAI': 64, 'VASANTHI KRISHNA RAO': 65, 'VIDUSHI NEELA RAMGOPAL': 66, 'VIDUSHI SUMITRA GUHA': 67, 'YASHPAL': 68}\n"
     ]
    }
   ],
   "source": [
    "csv_file = '/home/surge_siya/ALL CSV/Panns_CSV.csv'\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "# nr_samples = 160000\n",
    "nr_samples = 480000\n",
    "normalize = True\n",
    "augmentations = {\n",
    "    \"enable\": True,\n",
    "    \"gaussian_noise\": 0.5,\n",
    "    \"pitch_shift_naive\": 0,\n",
    "    \"time_stretch\": 0,\n",
    "    \"gain\": 0.5,\n",
    "    \"shift\": 0,\n",
    "    \"parametric_eq\": 0,\n",
    "    \"tanh_distortion\": 0,\n",
    "    \"time_mask\": 0.7,\n",
    "    # \"formant_shift_parselmouth\": 0,\n",
    "    # \"pitch_shift_parselmouth\": [1, 1.3],\n",
    "    # \"pitch_range_parselmouth\": 1.5,\n",
    "    # \"pitch_shift_parselmouth_prob\": 0.5\n",
    "}\n",
    "transform_override = False\n",
    "batch_sampling_mode = \"sample_clips\"\n",
    "sr = 16000\n",
    "multi_epoch = 1\n",
    "\n",
    "data_module = CSVBaseDataModule(\n",
    "    csv_file=csv_file,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    nr_samples=nr_samples,\n",
    "    normalize=normalize,\n",
    "    augmentations=augmentations,\n",
    "    transform_override=transform_override,\n",
    "    batch_sampling_mode=batch_sampling_mode,\n",
    "    sr=sr,\n",
    "    multi_epoch=multi_epoch,\n",
    ")\n",
    "\n",
    "data_module.setup()\n",
    "train_loader = data_module.train_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models, network_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Union, Callable, List, Optional\n",
    "from torchvision.models import efficientnet_b0, efficientnet_b4\n",
    "import torchvision.transforms as vt\n",
    "\n",
    "\n",
    "def get_vision_backbone(\n",
    "    vismod=\"efficientnet_b0\", num_classes=1000, pretrained=False, **kwargs\n",
    "):\n",
    "    if vismod == \"efficientnet_b0\":\n",
    "        return efficientnet_b0(pretrained=pretrained, num_classes=num_classes, **kwargs)\n",
    "    elif vismod == \"efficientnet_b4\":\n",
    "        return efficientnet_b4(pretrained=pretrained, num_classes=num_classes, **kwargs)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Grey2Rgb(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.normalize = vt.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        batch_size, freq_bins, times = data.shape\n",
    "        data /= data.max()\n",
    "        data = data.unsqueeze(1).expand(batch_size, 3, freq_bins, times)\n",
    "        data = self.normalize(data)\n",
    "        return data\n",
    "\n",
    "\n",
    "class LogScale(nn.Module):\n",
    "    def forward(self, data):\n",
    "        # eps = 1e-8\n",
    "        eps = torch.tensor(1e-8, device=data.device)\n",
    "        return torch.log(data + eps)\n",
    "\n",
    "\n",
    "class Aggregator(nn.Module):\n",
    "    \"\"\"Aggregates (in time) a list of features\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.aggregation = nn.Sequential(nn.AdaptiveAvgPool1d(1), nn.Flatten(1))\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            outputs_feature: torch.Tensor of shape(B x C x t)\n",
    "        \"\"\"\n",
    "        if isinstance(features, list):\n",
    "            output_feature = [self.aggregation(feature) for feature in features]\n",
    "        else:\n",
    "            output_feature = self.aggregation(features)\n",
    "        return output_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.error\n",
    "import pathlib\n",
    "import logging\n",
    "from enum import Enum\n",
    "import huggingface_hub\n",
    "from typing import Union\n",
    "from collections import namedtuple\n",
    "from requests.exceptions import HTTPError\n",
    "import hashlib\n",
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _missing_ok_unlink(path):\n",
    "    # missing_ok=True was added to Path.unlink() in Python 3.8\n",
    "    # This does the same.\n",
    "    try:\n",
    "        path.unlink()\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "\n",
    "class FetchFrom(Enum):\n",
    "    \"\"\"Designator where to fetch models/audios from.\n",
    "\n",
    "    Note: HuggingFace repository sources and local folder sources may be confused if their source type is undefined.\n",
    "    \"\"\"\n",
    "\n",
    "    LOCAL = 1\n",
    "    HUGGING_FACE = 2\n",
    "    URI = 3\n",
    "\n",
    "\n",
    "# For easier use\n",
    "FetchSource = namedtuple(\"FetchSource\", [\"FetchFrom\", \"path\"])\n",
    "FetchSource.__doc__ = \"\"\"NamedTuple describing a source path and how to fetch it\"\"\"\n",
    "FetchSource.__hash__ = lambda self: hash(self.path)\n",
    "FetchSource.encode = lambda self, *args, **kwargs: \"_\".join(\n",
    "    (str(self.path), str(self.FetchFrom))\n",
    ").encode(*args, **kwargs)\n",
    "# FetchSource.__str__ = lambda self: str(self.path)\n",
    "\n",
    "\n",
    "def fetch(\n",
    "    filename,\n",
    "    source,\n",
    "    savedir=\"./pretrained_model_checkpoints\",\n",
    "    overwrite=False,\n",
    "    save_filename=None,\n",
    "    use_auth_token=False,\n",
    "    revision=None,\n",
    "    cache_dir: Union[str, pathlib.Path, None] = None,\n",
    "    silent_local_fetch: bool = False,\n",
    "):\n",
    "    \"\"\"Ensures you have a local copy of the file, returns its path\n",
    "\n",
    "    In case the source is an external location, downloads the file.  In case\n",
    "    the source is already accessible on the filesystem, creates a symlink in\n",
    "    the savedir. Thus, the side effects of this function always look similar:\n",
    "    savedir/save_filename can be used to access the file. And save_filename\n",
    "    defaults to the filename arg.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    filename : str\n",
    "        Name of the file including extensions.\n",
    "    source : str or FetchSource\n",
    "        Where to look for the file. This is interpreted in special ways:\n",
    "        First, if the source begins with \"http://\" or \"https://\", it is\n",
    "        interpreted as a web address and the file is downloaded.\n",
    "        Second, if the source is a valid directory path, a symlink is\n",
    "        created to the file.\n",
    "        Otherwise, the source is interpreted as a Huggingface model hub ID, and\n",
    "        the file is downloaded from there.\n",
    "    savedir : str\n",
    "        Path where to save downloads/symlinks.\n",
    "    overwrite : bool\n",
    "        If True, always overwrite existing savedir/filename file and download\n",
    "        or recreate the link. If False (as by default), if savedir/filename\n",
    "        exists, assume it is correct and don't download/relink. Note that\n",
    "        Huggingface local cache is always used - with overwrite=True we just\n",
    "        relink from the local cache.\n",
    "    save_filename : str\n",
    "        The filename to use for saving this file. Defaults to filename if not\n",
    "        given.\n",
    "    use_auth_token : bool (default: False)\n",
    "        If true Hugginface's auth_token will be used to load private models from the HuggingFace Hub,\n",
    "        default is False because majority of models are public.\n",
    "    revision : str\n",
    "        The model revision corresponding to the HuggingFace Hub model revision.\n",
    "        This is particularly useful if you wish to pin your code to a particular\n",
    "        version of a model hosted at HuggingFace.\n",
    "    cache_dir: str or Path (default: None)\n",
    "        Location of HuggingFace cache for storing pre-trained models, to which symlinks are created.\n",
    "    silent_local_fetch: bool (default: False)\n",
    "        Surpress logging messages (quiet mode).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pathlib.Path\n",
    "        Path to file on local file system.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If file is not found\n",
    "    \"\"\"\n",
    "    if save_filename is None:\n",
    "        save_filename = filename\n",
    "    savedir = pathlib.Path(savedir)\n",
    "    savedir.mkdir(parents=True, exist_ok=True)\n",
    "    fetch_from = None\n",
    "    if isinstance(source, FetchSource):\n",
    "        fetch_from, source = source\n",
    "    sourcefile = f\"{source}/{filename}\"\n",
    "    if pathlib.Path(source).is_dir() and fetch_from not in [\n",
    "        FetchFrom.HUGGING_FACE,\n",
    "        FetchFrom.URI,\n",
    "    ]:\n",
    "        # Interpret source as local directory path & return it as destination\n",
    "        sourcepath = pathlib.Path(sourcefile).absolute()\n",
    "        MSG = f\"Destination {filename}: local file in {str(sourcepath)}.\"\n",
    "        if not silent_local_fetch:\n",
    "            logger.info(MSG)\n",
    "        return sourcepath\n",
    "    destination = savedir / save_filename\n",
    "    if destination.exists() and not overwrite:\n",
    "        MSG = f\"Fetch {filename}: Using existing file/symlink in {str(destination)}.\"\n",
    "        logger.info(MSG)\n",
    "        return destination\n",
    "    if (\n",
    "        str(source).startswith(\"http:\") or str(source).startswith(\"https:\")\n",
    "    ) or fetch_from is FetchFrom.URI:\n",
    "        # Interpret source as web address.\n",
    "        MSG = f\"Fetch {filename}: Downloading from normal URL {str(sourcefile)}.\"\n",
    "        logger.info(MSG)\n",
    "        # Download\n",
    "        try:\n",
    "            urllib.request.urlretrieve(sourcefile, destination)\n",
    "        except urllib.error.URLError:\n",
    "            raise ValueError(\n",
    "                f\"Interpreted {source} as web address, but could not download.\"\n",
    "            )\n",
    "    else:  # FetchFrom.HUGGING_FACE check is spared (no other option right now)\n",
    "        # Interpret source as huggingface hub ID\n",
    "        # Use huggingface hub's fancy cached download.\n",
    "        MSG = f\"Fetch {filename}: Delegating to Huggingface hub, source {str(source)}.\"\n",
    "        print(MSG)\n",
    "        logger.info(MSG)\n",
    "        try:\n",
    "            fetched_file = huggingface_hub.hf_hub_download(\n",
    "                repo_id=source,\n",
    "                filename=filename,\n",
    "                use_auth_token=use_auth_token,\n",
    "                revision=revision,\n",
    "                cache_dir=cache_dir,\n",
    "            )\n",
    "            logger.info(f\"HF fetch: {fetched_file}\")\n",
    "        except HTTPError as e:\n",
    "            if \"404 Client Error\" in str(e):\n",
    "                raise ValueError(\"File not found on HF hub\")\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        # Huggingface hub downloads to etag filename, symlink to the expected one:\n",
    "        sourcepath = pathlib.Path(fetched_file).absolute()\n",
    "        # Create destination directory if it does not exist\n",
    "        destination.parent.mkdir(parents=True, exist_ok=True)\n",
    "        _missing_ok_unlink(destination)\n",
    "        destination.symlink_to(sourcepath)\n",
    "    return destination\n",
    "\n",
    "\n",
    "def from_hparams(\n",
    "    cls,\n",
    "    source,\n",
    "    hparams_file=\"hyperparams.yaml\",\n",
    "    weights_file=\"model.pt\",\n",
    "    pymodule_file=\"custom.py\",\n",
    "    overrides={},\n",
    "    savedir=None,\n",
    "    use_auth_token=False,\n",
    "    revision=None,\n",
    "    download_only=False,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Fetch and load based from outside source based on HyperPyYAML file\n",
    "\n",
    "    The source can be a location on the filesystem or online/huggingface\n",
    "\n",
    "    You can use the pymodule_file to include any custom implementations\n",
    "    that are needed: if that file exists, then its location is added to\n",
    "    sys.path before Hyperparams YAML is loaded, so it can be referenced\n",
    "    in the YAML.\n",
    "\n",
    "    The hyperparams file should contain a \"modules\" key, which is a\n",
    "    dictionary of torch modules used for computation.\n",
    "\n",
    "    The hyperparams file should contain a \"pretrainer\" key, which is a\n",
    "    speechbrain.utils.parameter_transfer.Pretrainer\n",
    "\n",
    "    Adapted from https://github.com/speechbrain/\n",
    "\n",
    "    \"\"\"\n",
    "    if savedir is None:\n",
    "        clsname = cls.__name__\n",
    "        savedir = f\"./pretrained_models/{clsname}-{hashlib.md5(source.encode('UTF-8', errors='replace')).hexdigest()}\"\n",
    "    hparams_local_path = fetch(\n",
    "        filename=hparams_file,\n",
    "        source=source,\n",
    "        savedir=savedir,\n",
    "        overwrite=False,\n",
    "        save_filename=None,\n",
    "        use_auth_token=use_auth_token,\n",
    "        revision=revision,\n",
    "    )\n",
    "    weights_local_path = fetch(\n",
    "        filename=weights_file,\n",
    "        source=source,\n",
    "        savedir=savedir,\n",
    "        overwrite=False,\n",
    "        save_filename=None,\n",
    "        use_auth_token=use_auth_token,\n",
    "        revision=revision,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        pymodule_local_path = fetch(\n",
    "            filename=pymodule_file,\n",
    "            source=source,\n",
    "            savedir=savedir,\n",
    "            overwrite=False,\n",
    "            save_filename=None,\n",
    "            use_auth_token=use_auth_token,\n",
    "            revision=revision,\n",
    "        )\n",
    "        sys.path.append(str(pymodule_local_path.parent))\n",
    "    except ValueError:\n",
    "        if pymodule_file == \"custom.py\":\n",
    "            # The optional custom Python module file did not exist\n",
    "            # and had the default name\n",
    "            pass\n",
    "        else:\n",
    "            # Custom Python module file not found, but some other\n",
    "            # filename than the default was given.\n",
    "            raise\n",
    "\n",
    "    # Load the modules:\n",
    "    # with open(hparams_local_path) as fin:\n",
    "    #     hparams = load_hyperpyyaml(fin, overrides)\n",
    "\n",
    "    hparams = yaml.safe_load(open(hparams_local_path, \"r\"))\n",
    "\n",
    "    # Load on the CPU. Later the params can be moved elsewhere by specifying\n",
    "    if not download_only:\n",
    "        # Now return the system\n",
    "        model_class = cls(**hparams, **kwargs)\n",
    "        model_class.load_state_dict(torch.load(weights_local_path, map_location=\"cpu\"))\n",
    "        print(\"Model loaded from\", weights_local_path)\n",
    "        return model_class\n",
    "\n",
    "\n",
    "def from_scripted(filename, source, savedir=None):\n",
    "    \"\"\"Load a model from a scripted file\"\"\"\n",
    "    if savedir is None:\n",
    "        savedir = f\"./pretrained_models/{filename}-{hashlib.md5(source.encode('UTF-8', errors='replace')).hexdigest()}\"\n",
    "    filename = filename + \".ts\" if not filename.endswith(\".ts\") else filename\n",
    "\n",
    "    print(filename, source, savedir)\n",
    "    model_file = fetch(\n",
    "        filename=filename,\n",
    "        source=source,\n",
    "        savedir=savedir,\n",
    "    )\n",
    "\n",
    "    model = torch.jit.load(model_file)\n",
    "    print(\"Model loaded from\", model_file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Optional, List, Callable, Union\n",
    "import torchaudio.transforms as T\n",
    "from nnAudio import features\n",
    "import warnings\n",
    "\n",
    "HF_SOURCE = \"BernardoTorres/singer-identity\"\n",
    "\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spec_layer: str = \"melspectogram\",\n",
    "        n_fft: int = 2048,\n",
    "        hop_length: int = 512,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if spec_layer == \"melspectogram\":\n",
    "            n_mels = 128\n",
    "            if kwargs.get(\"n_mels\", 0):\n",
    "                n_mels = kwargs[\"n_mels\"]\n",
    "            self.spec_layer = features.MelSpectrogram(\n",
    "                n_fft=n_fft, hop_length=hop_length, verbose=False, n_mels=n_mels\n",
    "            )\n",
    "        elif spec_layer == \"stft\":\n",
    "            self.spec_layer = features.STFT(\n",
    "                n_fft=n_fft,\n",
    "                hop_length=hop_length,\n",
    "                verbose=False,\n",
    "                output_format=\"Magnitude\" ** kwargs,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.spec_layer(x)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder, used to extract embeddings from the input acoustic features\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, backbone=\"efficientnet_b0\", embedding_dim=1000, pretrained=False, **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # With attention pooling, not used in the paper\n",
    "        if backbone == \"efficientnet_b0_att\":\n",
    "            encoder_backbone = Efficientnet_att(\n",
    "                vismod=\"efficientnet_b0\",\n",
    "                num_classes=1000,\n",
    "                pretrained=pretrained,\n",
    "                embedding_dim=embedding_dim,\n",
    "                **kwargs,\n",
    "            )\n",
    "            self.net = nn.Sequential(LogScale(), Grey2Rgb(), encoder_backbone)\n",
    "\n",
    "        # Default to efficientnet backbone with average pooling, used in the paper\n",
    "        else:\n",
    "            encoder_backbone = get_vision_backbone(\n",
    "                vismod=backbone,\n",
    "                num_classes=embedding_dim,\n",
    "                pretrained=pretrained,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "            # Grey2Rgb() is used to replicate mel-spec channel (efficientnet expects 3)\n",
    "            self.net = nn.Sequential(LogScale(), Grey2Rgb(), encoder_backbone)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape [batch, channels, frames]\n",
    "        \"\"\"\n",
    "        embedding = self.net(x)\n",
    "        return embedding\n",
    "\n",
    "\n",
    "class Projection(nn.Module):\n",
    "    \"\"\"Projection head, used to reduce the dimensionality of the embedding\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=1000,\n",
    "        output_dim=128,\n",
    "        nonlinearity=None,\n",
    "        is_identity=False,\n",
    "        l2_normalize=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.l2_normalize = l2_normalize\n",
    "        self.is_identity = is_identity\n",
    "        self.output_dim = output_dim\n",
    "        if is_identity:\n",
    "            self.net = nn.Identity()\n",
    "        else:\n",
    "            if nonlinearity is None:\n",
    "                nonlinearity = torch.nn.SiLU()\n",
    "            self.net = nn.Sequential(\n",
    "                nonlinearity, torch.nn.Linear(input_dim, output_dim)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        projection = self.net(x)\n",
    "        if self.l2_normalize and not self.is_identity:\n",
    "            projection = torch.nn.functional.normalize(projection, dim=-1)\n",
    "        return projection\n",
    "\n",
    "\n",
    "class IdentityEncoder(nn.Module):\n",
    "    \"\"\"Wraps a feature extractor with an encoder, without projection head\n",
    "    Useful for loading pretrained models\"\"\"\n",
    "\n",
    "    def __init__(self, feature_extractor, encoder):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = FeatureExtractor(**feature_extractor)\n",
    "        self.encoder = Encoder(**encoder)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(self.feature_extractor(x))\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dims: List[int],\n",
    "        activation: Union[bool, nn.Module] = True,\n",
    "        use_batchnorm: Union[bool, int] = False,\n",
    "        batchnorm_fn: Optional[nn.Module] = None,\n",
    "        last_layer: Optional[nn.Module] = None,\n",
    "        bias: Optional[bool] = None,\n",
    "        layer_init: Optional[Union[Callable[[nn.Module], nn.Module], str]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_dim = dims[0]\n",
    "        self.out_dim = dims[-1]\n",
    "\n",
    "        if len(dims) < 2:\n",
    "            self.model = nn.Identity()\n",
    "\n",
    "            if activation or use_batchnorm:\n",
    "                warnings.warn(\n",
    "                    \"An activation/batch-norm is defined for the projector \"\n",
    "                    \"whereas it is the identity function.\"\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            # define activation layer\n",
    "            if activation is True:\n",
    "                activation = nn.ReLU()\n",
    "\n",
    "            # define batch-norm layer\n",
    "            if use_batchnorm is not False and batchnorm_fn is None:\n",
    "                batchnorm_fn = nn.BatchNorm1d\n",
    "\n",
    "            # useless to add bias just before a batch-norm layer but add the option for completeness\n",
    "            if bias is None:\n",
    "                bias = isinstance(use_batchnorm, bool) and not use_batchnorm\n",
    "\n",
    "            # NOTE: with old implementation use_batchnorm=True means use_batchnorm=False + bias=True\n",
    "            if use_batchnorm is True:\n",
    "                use_batchnorm = 0\n",
    "            elif use_batchnorm is False:\n",
    "                use_batchnorm = float(\"inf\")\n",
    "\n",
    "            ckpt_path = None\n",
    "            if isinstance(layer_init, str):\n",
    "                ckpt_path = layer_init\n",
    "                layer_init = lambda x: x\n",
    "            elif layer_init is None:\n",
    "                layer_init = lambda x: x\n",
    "\n",
    "            layers = []\n",
    "\n",
    "            output_dim = dims.pop()\n",
    "\n",
    "            for i in range(len(dims) - 1):\n",
    "                in_dim, out_dim = dims[i], dims[i + 1]\n",
    "                layers.append(layer_init(nn.Linear(in_dim, out_dim, bias=bias)))\n",
    "                if i >= use_batchnorm:\n",
    "                    layers.append(batchnorm_fn(out_dim))\n",
    "                if activation:\n",
    "                    layers.append(activation)\n",
    "            layers.append(nn.Linear(dims.pop(), output_dim, bias=True))\n",
    "            if last_layer is not None:\n",
    "                layers.append(last_layer)\n",
    "\n",
    "            self.model = nn.Sequential(*layers)\n",
    "\n",
    "            if ckpt_path is not None:\n",
    "                self.load_state_dict(torch.load(ckpt_path))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "class Efficientnet_att(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vismod=\"efficientnet_b0\",\n",
    "        num_classes=1000,\n",
    "        pretrained=False,\n",
    "        embedding_dim=1000,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(Efficientnet_att, self).__init__()\n",
    "\n",
    "        self.vision = get_vision_backbone(\n",
    "            vismod=vismod, num_classes=num_classes, pretrained=pretrained, **kwargs\n",
    "        ).features\n",
    "\n",
    "        self.att = nn.Sequential(\n",
    "            nn.Conv1d(1280, int(embedding_dim / 2), kernel_size=1, groups=2),\n",
    "            AttentiveStatisticPool(int(embedding_dim / 2), 128),\n",
    "        )\n",
    "\n",
    "        self.avg = nn.AvgPool2d((4, 1))\n",
    "        self.bn1 = nn.BatchNorm1d(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.vision(x)\n",
    "        y = self.avg(y).squeeze(2)\n",
    "        y = self.att(y)\n",
    "        return self.bn1(y)\n",
    "\n",
    "\n",
    "# Not used in the experiments in the paper\n",
    "class AttentiveStatisticPool(nn.Module):\n",
    "    def __init__(self, c_in, c_mid):\n",
    "        super(AttentiveStatisticPool, self).__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv1d(c_in, c_mid, kernel_size=1),\n",
    "            nn.Tanh(),  # seems like most implementations uses tanh?\n",
    "            nn.Conv1d(c_mid, c_in, kernel_size=1),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape: B x C x t\n",
    "        alpha = self.network(x)\n",
    "        mu_hat = torch.sum(alpha * x, dim=-1)\n",
    "        var = torch.sum(alpha * x**2, dim=-1) - mu_hat**2\n",
    "        std_hat = torch.sqrt(var.clamp(min=1e-9))\n",
    "        y = torch.cat([mu_hat, std_hat], dim=-1)\n",
    "        # y.shape: B x (c_in*2)\n",
    "        return y\n",
    "\n",
    "\n",
    "def load_model(\n",
    "    model, source=HF_SOURCE, torchscript=False, savedir=None, input_sr=44100\n",
    "):\n",
    "    \"\"\"Load a model from a source, can be a local path or a huggingface model hub ID\"\"\"\n",
    "\n",
    "    if torchscript:\n",
    "        if input_sr != 44100:\n",
    "            raise Exception(\"Torchscript models only support 44100 Hz input\")\n",
    "        model = from_scripted(f\"{model}/model.ts\", source, savedir=savedir)\n",
    "    elif \".\" in model:\n",
    "        # Instantiate IdentityEncoder with input_sr argument\n",
    "        model = from_hparams(\n",
    "            IdentityEncoder,\n",
    "            source,\n",
    "            hparams_file=f\"{model}/hyperparams.yaml\",\n",
    "            weights_file=f\"{model}/model.pt\",\n",
    "            savedir=savedir,\n",
    "        )\n",
    "    else:\n",
    "        # CHeck\n",
    "        model = from_hparams(\n",
    "            IdentityEncoder,\n",
    "            source,\n",
    "            hparams_file=f\"{model}/hyperparams.yaml\",\n",
    "            weights_file=f\"{model}/model.pt\",\n",
    "            savedir=savedir,\n",
    "        )\n",
    "    if input_sr != 44100:\n",
    "        # Replace feature extractor with a resampler\n",
    "        feature_extractor = model.feature_extractor\n",
    "        model.feature_extractor = nn.Sequential(\n",
    "            T.Resample(input_sr, 44100), feature_extractor\n",
    "        )\n",
    "        print(f\"Resampling input from {input_sr} to 44100 Hz\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from singer_identity.utils.core import similarity, roll\n",
    "\n",
    "\n",
    "def std_batch(x, var=1, eps=1e-8):\n",
    "    std = torch.sqrt(x.var(dim=0) + eps)\n",
    "    return torch.mean(F.relu(var - std))\n",
    "\n",
    "\n",
    "def variance_hinge_reg(x, y, var=1):\n",
    "    # From https://github.com/facebookresearch/vicreg\n",
    "    std_x = std_batch(x, var=var)\n",
    "    std_y = std_batch(y, var=var)\n",
    "    std_loss = std_x / 2 + std_y / 2\n",
    "    return std_loss\n",
    "\n",
    "\n",
    "def covariance(x):\n",
    "    # In official implementation they do mean over batch (to verify)\n",
    "    # mean = x.mean(1, keepdims=True)\n",
    "    mean = x.mean(dim=0)\n",
    "    x = x - mean\n",
    "    cov = torch.matmul(x.transpose(0, 1), x) / (x.shape[0] - 1)\n",
    "    # cov = (x.T @ x) / (x.shape[0] - 1)\n",
    "    return cov\n",
    "\n",
    "\n",
    "def covariance_reg(x, y):\n",
    "    eye = torch.eye(x.shape[1]).to(x.device)\n",
    "    cov_x = covariance(x)\n",
    "    cov_y = covariance(y)\n",
    "    assert cov_x.shape[0] == cov_x.shape[1]\n",
    "    assert cov_y.shape[0] == cov_y.shape[1]\n",
    "    cov_reg = (cov_x * (1 - eye)).pow(2).sum() / x.shape[1] + (cov_y * (1 - eye)).pow(\n",
    "        2\n",
    "    ).sum() / x.shape[1]\n",
    "    return cov_reg\n",
    "\n",
    "\n",
    "def invariance_loss(x, y):\n",
    "    return F.mse_loss(x, y)\n",
    "\n",
    "\n",
    "def vicreg_loss(x, y, gamma=1, fact_inv_loss=1, fact_var=1, fact_cov=1):\n",
    "    # Adapted from https://github.com/facebookresearch/vicreg\n",
    "    repr_loss = invariance_loss(x, y)\n",
    "    std_loss = variance_hinge_reg(x, y, var=gamma)\n",
    "    cov_loss = covariance_reg(x, y)\n",
    "    loss = fact_inv_loss * repr_loss + fact_var * std_loss + fact_cov * cov_loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_norms(*args):\n",
    "    norms = []\n",
    "    for arg in args:\n",
    "        norms.append(torch.sqrt((arg**2).sum(1)))\n",
    "    return norms\n",
    "\n",
    "\n",
    "def align_loss(x, y, alpha=2):\n",
    "    # From https://github.com/SsnL/align_uniform\n",
    "    return (x - y).norm(p=2, dim=1).pow(alpha).mean()\n",
    "\n",
    "\n",
    "def uniform_loss(x, t=2):\n",
    "    # From https://github.com/SsnL/align_uniform\n",
    "    return torch.pdist(x, p=2).pow(2).mul(-t).exp().mean().log()\n",
    "\n",
    "\n",
    "def contrastive_loss(z1, z2, temp=0.3, nr_negative=3, decouple=False):\n",
    "    cost_pos = similarity(z1, z2, temp)  # Positive samples\n",
    "    cost_neg = []\n",
    "\n",
    "    n_rolls = min(z1.shape[0] - 1, nr_negative)  # Number of negative samples\n",
    "    curr_neg_z = z2\n",
    "\n",
    "    for i in range(n_rolls):\n",
    "        curr_neg_z = roll(curr_neg_z)  # Shifts batch\n",
    "        cost_neg.append(similarity(z1, curr_neg_z, temp))  # Negative sim.\n",
    "\n",
    "    if not decouple:\n",
    "        cost_neg.append(cost_pos)  # Adds positive similarity in denominator\n",
    "\n",
    "    cost_neg = torch.stack(cost_neg).transpose(1, 0)\n",
    "    cost = (-cost_pos + torch.logsumexp(cost_neg, 1)).mean()\n",
    "    # TODO: implement similarities with less operations, but this works\n",
    "    ratio = torch.mean(cost_neg) / (\n",
    "        torch.mean(cost_pos) + torch.tensor(1e-6).type_as(z1)\n",
    "    )\n",
    "    return cost, ratio.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning.cli import instantiate_class\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class SSLTrainer(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_extractor: dict = {},\n",
    "        backbone: dict = {},\n",
    "        projection: dict = {},\n",
    "        optimizer1_init: dict = {},\n",
    "        use_contrastive_loss: bool = True,\n",
    "        temp: float = 0.3,\n",
    "        nr_negative: int = 64,\n",
    "        decouple: bool = True,\n",
    "        use_invariance_loss: bool = False,\n",
    "        fact_inv_loss: float = 1,\n",
    "        use_covariance_reg: bool = False,\n",
    "        fact_cov: float = 1,\n",
    "        use_variance_reg: bool = False,\n",
    "        fact_var: float = 1,\n",
    "        gamma: float = 1,\n",
    "        use_vicreg_loss: bool = False,\n",
    "        use_align_loss: bool = False,\n",
    "        fact_align_loss: float = 0.25,\n",
    "        fact_unif_loss: float = 0.5,\n",
    "        use_uniform_loss: bool = False,\n",
    "        compute_test_loss: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.feature_extractor = FeatureExtractor(**feature_extractor)\n",
    "        self.encoder = Encoder(**backbone)\n",
    "        self.projection = Projection(**projection)\n",
    "        self.net = nn.Sequential(self.feature_extractor, self.encoder)\n",
    "\n",
    "        self.optimizer1_init = optimizer1_init\n",
    "        self.gamma = (\n",
    "            gamma\n",
    "            if not self.projection.l2_normalize\n",
    "            else (1 / np.sqrt(self.projection.output_dim))\n",
    "        )\n",
    "        self.temp = temp\n",
    "        self.nr_negative = nr_negative\n",
    "        self.fact_inv_loss = fact_inv_loss\n",
    "        self.fact_cov = fact_cov\n",
    "        self.fact_var = fact_var\n",
    "        self.fact_align_loss = fact_align_loss\n",
    "        self.fact_unif_loss = fact_unif_loss\n",
    "        self.decouple = decouple\n",
    "\n",
    "        self.use_contrastive_loss = use_contrastive_loss\n",
    "        self.use_vicreg_loss = use_vicreg_loss\n",
    "        self.use_invariance_loss = use_invariance_loss\n",
    "        self.use_covariance_reg = use_covariance_reg\n",
    "        self.use_variance_reg = use_variance_reg\n",
    "        self.use_align_loss = use_align_loss\n",
    "        self.use_uniform_loss = use_uniform_loss\n",
    "        self.reloaded = True\n",
    "        self.compute_test_loss = compute_test_loss\n",
    "        print(\"Declaring trainer\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # features = self.feature_extractor(x)\n",
    "        # feature_embeddings = self.encoder(features)\n",
    "        # projection = self.projection(feature_embeddings)\n",
    "        # return projection\n",
    "        return self.net(x)\n",
    "\n",
    "    def encode(self, x):\n",
    "        acoustic_features = self.feature_extractor(x)\n",
    "        feature_embeddings = self.encoder(acoustic_features)\n",
    "        return feature_embeddings\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = list(self.feature_extractor.parameters())\n",
    "        params += list(self.encoder.parameters())\n",
    "        params += list(self.projection.parameters())\n",
    "        optimizer1 = instantiate_class(params, self.optimizer1_init)\n",
    "        return optimizer1\n",
    "\n",
    "    def shared_step(self, batch, batch_idx, step_name, sync_dist=True):\n",
    "        step_name = f\"/{step_name}\" if step_name != \"\" else step_name\n",
    "        pos_sample1 = batch[\"clip1\"]\n",
    "        pos_sample2 = batch[\"clip2\"]\n",
    "        # print('forward done')\n",
    "        batch_size = pos_sample1.shape[0]\n",
    "\n",
    "        # Projections of positive pairs\n",
    "        z1 = self(pos_sample1)\n",
    "        z2 = self(pos_sample2)\n",
    "\n",
    "        loss = torch.tensor(0).type_as(z1)\n",
    "        cont_loss = torch.tensor(0).type_as(z1)\n",
    "        ratio = 0\n",
    "        vicreg_loss = torch.tensor(0).type_as(z1)\n",
    "        inv_loss = torch.tensor(0).type_as(z1)\n",
    "        cov_reg = torch.tensor(0).type_as(z1)\n",
    "        var_reg = torch.tensor(0).type_as(z1)\n",
    "        align_loss = torch.tensor(0).type_as(z1)\n",
    "        uniform_loss = torch.tensor(0).type_as(z1)\n",
    "\n",
    "        if self.use_contrastive_loss:\n",
    "            cont_loss, ratio = contrastive_loss(\n",
    "                z1,\n",
    "                z2,\n",
    "                temp=self.temp,\n",
    "                nr_negative=self.nr_negative,\n",
    "                decouple=self.decouple,\n",
    "            )\n",
    "            loss += cont_loss\n",
    "            self.log(f\"loss_contrastive{step_name}\", cont_loss, batch_size=batch_size)\n",
    "            self.log(\n",
    "                f\"ratio_contrastive_pos_neg{step_name}\", ratio, batch_size=batch_size\n",
    "            )\n",
    "\n",
    "        if self.use_vicreg_loss:\n",
    "            vicreg_loss = vicreg_loss(\n",
    "                z1,\n",
    "                z2,\n",
    "                gamma=self.gamma,\n",
    "                fact_inv_loss=self.fact_inv_loss,\n",
    "                fact_var=self.fact_var,\n",
    "                fact_cov=self.fact_cov,\n",
    "            )\n",
    "            loss += vicreg_loss\n",
    "            self.log(f\"loss_vicreg{step_name}\", vicreg_loss, batch_size=batch_size)\n",
    "        else:\n",
    "            if self.use_invariance_loss:\n",
    "                inv_loss = invariance_loss(z1, z2) * self.fact_inv_loss\n",
    "                loss = loss + inv_loss\n",
    "                self.log(f\"loss_invariance{step_name}\", inv_loss, batch_size=batch_size)\n",
    "            if self.use_covariance_reg:\n",
    "                cov_reg = covariance_reg(z1, z2) * self.fact_cov\n",
    "                loss += cov_reg\n",
    "                self.log(f\"reg_covariance{step_name}\", cov_reg, batch_size=batch_size)\n",
    "            if self.use_variance_reg:\n",
    "                var_reg = variance_hinge_reg(z1, z2, self.gamma) * self.fact_var\n",
    "                loss = loss + var_reg\n",
    "                self.log(f\"reg_variance{step_name}\", var_reg, batch_size=batch_size)\n",
    "        if self.use_align_loss:\n",
    "            align_loss = align_loss(z1, z2) * self.fact_align_loss\n",
    "            loss += align_loss\n",
    "            self.log(f\"loss_align{step_name}\", align_loss, batch_size=batch_size)\n",
    "        if self.use_uniform_loss:\n",
    "            uniform_loss = (\n",
    "                (uniform_loss(z1) + uniform_loss(z2))\n",
    "                * self.fact_unif_loss\n",
    "                / 2\n",
    "            )\n",
    "            loss += uniform_loss\n",
    "            self.log(f\"loss_uniform{step_name}\", uniform_loss, batch_size=batch_size)\n",
    "\n",
    "        self.log(f\"loss{step_name}\", loss, prog_bar=True, batch_size=batch_size)\n",
    "        # print(f'loss is {loss}')\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.shared_step(batch, batch_idx, \"train\")\n",
    "        # print(f'loss is {loss}')\n",
    "        return loss\n",
    "\n",
    "    # def validation_step(self, batch, batch_idx):\n",
    "    #     return self.shared_step(batch, batch_idx, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx, dataloader_idx):\n",
    "        if self.compute_test_loss:\n",
    "            return self.shared_step(batch, batch_idx, \"\")\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        train_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        print(f'Epoch {self.current_epoch}: Train Loss: {train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Declaring trainer\n"
     ]
    }
   ],
   "source": [
    "model = SSLTrainer(\n",
    "        optimizer1_init = {'class_path': 'torch.optim.Adam', 'init_args': {'lr': 0.0001, 'weight_decay': 1e-5}},\n",
    "        feature_extractor = {'spec_layer': 'melspectogram', 'n_fft' : 2048, 'hop_length' : 512},\n",
    "        backbone = {'backbone' : \"efficientnet_b0\", 'pretrained' : True, 'embedding_dim': 1000},\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "trainer = Trainer(max_epochs=15,max_steps = 1000, num_nodes = 1, accelerator='gpu', devices=1)  \n",
    "\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "all_embeddings = []\n",
    "all_embeddings_aug = []\n",
    "all_labels = []\n",
    "all_audio_files = []\n",
    "\n",
    "for batch in tqdm(train_loader):\n",
    "    audio_batch = batch['clip1']\n",
    "    audio_batch_aug = batch['clip2']\n",
    "    labels = batch['group_name']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emd = model(audio_batch)\n",
    "        emd_aug = model(audio_batch_aug)\n",
    "\n",
    "    all_embeddings.append(emd)\n",
    "    all_embeddings_aug.append(emd_aug)\n",
    "    all_labels.extend(labels)\n",
    "    all_audio_files.extend(batch['audio_files'])\n",
    "\n",
    "print(len(all_embeddings))\n",
    "print(len(all_embeddings_aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/sklearn/manifold/_t_sne.py:1162: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/sklearn/manifold/_t_sne.py:1162: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mplcursors\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import hsv_to_rgb\n",
    "import colorsys \n",
    "import distinctipy\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.colors as mcolors\n",
    "from tqdm import tqdm\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "all_labels_encoded = label_encoder.fit_transform(all_labels)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=25, n_iter=5000)\n",
    "embeddings_2d = tsne.fit_transform(all_embeddings.numpy())\n",
    "embeddings_2d_aug = tsne.fit_transform(all_embeddings_aug.numpy())\n",
    "\n",
    "num_labels = len(np.unique(all_labels_encoded))\n",
    "# palette = sns.color_palette(\"husl\", num_labels) \n",
    "# color_map = {label: palette[i] for i, label in enumerate(np.unique(all_labels_encoded))}\n",
    "\n",
    "color_map = distinctipy.get_colors(num_labels) \n",
    "# print(len(color_map))\n",
    "# colors = list(mcolors.CSS4_COLORS.values())\n",
    "\n",
    "# filtered_colors = [color for color in colors if mcolors.to_rgba_array(color)[:, :3].mean(axis=0).max() < 0.87]\n",
    "\n",
    "# if len(filtered_colors) < num_labels:\n",
    "#     print(f'colours are less by {num_labels} - {len(filtered_colors)}') \n",
    "#     filtered_colors = (filtered_colors * (num_labels // len(filtered_colors) + 1))[:num_labels]\n",
    "\n",
    "random_labels = all_labels_encoded[idx]\n",
    "\n",
    "\n",
    "artists = []\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, label in enumerate(np.unique(random_labels)):\n",
    "    indices = all_labels_encoded == label\n",
    "    plt.scatter(\n",
    "        embeddings_2d[indices, 0],\n",
    "        embeddings_2d[indices, 1],\n",
    "        color=color_map[label],\n",
    "        label=label_encoder.inverse_transform([label])[0],\n",
    "        s=20\n",
    "    )\n",
    "    plt.scatter(\n",
    "        embeddings_2d_aug[indices, 0],\n",
    "        embeddings_2d_aug[indices, 1],\n",
    "        color=color_map[label+1],\n",
    "        label='aug',\n",
    "        s=20\n",
    "    )\n",
    "    for j, (x, y) in enumerate(zip(embeddings_2d[indices, 0], embeddings_2d[indices, 1])):\n",
    "        overall_index = np.arange(len(all_embeddings))[indices][j] \n",
    "        plt.text(x, y + 0.02, str(overall_index), color='black', fontsize=8, ha='center', va='bottom')\n",
    "    for j, (x, y) in enumerate(zip(embeddings_2d_aug[indices, 0], embeddings_2d_aug[indices, 1])):\n",
    "        overall_index = np.arange(len(all_embeddings_aug))[indices][j] \n",
    "        plt.text(x, y + 0.02, str(overall_index), color='black', fontsize=8, ha='center', va='bottom')\n",
    "\n",
    "plt.title('t-SNE plot of audio embeddings')\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class YourModel(pl.LightningModule):\n",
    "    def __init__(self, model_cont, lr, weight_decay, num_classes):\n",
    "        super().__init__()\n",
    "        self.model_cont = model_cont\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.num_classes = num_classes\n",
    "        self.fc = torch.nn.Linear(1000, self.num_classes)\n",
    "        self.train_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=self.num_classes)\n",
    "        self.train_recall = torchmetrics.Recall(task='multiclass', num_classes=self.num_classes)\n",
    "        self.train_f1 = torchmetrics.F1Score(task='multiclass', num_classes=self.num_classes)\n",
    "        \n",
    "        self.test_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=self.num_classes)\n",
    "        self.test_recall = torchmetrics.Recall(task='multiclass', num_classes=self.num_classes)\n",
    "        self.test_f1 = torchmetrics.F1Score(task='multiclass', num_classes=self.num_classes)\n",
    "        self.training_losses = []\n",
    "        self.all_y_pred_labels_train = []\n",
    "        self.all_y_true_train = []\n",
    "        self.all_y_pred_labels_test = []\n",
    "        self.all_y_true_test = []\n",
    "\n",
    "    def forward(self, x, embeddings=False):\n",
    "        out = self.model_cont(x)\n",
    "        if embeddings:\n",
    "            return out\n",
    "        else:\n",
    "            return self.fc(out)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch['clip1']\n",
    "        y = batch['group_name']\n",
    "        y = torch.tensor(y)\n",
    "        \n",
    "        y_pred = self.forward(x)\n",
    "\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        self.train_accuracy(y_pred, y)\n",
    "        self.train_recall(y_pred, y)\n",
    "        self.train_f1(y_pred, y)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        self.log('train_accuracy', self.train_accuracy, on_step=True, on_epoch=True)\n",
    "        self.log('train_recall', self.train_recall, on_step=True, on_epoch=True)\n",
    "        self.log('train_f1', self.train_f1, on_step=True, on_epoch=True)\n",
    "\n",
    "        y_pred_labels = torch.argmax(y_pred, dim=1)\n",
    "        \n",
    "        self.all_y_pred_labels_train.extend(y_pred_labels.cpu().numpy())\n",
    "        self.all_y_true_train.extend(y.cpu().numpy())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        train_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        train_acc = self.train_accuracy.compute()\n",
    "        train_rec = self.train_recall.compute()\n",
    "        train_f1 = self.train_f1.compute()\n",
    "        print(f'Epoch {self.current_epoch}: Train Loss: {train_loss}, Train Accuracy: {train_acc}, Train Recall: {train_rec}, Train F1: {train_f1}')\n",
    "\n",
    "        y_true_np = np.array(self.all_y_true_train)\n",
    "        y_pred_np = np.array(self.all_y_pred_labels_train)\n",
    "        if self.current_epoch == 10:\n",
    "            print(classification_report(y_true_np, y_pred_np))\n",
    "\n",
    "        self.all_y_pred_labels_train = []\n",
    "        self.all_y_true_train = []\n",
    "\n",
    "        self.train_accuracy.reset()\n",
    "        self.train_recall.reset()\n",
    "        self.train_f1.reset()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch['clip1']\n",
    "        y = batch['group_name']\n",
    "        y = torch.tensor(y)\n",
    "        y_pred = self.forward(x)\n",
    "        self.test_accuracy(y_pred, y)\n",
    "        self.test_recall(y_pred, y)\n",
    "        self.test_f1(y_pred, y)\n",
    "        self.log('test_accuracy', self.test_accuracy, on_step=True, on_epoch=True)\n",
    "        self.log('test_recall', self.test_recall, on_step=True, on_epoch=True)\n",
    "        self.log('test_f1', self.test_f1, on_step=True, on_epoch=True)\n",
    "\n",
    "        y_pred_labels = torch.argmax(y_pred, dim=1)\n",
    "        \n",
    "        self.all_y_pred_labels_test.extend(y_pred_labels.cpu().numpy())\n",
    "        self.all_y_true_test.extend(y.cpu().numpy())\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        test_acc = self.test_accuracy.compute()\n",
    "        test_rec = self.test_recall.compute()\n",
    "        test_f1 = self.test_f1.compute()\n",
    "        print(f'Test Accuracy: {test_acc}, Test Recall: {test_rec}, Test F1: {test_f1}')\n",
    "        \n",
    "        y_true_np = np.array(self.all_y_true_test)\n",
    "        y_pred_np = np.array(self.all_y_pred_labels_test)\n",
    "        print(classification_report(y_true_np, y_pred_np))\n",
    "\n",
    "        self.all_y_pred_labels_test = []\n",
    "        self.all_y_true_test = []\n",
    "\n",
    "        self.test_accuracy.reset()\n",
    "        self.test_recall.reset()\n",
    "        self.test_f1.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "lr = 0.0004\n",
    "weight_decay = 1e-5\n",
    "\n",
    "test_model = YourModel(model, lr, weight_decay, 69)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=6,accelerator='gpu', devices=1)  \n",
    "\n",
    "trainer.fit(test_model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 4\n",
    "data_module_test = CSVBaseDataModule(\n",
    "    csv_file=csv_file,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    nr_samples=nr_samples,\n",
    "    normalize=normalize,\n",
    "    augmentations=augmentations,\n",
    "    transform_override=transform_override,\n",
    "    batch_sampling_mode=batch_sampling_mode,\n",
    "    sr=sr,\n",
    "    multi_epoch=multi_epoch,\n",
    "    phase = 'TESTING'\n",
    ")\n",
    "\n",
    "data_module_test.setup()\n",
    "test_loader = data_module_test.train_dataloader()\n",
    "\n",
    "trainer.test(test_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:18<00:00,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9892008639308856\n",
      "F1 Score: 0.9892052811599249\n",
      "Recall: 0.9892008639308856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_true_labels = []\n",
    "all_pred_labels = []\n",
    "\n",
    "for batch in tqdm(train_loader):\n",
    "    audio_batch = batch['clip1']\n",
    "    audio_batch = audio_batch.float()\n",
    "    labels = batch['group_name']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = test_model(audio_batch)\n",
    "        y_pred = F.softmax(y_pred, dim=1)\n",
    "        predicted = torch.argmax(y_pred, 1)\n",
    "        # print(predicted)\n",
    "        # _, predicted = torch.max(y_pred.data, 1)\n",
    "\n",
    "    all_pred_labels.extend(list(predicted))\n",
    "    all_true_labels.extend(list(labels))\n",
    "\n",
    "accuracy = accuracy_score(all_true_labels, all_pred_labels)\n",
    "f1 = f1_score(all_true_labels, all_pred_labels, average='weighted')\n",
    "recall = recall_score(all_true_labels, all_pred_labels, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         7\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       0.88      1.00      0.93         7\n",
      "           3       1.00      1.00      1.00         6\n",
      "           4       1.00      1.00      1.00         7\n",
      "           5       1.00      1.00      1.00         7\n",
      "           6       1.00      1.00      1.00         7\n",
      "           7       1.00      1.00      1.00         6\n",
      "           8       1.00      1.00      1.00         6\n",
      "           9       1.00      0.71      0.83         7\n",
      "          10       1.00      1.00      1.00         7\n",
      "          11       1.00      1.00      1.00         7\n",
      "          12       1.00      1.00      1.00         7\n",
      "          13       1.00      0.80      0.89         5\n",
      "          14       1.00      1.00      1.00         7\n",
      "          15       1.00      1.00      1.00         6\n",
      "          16       1.00      1.00      1.00         7\n",
      "          17       1.00      1.00      1.00         6\n",
      "          18       1.00      1.00      1.00         7\n",
      "          19       1.00      1.00      1.00         6\n",
      "          20       1.00      1.00      1.00         7\n",
      "          21       1.00      1.00      1.00         7\n",
      "          22       1.00      1.00      1.00         7\n",
      "          23       1.00      1.00      1.00         7\n",
      "          24       1.00      1.00      1.00         6\n",
      "          25       1.00      1.00      1.00         7\n",
      "          26       1.00      1.00      1.00         6\n",
      "          27       1.00      1.00      1.00         7\n",
      "          28       1.00      1.00      1.00         6\n",
      "          29       1.00      1.00      1.00         6\n",
      "          30       1.00      1.00      1.00         7\n",
      "          31       0.78      1.00      0.88         7\n",
      "          32       1.00      1.00      1.00         7\n",
      "          33       1.00      1.00      1.00         7\n",
      "          34       1.00      1.00      1.00         7\n",
      "          35       1.00      1.00      1.00         7\n",
      "          36       1.00      1.00      1.00         7\n",
      "          37       1.00      1.00      1.00         7\n",
      "          38       1.00      1.00      1.00         6\n",
      "          39       1.00      1.00      1.00         7\n",
      "          40       1.00      1.00      1.00         7\n",
      "          41       1.00      1.00      1.00         6\n",
      "          42       1.00      1.00      1.00         6\n",
      "          43       1.00      1.00      1.00         7\n",
      "          44       1.00      1.00      1.00         6\n",
      "          45       1.00      1.00      1.00         7\n",
      "          46       1.00      1.00      1.00         7\n",
      "          47       1.00      1.00      1.00         7\n",
      "          48       1.00      1.00      1.00         7\n",
      "          49       1.00      1.00      1.00         7\n",
      "          50       1.00      1.00      1.00         7\n",
      "          51       1.00      1.00      1.00         7\n",
      "          52       1.00      1.00      1.00         5\n",
      "          53       1.00      1.00      1.00         7\n",
      "          54       1.00      1.00      1.00         7\n",
      "          55       1.00      1.00      1.00         7\n",
      "          56       1.00      1.00      1.00         7\n",
      "          57       1.00      1.00      1.00         7\n",
      "          58       1.00      1.00      1.00         7\n",
      "          59       1.00      1.00      1.00         7\n",
      "          60       1.00      0.86      0.92         7\n",
      "          61       1.00      0.86      0.92         7\n",
      "          62       1.00      1.00      1.00         7\n",
      "          63       1.00      1.00      1.00         7\n",
      "          64       1.00      1.00      1.00         7\n",
      "          65       1.00      1.00      1.00         7\n",
      "          66       0.75      1.00      0.86         6\n",
      "          67       1.00      1.00      1.00         6\n",
      "          68       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           0.99       463\n",
      "   macro avg       0.99      0.99      0.99       463\n",
      "weighted avg       0.99      0.99      0.99       463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(all_true_labels, all_pred_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A.S. MURALI': 0, 'AFROZ BANO': 1, 'AJAY CHAKRAVARTY': 2, 'ASLAM SABRI & PARTY': 3, 'BAKUL CHATTOPADHYAY': 4, 'BALDEV SHARAN NARANG': 5, 'BHAI NARMAL SINGH': 6, 'Bal Gandharva': 7, 'Chorus (Students)': 8, 'DINA NATH MISHRA': 9, 'DR. BASAVARAJ RAJGURU': 10, 'DR. NIRMALA SUNDARA RAJAN': 11, 'DR. NIVEDITA SINGH': 12, 'DR. PANTULA RAMA': 13, 'DWARAM DURGA PRASAD RAO & DWARAM SATYANARAYANA RAO': 14, 'GAYATRY GIRISH': 15, 'K. SARASWATHI VIDYARTHI': 16, 'KOYAL DAS GUPTA': 17, 'LOU MAJAW (SOLO)': 18, 'M K SHANKARA NAMBOODRI': 19, 'M. K. PRANESH': 20, 'M. VENKATESH KUMAR.': 21, 'M.K.SWAMINATH PILLAI & D.MURUGESA PILLAI': 22, 'M.M. DANDAPANI DESIKAR': 23, 'M.S. SHEELA': 24, 'MALA SHRI PRASAD': 25, 'MALLADI SURI BABU': 26, 'MANDIRA LAHIRI': 27, 'MANJUSHA KULKARNI PATIL': 28, 'MEETA  PANDIT': 29, 'MOHINDER SINGH': 30, 'MUJAHID HUSSAIN KHAN': 31, 'NAMAGIRIPETTAI .K.KRISHNAN': 32, 'PARMESHWAR HEGDE': 33, 'PITTUKULI MURUGADAS': 34, 'PT. DHARAM NATH MISHRA': 35, 'PT. MADHAV INGALE': 36, 'PT. SIDDARAMA JAMBALADINNI': 37, 'R. SANTANA GOPALAN': 38, 'RAJAN MISHRA & PT. SAJAN MISHRA': 39, 'RAJINDAR SINGH': 40, 'RAJSHREE PATHAK': 41, 'RAMNATH AHEERVAAR': 42, 'RITA GANGULI': 43, 'RITESH & RAJNISH MISHRA': 44, 'S. BALAMBAL': 45, 'S. RUKMINI': 46, 'S. SOWMYA': 47, 'SAVITA DEVI': 48, 'SINGH BANDHU': 49, 'SITARAM SINGH': 50, 'SMT. AVANTIKA CHAKRAVARTI': 51, 'SMT. GEETHA RAJA': 52, 'SMT. KALPANA JHOKARKAR': 53, 'SMT. KAMATCHI KUPPUSWAMI': 54, 'SMT. NITHYASREE MAHADEVAN': 55, 'SMT. VIDYA KATGADE': 56, 'SREEVALSAN J. MENON': 57, 'SUBHRA GUHA': 58, 'T. R. Mahalingam': 59, 'TIRUVAIYARU BV JAYASHREE': 60, 'TUSHAR DUTTA': 61, 'USTAD GHULAM SADIQ KHAN': 62, 'USTAD SALEEM IQBAL': 63, 'VALLAM KRISHNASWAMI PILLAI & K.P.GOVINDARAJA PILLAI': 64, 'VASANTHI KRISHNA RAO': 65, 'VIDUSHI NEELA RAMGOPAL': 66, 'VIDUSHI SUMITRA GUHA': 67, 'YASHPAL': 68}\n"
     ]
    }
   ],
   "source": [
    "num_workers = 0\n",
    "data_module_test = CSVBaseDataModule(\n",
    "    csv_file=csv_file,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    nr_samples=nr_samples,\n",
    "    normalize=normalize,\n",
    "    augmentations=augmentations,\n",
    "    transform_override=transform_override,\n",
    "    batch_sampling_mode=batch_sampling_mode,\n",
    "    sr=sr,\n",
    "    multi_epoch=multi_epoch,\n",
    "    phase = 'TESTING'\n",
    ")\n",
    "\n",
    "data_module_test.setup()\n",
    "test_loader = data_module_test.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:11<00:00,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5625\n",
      "F1 Score: 0.563587738432887\n",
      "Recall: 0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_true_labels = []\n",
    "all_pred_labels = []\n",
    "\n",
    "for batch in tqdm(test_loader):\n",
    "    audio_batch = batch['clip1']\n",
    "    audio_batch = audio_batch.float()\n",
    "    labels = batch['group_name']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = test_model(audio_batch)\n",
    "        y_pred = F.softmax(y_pred, dim=1)\n",
    "        predicted = torch.argmax(y_pred, 1)\n",
    "        # print(predicted)\n",
    "        # _, predicted = torch.max(y_pred.data, 1)\n",
    "\n",
    "    all_pred_labels.extend(list(predicted))\n",
    "    all_true_labels.extend(list(labels))\n",
    "\n",
    "accuracy = accuracy_score(all_true_labels, all_pred_labels)\n",
    "f1 = f1_score(all_true_labels, all_pred_labels, average='weighted')\n",
    "recall = recall_score(all_true_labels, all_pred_labels, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       1.00      1.00      1.00         3\n",
      "           2       0.25      0.40      0.31         5\n",
      "           3       0.09      1.00      0.17         1\n",
      "           4       0.30      1.00      0.46         3\n",
      "           5       0.00      0.00      0.00         2\n",
      "           6       1.00      0.86      0.92         7\n",
      "           7       0.83      0.23      0.36        22\n",
      "           8       1.00      0.44      0.61        16\n",
      "           9       1.00      0.50      0.67         4\n",
      "          10       0.75      0.33      0.46         9\n",
      "          11       1.00      0.82      0.90        11\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       0.50      0.50      0.50         2\n",
      "          14       1.00      0.67      0.80         3\n",
      "          15       0.00      0.00      0.00         1\n",
      "          16       0.50      1.00      0.67         1\n",
      "          17       0.50      1.00      0.67         3\n",
      "          18       0.50      1.00      0.67         1\n",
      "          19       1.00      1.00      1.00         3\n",
      "          20       0.40      0.29      0.33         7\n",
      "          21       1.00      0.33      0.50         9\n",
      "          22       0.25      1.00      0.40         1\n",
      "          23       0.67      0.67      0.67         6\n",
      "          24       0.67      0.67      0.67         3\n",
      "          25       1.00      1.00      1.00         1\n",
      "          26       0.20      0.50      0.29         2\n",
      "          27       0.50      0.50      0.50         2\n",
      "          28       0.00      0.00      0.00         2\n",
      "          29       0.67      0.33      0.44         6\n",
      "          30       1.00      0.25      0.40         4\n",
      "          31       0.25      0.50      0.33         2\n",
      "          32       0.80      1.00      0.89         4\n",
      "          33       1.00      0.33      0.50         3\n",
      "          34       0.67      1.00      0.80         2\n",
      "          35       0.43      0.60      0.50         5\n",
      "          36       0.50      1.00      0.67         1\n",
      "          37       0.55      0.67      0.60         9\n",
      "          38       0.00      0.00      0.00         2\n",
      "          39       1.00      0.50      0.67         2\n",
      "          40       0.25      1.00      0.40         1\n",
      "          41       0.25      1.00      0.40         1\n",
      "          42       1.00      1.00      1.00         1\n",
      "          43       0.50      0.50      0.50         2\n",
      "          44       0.67      1.00      0.80         2\n",
      "          45       0.17      1.00      0.29         1\n",
      "          46       0.00      0.00      0.00         2\n",
      "          47       1.00      0.89      0.94         9\n",
      "          48       0.67      0.67      0.67         3\n",
      "          49       0.78      0.58      0.67        12\n",
      "          50       0.62      1.00      0.77         5\n",
      "          51       1.00      0.67      0.80         3\n",
      "          52       1.00      0.33      0.50         3\n",
      "          53       0.00      0.00      0.00         1\n",
      "          54       0.20      1.00      0.33         1\n",
      "          55       0.44      0.80      0.57         5\n",
      "          56       1.00      1.00      1.00         4\n",
      "          57       0.50      1.00      0.67         1\n",
      "          58       1.00      0.33      0.50         6\n",
      "          59       0.50      1.00      0.67         1\n",
      "          60       0.50      1.00      0.67         1\n",
      "          61       0.25      0.33      0.29         3\n",
      "          62       0.00      0.00      0.00         1\n",
      "          63       0.67      1.00      0.80         2\n",
      "          64       1.00      1.00      1.00         1\n",
      "          65       0.67      0.40      0.50         5\n",
      "          66       0.20      1.00      0.33         2\n",
      "          67       0.50      1.00      0.67         1\n",
      "          68       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.56       256\n",
      "   macro avg       0.55      0.63      0.52       256\n",
      "weighted avg       0.70      0.56      0.56       256\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/surge_siya/miniconda3/envs/siya/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(all_true_labels, all_pred_labels)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "siya",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
